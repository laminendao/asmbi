{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\ProgramData\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "okay\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from varclushi import VarClusHi\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from lime.lime_tabular import RecurrentTabularExplainer\n",
    "from tqdm import tqdm\n",
    "import keras\n",
    "from sp_modif.model_function import *\n",
    "from sp_modif.methods import *\n",
    "from sp_modif.data_prep import *\n",
    "from sp_modif.evaluator import *\n",
    "from sp_modif.SHAP import *\n",
    "from sp_modif.L2X import *\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import random\n",
    "# import keras\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score \n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn import preprocessing\n",
    "from keras import backend as K\n",
    "from sklearn.preprocessing import MinMaxScaler , StandardScaler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Activation, GRU\n",
    "from scipy import optimize\n",
    "from methods import *\n",
    "import warnings\n",
    "from tensorflow.keras import optimizers\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"okay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "training_path = 'PRONOSTIA/Learning_set'\n",
    "bearing = 'Bearing1_1'\n",
    "base_dir = os.path.join(training_path, bearing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  bearing_id  time_seconds       RUL\n",
      "0      00001  34779.065664  0.287402\n",
      "1      00001  34779.065703  0.287290\n",
      "2      00001  34779.065742  0.287178\n",
      "3      00001  34779.065781  0.287066\n",
      "4      00001  34779.065820  0.286953\n"
     ]
    }
   ],
   "source": [
    "def load_vibration_data(data_folder):\n",
    "    \"\"\"Charge les fichiers de vibration et calcule le temps en secondes pour chaque échantillon.\"\"\"\n",
    "    vibration_files = [f for f in os.listdir(data_folder) if f.startswith('acc_') and f.endswith('.csv')]\n",
    "    vibration_data = []\n",
    "\n",
    "    for file in vibration_files:\n",
    "        file_path = os.path.join(data_folder, file)\n",
    "        df = pd.read_csv(file_path, names=['Hour', 'Minute', 'Second', 'Microsecond', 'Horizontal_Accel', 'Vertical_Accel'])\n",
    "        \n",
    "        # Extraction de l'identifiant du roulement\n",
    "        bearing_id = file.split('_')[1].split('.')[0]\n",
    "        df['bearing_id'] = bearing_id\n",
    "        \n",
    "        # Calcul du temps en secondes pour chaque échantillon\n",
    "        df['time_seconds'] = df['Hour'] * 3600 + df['Minute'] * 60 + df['Second'] + df['Microsecond'] * 1e-6\n",
    "        vibration_data.append(df)\n",
    "\n",
    "    # Concaténation de toutes les données de vibration en un DataFrame unique\n",
    "    return pd.concat(vibration_data, ignore_index=True)\n",
    "\n",
    "def calculate_total_time(vibration_data):\n",
    "    \"\"\"Calcule le temps total de l'expérience pour chaque roulement.\"\"\"\n",
    "    return vibration_data.groupby('bearing_id')['time_seconds'].max()\n",
    "\n",
    "def calculate_rul(vibration_data, total_time):\n",
    "    \"\"\"Calcule la RUL pour chaque échantillon en utilisant la formule RUL = (tend - tsample) / tend.\"\"\"\n",
    "    vibration_data['RUL'] = vibration_data.apply(\n",
    "        lambda row: (total_time[row['bearing_id']] - row['time_seconds']) / total_time[row['bearing_id']]*100000,\n",
    "        axis=1\n",
    "    )\n",
    "    return vibration_data\n",
    "\n",
    "def main(data_folder):\n",
    "    \"\"\"Fonction principale pour charger les données, calculer le temps total et la RUL.\"\"\"\n",
    "    # Charger les données de vibration\n",
    "    vibration_data = load_vibration_data(data_folder)\n",
    "    \n",
    "    # Calculer le temps total de l'expérience pour chaque roulement\n",
    "    total_time = calculate_total_time(vibration_data)\n",
    "    \n",
    "    # Calculer la RUL\n",
    "    vibration_data_with_rul = calculate_rul(vibration_data, total_time)\n",
    "    \n",
    "    return vibration_data_with_rul\n",
    "\n",
    "# Utilisation de la fonction principale\n",
    "data_folder = base_dir\n",
    "vibration_data_with_rul = main(data_folder)\n",
    "print(vibration_data_with_rul[['bearing_id', 'time_seconds', 'RUL']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  bearing_id  time_seconds  RUL_norm\n",
      "0      00001  34779.065664  0.999997\n",
      "1      00001  34779.065703  0.999997\n",
      "2      00001  34779.065742  0.999997\n",
      "3      00001  34779.065781  0.999997\n",
      "4      00001  34779.065820  0.999997\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def load_vibration_data(data_folder):\n",
    "    \"\"\"Charge les fichiers de vibration et calcule le temps en secondes pour chaque échantillon.\"\"\"\n",
    "    vibration_files = [f for f in os.listdir(data_folder) if f.startswith('acc_') and f.endswith('.csv')]\n",
    "    vibration_data = []\n",
    "\n",
    "    for file in vibration_files:\n",
    "        file_path = os.path.join(data_folder, file)\n",
    "        df = pd.read_csv(file_path, names=['Hour', 'Minute', 'Second', 'Microsecond', 'Horizontal_Accel', 'Vertical_Accel'])\n",
    "        \n",
    "        # Extraction de l'identifiant du roulement\n",
    "        bearing_id = file.split('_')[1].split('.')[0]\n",
    "        df['bearing_id'] = bearing_id\n",
    "        \n",
    "        # Calcul du temps en secondes pour chaque échantillon\n",
    "        df['time_seconds'] = df['Hour'] * 3600 + df['Minute'] * 60 + df['Second'] + df['Microsecond'] * 1e-6\n",
    "        vibration_data.append(df)\n",
    "\n",
    "    # Concaténation de toutes les données de vibration en un DataFrame unique\n",
    "    return pd.concat(vibration_data, ignore_index=True)\n",
    "\n",
    "def calculate_total_time(vibration_data):\n",
    "    \"\"\"Calcule le temps total de l'expérience pour chaque roulement.\"\"\"\n",
    "    return vibration_data.groupby('bearing_id')['time_seconds'].max()\n",
    "\n",
    "def calculate_normalized_rul(vibration_data, total_time):\n",
    "    \"\"\"Calcule le RUL normalisé pour chaque échantillon en utilisant la formule RUL_norm = 1 - (RUL_t / T_cycle).\"\"\"\n",
    "    vibration_data['RUL_norm'] = vibration_data.apply(\n",
    "        lambda row: 1 - (total_time[row['bearing_id']] - row['time_seconds']) / total_time[row['bearing_id']],\n",
    "        axis=1\n",
    "    )\n",
    "    return vibration_data\n",
    "\n",
    "def main(data_folder):\n",
    "    \"\"\"Fonction principale pour charger les données, calculer le temps total et le RUL normalisé.\"\"\"\n",
    "    # Charger les données de vibration\n",
    "    vibration_data = load_vibration_data(data_folder)\n",
    "    \n",
    "    # Calculer le temps total de l'expérience pour chaque roulement\n",
    "    total_time = calculate_total_time(vibration_data)\n",
    "    \n",
    "    # Calculer le RUL normalisé\n",
    "    vibration_data_with_rul = calculate_normalized_rul(vibration_data, total_time)\n",
    "    \n",
    "    return vibration_data_with_rul\n",
    "\n",
    "# Utilisation de la fonction principale\n",
    "# data_folder = 'chemin/vers/dossier_donnees'\n",
    "vibration_data_with_rul = main(data_folder)\n",
    "print(vibration_data_with_rul[['bearing_id', 'time_seconds', 'RUL_norm']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import kurtosis, skew\n",
    "from scipy.fft import fft\n",
    "import os\n",
    "\n",
    "def load_vibration_data(data_folder):\n",
    "    \"\"\"Charge les fichiers de vibration et calcule le temps en secondes pour chaque échantillon.\"\"\"\n",
    "    vibration_files = [f for f in os.listdir(data_folder) if f.startswith('acc_') and f.endswith('.csv')]\n",
    "    vibration_data = []\n",
    "\n",
    "    for file in vibration_files:\n",
    "        file_path = os.path.join(data_folder, file)\n",
    "        df = pd.read_csv(file_path, names=['Hour', 'Minute', 'Second', 'Microsecond', 'Horizontal_Accel', 'Vertical_Accel'])\n",
    "        \n",
    "        # Extraction de l'identifiant du roulement\n",
    "        bearing_id = file.split('_')[1].split('.')[0]\n",
    "        df['bearing_id'] = bearing_id\n",
    "        \n",
    "        # Calcul du temps en secondes pour chaque échantillon\n",
    "        df['time_seconds'] = df['Hour'] * 3600 + df['Minute'] * 60 + df['Second'] + df['Microsecond'] * 1e-6\n",
    "        vibration_data.append(df)\n",
    "\n",
    "    # Concaténation de toutes les données de vibration en un DataFrame unique\n",
    "    return pd.concat(vibration_data, ignore_index=True)\n",
    "\n",
    "# Caractéristiques temporelles\n",
    "def extract_temporal_features(signal):\n",
    "    \"\"\"Extrait les caractéristiques temporelles : moyenne, écart type, RMS, skewness, etc.\"\"\"\n",
    "    features = {\n",
    "        'mean': signal.mean(),\n",
    "        'std': signal.std(),\n",
    "        'peak_to_peak': signal.max() - signal.min(),\n",
    "        'rms': np.sqrt(np.mean(signal**2)),\n",
    "        'mean_abs': np.mean(np.abs(signal)),\n",
    "        'max_abs': np.max(np.abs(signal)),\n",
    "        'skewness': skew(signal),\n",
    "        'kurtosis': kurtosis(signal),\n",
    "        'form_factor': np.sqrt(np.mean(signal**2)) / np.mean(np.abs(signal)),\n",
    "        'crest_factor': signal.max() / np.sqrt(np.mean(signal**2)),\n",
    "        'impulse_factor': signal.max() / np.mean(np.abs(signal)),\n",
    "        'margin_factor': signal.max() / (np.mean(np.sqrt(np.abs(signal))) ** 2)\n",
    "    }\n",
    "    return features\n",
    "\n",
    "# Caractéristiques fréquentielles\n",
    "def extract_frequency_features(signal, sampling_rate=25600):\n",
    "    \"\"\"Extrait les caractéristiques fréquentielles.\"\"\"\n",
    "    N = len(signal)\n",
    "    freqs = fft(signal)\n",
    "    freqs = np.abs(freqs[:N // 2])  # On garde seulement les composantes positives\n",
    "    freq_bins = np.fft.fftfreq(N, d=1/sampling_rate)[:N // 2]\n",
    "\n",
    "    max_amplitude = np.max(freqs)\n",
    "    energy_band1 = np.sum(freqs[(freq_bins >= 0) & (freq_bins < 500)]**2)\n",
    "    energy_band2 = np.sum(freqs[(freq_bins >= 500) & (freq_bins < 1000)]**2)\n",
    "    mean_freq = np.mean(freqs)\n",
    "    rms_freq = np.sqrt(np.mean(freqs**2))\n",
    "    \n",
    "    features = {\n",
    "        'max_amplitude': max_amplitude,\n",
    "        'energy_band1': energy_band1,\n",
    "        'energy_band2': energy_band2,\n",
    "        'mean_freq': mean_freq,\n",
    "        'rms_freq': rms_freq,\n",
    "        'variance_freq': np.var(freqs),\n",
    "        'std_freq': np.std(freqs),\n",
    "        'kurtosis_freq': kurtosis(freqs),\n",
    "        'skewness_freq': skew(freqs),\n",
    "        'peak_freq': freqs.max(),\n",
    "        'form_factor_freq': rms_freq / mean_freq,\n",
    "        'crest_factor_freq': max_amplitude / rms_freq\n",
    "    }\n",
    "    return features\n",
    "\n",
    "def extract_all_features(data_folder):\n",
    "    \"\"\"Fonction principale pour extraire les 52 caractéristiques (26 pour chaque direction de vibration).\"\"\"\n",
    "    # Charger les données de vibration\n",
    "    vibration_data = load_vibration_data(data_folder)\n",
    "    \n",
    "    # Initialiser un DataFrame pour stocker toutes les caractéristiques\n",
    "    features_list = []\n",
    "\n",
    "    # Extraire les caractéristiques pour chaque roulement\n",
    "    for bearing_id, group in vibration_data.groupby('bearing_id'):\n",
    "        temporal_features_h = extract_temporal_features(group['Horizontal_Accel'])\n",
    "        temporal_features_v = extract_temporal_features(group['Vertical_Accel'])\n",
    "        frequency_features_h = extract_frequency_features(group['Horizontal_Accel'])\n",
    "        frequency_features_v = extract_frequency_features(group['Vertical_Accel'])\n",
    "        \n",
    "        # Renommer les caractéristiques pour chaque direction\n",
    "        features = {f\"{k}_h\": v for k, v in temporal_features_h.items()}\n",
    "        features.update({f\"{k}_v\": v for k, v in temporal_features_v.items()})\n",
    "        features.update({f\"{k}_freq_h\": v for k, v in frequency_features_h.items()})\n",
    "        features.update({f\"{k}_freq_v\": v for k, v in frequency_features_v.items()})\n",
    "        \n",
    "        # Ajouter l'identifiant du roulement\n",
    "        features['bearing_id'] = bearing_id\n",
    "        \n",
    "        # Ajouter aux résultats\n",
    "        features_list.append(features)\n",
    "\n",
    "    # Convertir la liste de caractéristiques en DataFrame\n",
    "    features_df = pd.DataFrame(features_list)\n",
    "    return features_df\n",
    "\n",
    "# Utilisation de la fonction principale\n",
    "# data_folder = 'chemin/vers/dossier_donnees'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2803, 45)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df = extract_all_features(data_folder)\n",
    "features_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frequency_features(signal, sampling_rate=25600):\n",
    "    \"\"\"Extrait les caractéristiques fréquentielles.\"\"\"\n",
    "    if signal.empty:\n",
    "        print(\"Signal vide. Aucun calcul de caractéristiques fréquentielles.\")\n",
    "        return {}\n",
    "    \n",
    "    # Vérifier que le signal est sous forme numérique\n",
    "    signal = signal.dropna()\n",
    "    if not np.issubdtype(signal.dtype, np.number):\n",
    "        print(\"Le signal contient des données non numériques.\")\n",
    "        return {}\n",
    "\n",
    "    try:\n",
    "        N = len(signal)\n",
    "        freqs = fft(signal)\n",
    "        freqs = np.abs(freqs[:N // 2])  # On garde seulement les composantes positives\n",
    "        freq_bins = np.fft.fftfreq(N, d=1/sampling_rate)[:N // 2]\n",
    "\n",
    "        max_amplitude = np.max(freqs)\n",
    "        energy_band1 = np.sum(freqs[(freq_bins >= 0) & (freq_bins < 500)]**2)\n",
    "        energy_band2 = np.sum(freqs[(freq_bins >= 500) & (freq_bins < 1000)]**2)\n",
    "        mean_freq = np.mean(freqs)\n",
    "        rms_freq = np.sqrt(np.mean(freqs**2))\n",
    "        \n",
    "        features = {\n",
    "            'max_amplitude': max_amplitude,\n",
    "            'energy_band1': energy_band1,\n",
    "            'energy_band2': energy_band2,\n",
    "            'mean_freq': mean_freq,\n",
    "            'rms_freq': rms_freq,\n",
    "            'variance_freq': np.var(freqs),\n",
    "            'std_freq': np.std(freqs),\n",
    "            'kurtosis_freq': kurtosis(freqs),\n",
    "            'skewness_freq': skew(freqs),\n",
    "            'peak_freq': freqs.max(),\n",
    "            'form_factor_freq': rms_freq / mean_freq,\n",
    "            'crest_factor_freq': max_amplitude / rms_freq\n",
    "        }\n",
    "        return features\n",
    "    \n",
    "    except KeyError as e:\n",
    "        print(f\"Erreur dans l'extraction des caractéristiques fréquentielles : {e}\")\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.fft import fft\n",
    "from scipy.stats import kurtosis, skew\n",
    "\n",
    "def extract_frequency_features(signal, sampling_rate=25600):\n",
    "    \"\"\"Extrait les caractéristiques fréquentielles d'un signal donné.\"\"\"\n",
    "    # Convertir le signal en tableau NumPy\n",
    "    signal = np.asarray(signal)\n",
    "\n",
    "    # Vérifier si le signal est vide après conversion\n",
    "    if signal.size == 0:\n",
    "        raise ValueError(\"Signal vide : impossible de calculer des caractéristiques fréquentielles.\")\n",
    "\n",
    "    # Appliquer la FFT\n",
    "    N = len(signal)\n",
    "    freqs = fft(signal)\n",
    "    freqs = np.abs(freqs[:N // 2])  # On garde seulement les composantes positives\n",
    "    freq_bins = np.fft.fftfreq(N, d=1/sampling_rate)[:N // 2]\n",
    "    \n",
    "    # Calcul des caractéristiques fréquentielles\n",
    "    max_amplitude = np.max(freqs)\n",
    "    mean_freq = np.mean(freqs)\n",
    "    rms_freq = np.sqrt(np.mean(freqs**2))\n",
    "    variance_freq = np.var(freqs)\n",
    "    std_freq = np.std(freqs)\n",
    "    kurtosis_freq = kurtosis(freqs)\n",
    "    skewness_freq = skew(freqs)\n",
    "    peak_freq = freq_bins[np.argmax(freqs)]\n",
    "    form_factor_freq = rms_freq / mean_freq if mean_freq != 0 else 0\n",
    "    crest_factor_freq = max_amplitude / rms_freq if rms_freq != 0 else 0\n",
    "\n",
    "    features = {\n",
    "        'max_amplitude': max_amplitude,\n",
    "        'mean_freq': mean_freq,\n",
    "        'rms_freq': rms_freq,\n",
    "        'variance_freq': variance_freq,\n",
    "        'std_freq': std_freq,\n",
    "        'kurtosis_freq': kurtosis_freq,\n",
    "        'skewness_freq': skewness_freq,\n",
    "        'peak_freq': peak_freq,\n",
    "        'form_factor_freq': form_factor_freq,\n",
    "        'crest_factor_freq': crest_factor_freq\n",
    "    }\n",
    "    return features\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
