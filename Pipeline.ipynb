{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sommaire: <a class=\"anchor\" id=\"sommaire\"></a>\n",
    "* [Sommaire](#sommaire)\n",
    "* [Package Loading](#package)\n",
    "* [Model selection](#model_selection)\n",
    "    * [FD001](#fd001)\n",
    "    * [FD002](#fd002)\n",
    "    * [FD003](#fd003)\n",
    "    * [FD004](#fd004)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package's loading <a id='package'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\ProgramData\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "from lime.lime_tabular import RecurrentTabularExplainer\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error, r2_score \n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn import preprocessing\n",
    "from keras import backend as K\n",
    "from sklearn.preprocessing import MinMaxScaler , StandardScaler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Activation, GRU\n",
    "from scipy import optimize\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "\n",
    "from sp_modif.model_function import *\n",
    "from sp_modif.methods import *\n",
    "from sp_modif.data_prep import *\n",
    "from sp_modif.evaluator import *\n",
    "from sp_modif.SHAP import *\n",
    "from sp_modif.L2X import *\n",
    "from methods import *\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEED = 0\n",
    "def set_seed(seed=SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    tf.random.set_seed(SEED)\n",
    "\n",
    "# Appeler la fonction pour fixer le seed\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FD004 <a class=\"anchor\" id=\"fd004\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61249, 27) (41214, 26) (248, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# Load data and preprocess\n",
    "with tf.device('/device:GPU:0'):\n",
    "    train, test, y_test = prepare_data('FD004.txt')\n",
    "    print(train.shape, test.shape, y_test.shape)\n",
    "    sensor_names = ['T20','T24','T30','T50','P20','P15','P30','Nf','Nc','epr','Ps30','phi',\n",
    "                    'NRf','NRc','BPR','farB','htBleed','Nf_dmd','PCNfR_dmd','W31','W32']\n",
    "\n",
    "    remaining_sensors = [\"Ps30\", \"Nf\", \"phi\", \"BPR\", \"farB\"] # selection based on main_notebook\n",
    "\n",
    "    drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "\n",
    "rul_piecewise = 120\n",
    "train['RUL'].clip(upper=rul_piecewise, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1layers\n",
    "def model_lstm_1layer(input_shape, nodes_per_layer, dropout, activation):\n",
    "    \n",
    "    cb = keras.callbacks.EarlyStopping(monitor='loss', patience=4)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units = nodes_per_layer, activation=activation, \n",
    "                  input_shape=input_shape))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(256))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer=Adam(learning_rate=0.001))\n",
    "    # model.save_weights(weights_file)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\ProgramData\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "(51538, 40, 5) (51538, 1) (248, 40, 5)\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:From c:\\ProgramData\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "1611/1611 [==============================] - 10s 6ms/step - loss: 1015.2298 - val_loss: 1246.5740\n",
      "Epoch 2/20\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 455.8517 - val_loss: 1080.9202\n",
      "Epoch 3/20\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 375.6517 - val_loss: 647.6152\n",
      "Epoch 4/20\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 311.6118 - val_loss: 503.3330\n",
      "Epoch 5/20\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 292.3782 - val_loss: 393.4156\n",
      "Epoch 6/20\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 274.8222 - val_loss: 386.3050\n",
      "Epoch 7/20\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 260.6937 - val_loss: 391.2215\n",
      "Epoch 8/20\n",
      "1611/1611 [==============================] - 9s 6ms/step - loss: 245.6545 - val_loss: 435.1175\n",
      "Epoch 9/20\n",
      "1611/1611 [==============================] - 9s 6ms/step - loss: 236.1186 - val_loss: 370.4720\n",
      "Epoch 10/20\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 231.6123 - val_loss: 434.9679\n",
      "Epoch 11/20\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 224.4822 - val_loss: 471.8375\n",
      "Epoch 12/20\n",
      "1611/1611 [==============================] - 9s 6ms/step - loss: 220.8004 - val_loss: 486.1428\n",
      "Epoch 13/20\n",
      "1611/1611 [==============================] - 9s 6ms/step - loss: 215.1637 - val_loss: 509.5170\n",
      "Epoch 14/20\n",
      "1611/1611 [==============================] - 10s 6ms/step - loss: 211.0158 - val_loss: 313.4826\n",
      "Epoch 15/20\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 208.9800 - val_loss: 285.0822\n",
      "Epoch 16/20\n",
      "1611/1611 [==============================] - 9s 6ms/step - loss: 206.7787 - val_loss: 306.2220\n",
      "Epoch 17/20\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 204.7661 - val_loss: 295.4973\n",
      "Epoch 18/20\n",
      "1611/1611 [==============================] - 9s 6ms/step - loss: 201.5918 - val_loss: 361.5596\n",
      "Epoch 19/20\n",
      "1611/1611 [==============================] - 9s 6ms/step - loss: 200.7812 - val_loss: 373.8927\n",
      "Epoch 20/20\n",
      "1611/1611 [==============================] - 9s 6ms/step - loss: 199.2011 - val_loss: 357.3018\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "(51538, 40, 5) (51538, 1) (248, 40, 5)\n",
      "Epoch 1/20\n",
      "1611/1611 [==============================] - 10s 5ms/step - loss: 994.5823 - val_loss: 1112.5725\n",
      "Epoch 2/20\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 447.7617 - val_loss: 852.1896\n",
      "Epoch 3/20\n",
      "1611/1611 [==============================] - 9s 6ms/step - loss: 385.8868 - val_loss: 554.6016\n",
      "Epoch 4/20\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 311.6455 - val_loss: 369.1577\n",
      "Epoch 5/20\n",
      "1611/1611 [==============================] - 9s 6ms/step - loss: 294.4214 - val_loss: 411.0500\n",
      "Epoch 6/20\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 275.5525 - val_loss: 297.5707\n",
      "Epoch 7/20\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 261.7331 - val_loss: 275.4012\n",
      "Epoch 8/20\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 246.9024 - val_loss: 248.5061\n",
      "Epoch 9/20\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 234.3115 - val_loss: 240.8032\n",
      "Epoch 10/20\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 225.5567 - val_loss: 244.9121\n",
      "Epoch 11/20\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 219.6681 - val_loss: 242.3691\n",
      "Epoch 12/20\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 215.7907 - val_loss: 261.7549\n",
      "Epoch 13/20\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 211.4712 - val_loss: 245.3890\n",
      "Epoch 14/20\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 210.5243 - val_loss: 262.1417\n",
      "Epoch 15/20\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 206.6228 - val_loss: 247.0415\n",
      "Epoch 16/20\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 206.4291 - val_loss: 225.9198\n",
      "Epoch 17/20\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 203.1487 - val_loss: 232.4804\n",
      "Epoch 18/20\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 201.7720 - val_loss: 233.1680\n",
      "Epoch 19/20\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 201.1604 - val_loss: 224.5505\n",
      "Epoch 20/20\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 199.8868 - val_loss: 220.1625\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "(51538, 40, 5) (51538, 1) (248, 40, 5)\n",
      "Epoch 1/20\n",
      "1611/1611 [==============================] - 10s 6ms/step - loss: 1004.6995 - val_loss: 1115.7886\n",
      "Epoch 2/20\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 440.7860 - val_loss: 679.0598\n",
      "Epoch 3/20\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 378.2086 - val_loss: 354.3953\n",
      "Epoch 4/20\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 326.2924 - val_loss: 301.8978\n",
      "Epoch 5/20\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 296.0261 - val_loss: 266.2434\n",
      "Epoch 6/20\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 277.1972 - val_loss: 300.4142\n",
      "Epoch 7/20\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 262.6234 - val_loss: 371.4165\n",
      "Epoch 8/20\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 247.7545 - val_loss: 272.2933\n",
      "Epoch 9/20\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 236.1425 - val_loss: 343.1983\n",
      "Epoch 10/20\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 227.9061 - val_loss: 342.2729\n",
      "Epoch 11/20\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 221.0605 - val_loss: 301.8378\n",
      "Epoch 12/20\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 218.3087 - val_loss: 354.1901\n",
      "Epoch 13/20\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 215.3210 - val_loss: 349.4802\n",
      "Epoch 14/20\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 213.3982 - val_loss: 308.3845\n",
      "Epoch 15/20\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 209.3181 - val_loss: 346.5423\n",
      "Epoch 16/20\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 208.8185 - val_loss: 410.7566\n",
      "Epoch 17/20\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 205.7407 - val_loss: 419.6780\n",
      "Epoch 18/20\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 203.8573 - val_loss: 395.0708\n",
      "Epoch 19/20\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 200.6689 - val_loss: 390.9150\n",
      "Epoch 20/20\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 200.8645 - val_loss: 355.5815\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "(51538, 40, 5) (51538, 1) (248, 40, 5)\n",
      "Epoch 1/20\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 968.7945 - val_loss: 602.5361\n",
      "Epoch 2/20\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 446.5495 - val_loss: 770.0082\n",
      "Epoch 3/20\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 352.3344 - val_loss: 348.3073\n",
      "Epoch 4/20\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 312.1605 - val_loss: 313.3370\n",
      "Epoch 5/20\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 298.6693 - val_loss: 321.2198\n",
      "Epoch 6/20\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 282.1414 - val_loss: 317.2274\n",
      "Epoch 7/20\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 267.6494 - val_loss: 280.2747\n",
      "Epoch 8/20\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 252.4268 - val_loss: 310.6112\n",
      "Epoch 9/20\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 237.2674 - val_loss: 271.4266\n",
      "Epoch 10/20\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 231.5582 - val_loss: 259.7952\n",
      "Epoch 11/20\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 226.2669 - val_loss: 274.6731\n",
      "Epoch 12/20\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 221.3535 - val_loss: 247.4326\n",
      "Epoch 13/20\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 216.5626 - val_loss: 214.5829\n",
      "Epoch 14/20\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 215.2232 - val_loss: 236.2243\n",
      "Epoch 15/20\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 213.5568 - val_loss: 214.6338\n",
      "Epoch 16/20\n",
      "1611/1611 [==============================] - 9s 6ms/step - loss: 210.8000 - val_loss: 229.6100\n",
      "Epoch 17/20\n",
      "1611/1611 [==============================] - 9s 6ms/step - loss: 210.1759 - val_loss: 241.2258\n",
      "Epoch 18/20\n",
      "1611/1611 [==============================] - 9s 6ms/step - loss: 207.0597 - val_loss: 233.4725\n",
      "Epoch 19/20\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 207.0106 - val_loss: 251.1600\n",
      "Epoch 20/20\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 203.9889 - val_loss: 259.6230\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "(51538, 40, 5) (51538, 1) (248, 40, 5)\n",
      "Epoch 1/20\n",
      "1611/1611 [==============================] - 10s 6ms/step - loss: 954.8848 - val_loss: 1071.0214\n",
      "Epoch 2/20\n",
      "1611/1611 [==============================] - 9s 6ms/step - loss: 440.6110 - val_loss: 702.0142\n",
      "Epoch 3/20\n",
      "1611/1611 [==============================] - 9s 6ms/step - loss: 362.7706 - val_loss: 429.7177\n",
      "Epoch 4/20\n",
      "1611/1611 [==============================] - 9s 6ms/step - loss: 315.9576 - val_loss: 456.4773\n",
      "Epoch 5/20\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 294.2586 - val_loss: 498.5713\n",
      "Epoch 6/20\n",
      "1611/1611 [==============================] - 9s 6ms/step - loss: 280.0866 - val_loss: 535.5912\n",
      "Epoch 7/20\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 266.9965 - val_loss: 509.7064\n",
      "Epoch 8/20\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 248.5697 - val_loss: 456.8310\n",
      "Epoch 9/20\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 232.3078 - val_loss: 444.2433\n",
      "Epoch 10/20\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 223.2200 - val_loss: 435.8741\n",
      "Epoch 11/20\n",
      "1611/1611 [==============================] - 16s 10ms/step - loss: 216.9731 - val_loss: 346.5998\n",
      "Epoch 12/20\n",
      "1611/1611 [==============================] - 32s 20ms/step - loss: 213.2032 - val_loss: 371.5920\n",
      "Epoch 13/20\n",
      "1611/1611 [==============================] - 31s 19ms/step - loss: 211.3015 - val_loss: 302.6420\n",
      "Epoch 14/20\n",
      "1611/1611 [==============================] - 30s 19ms/step - loss: 208.2014 - val_loss: 357.6215\n",
      "Epoch 15/20\n",
      "1611/1611 [==============================] - 31s 19ms/step - loss: 205.1884 - val_loss: 339.9709\n",
      "Epoch 16/20\n",
      "1611/1611 [==============================] - 30s 19ms/step - loss: 204.5594 - val_loss: 320.3548\n",
      "Epoch 17/20\n",
      "1611/1611 [==============================] - 31s 19ms/step - loss: 203.0467 - val_loss: 368.9521\n",
      "Epoch 18/20\n",
      "1611/1611 [==============================] - 31s 19ms/step - loss: 199.6265 - val_loss: 349.8225\n",
      "Epoch 19/20\n",
      "1611/1611 [==============================] - 15s 9ms/step - loss: 199.7327 - val_loss: 360.2313\n",
      "Epoch 20/20\n",
      "1611/1611 [==============================] - 12s 8ms/step - loss: 198.2723 - val_loss: 350.1159\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "CPU times: total: 4min 15s\n",
      "Wall time: 17min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# RMSE\tstd_RMSE\tS_score\tstd_S_score\tMSE\tstd_MSE\tnodes\tdropout\tactivation\tbatch_size\tTW\n",
    "# 1\t15.449126\t0.0\t1.242658e+03\t0.0\t238.675491\t0.0\t[64]\t0.2\ttanh\t32\t40\n",
    "# RMSE\tstd_RMSE\tS_score\tstd_S_score\tMSE\tstd_MSE\talpha\tnodes\tdropout\tactivation\tbatch_size\tTW\n",
    "# 2\t16.100150\t0.0\t1.752538e+03\t0.0\t259.214813\t0.0\t0.30\t[64]\t0.2\ttanh\t32\t40\n",
    "results = pd.DataFrame()\n",
    "\n",
    "for SEED in range(5):  \n",
    "    tf.random.set_seed(SEED)\n",
    "    mse = []\n",
    "    R2_val = []\n",
    "    RMSE = []\n",
    "    score_val = []\n",
    "    \n",
    "    # 0.20\t[64]\t0.3\ttanh\t32\t25\n",
    "    \n",
    "    # parameter's sample\n",
    "    weights_file = \"weights_file_lstm_optimalmodel_clv.h5\"\n",
    "    alpha = 0.3\n",
    "    sequence_length = 40\n",
    "    epochs = 20\n",
    "    nodes_per_layer = [64]\n",
    "    dropout = 0.2\n",
    "    activation = 'tanh'\n",
    "    batch_size = 32\n",
    "    remaining_sensors = remaining_sensors\n",
    "    # create model\n",
    "    input_shape = (sequence_length, len(remaining_sensors))\n",
    "    model = model_lstm_1layer(input_shape, nodes_per_layer[0], dropout, activation)\n",
    "    \n",
    "    # Data prepration\n",
    "    X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "\n",
    "    # create sequences train, test\n",
    "    train_array = gen_data_wrapper(X_train_interim, sequence_length,remaining_sensors)\n",
    "    label_array = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'])\n",
    "\n",
    "    test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,remaining_sensors, -99.))\n",
    "               for unit_nr in X_test_interim['Unit'].unique())\n",
    "    \n",
    "    test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "    test_rul = rul_piecewise_fct(y_test,rul_piecewise)\n",
    "    print(train_array.shape, label_array.shape, test_array.shape)\n",
    "            \n",
    "    # Model fitting\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        start_time = time.time()\n",
    "        weights_file = model.get_weights()\n",
    "        model.set_weights(weights_file)  # reset optimizer and node weights before every training iteration\n",
    "        history = model.fit(train_array, label_array,\n",
    "                                validation_data=(test_array, test_rul),\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                # callbacks=[cb],\n",
    "                                verbose=1)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        training_time = end_time - start_time\n",
    "        mse.append(history.history['val_loss'][-1])\n",
    "\n",
    "        y_hat_val_split = model.predict(test_array)\n",
    "        R2_val.append(r2_score(test_rul, y_hat_val_split))\n",
    "        RMSE.append(np.sqrt(mean_squared_error(test_rul, y_hat_val_split)))\n",
    "        score_val.append(compute_s_score(test_rul, y_hat_val_split))\n",
    "            \n",
    "        \n",
    "    #  append results\n",
    "    d = {'RMSE' :np.mean(RMSE), 'std_RMSE' :np.std(RMSE),\n",
    "         'S_score' :np.mean(score_val), 'std_S_score' :np.std(score_val),\n",
    "         'MSE':np.mean(mse), 'std_MSE':np.std(mse),\n",
    "         'nodes':str(nodes_per_layer), 'dropout':dropout, \n",
    "         'activation':activation, 'batch_size':batch_size, 'TW' : sequence_length, \n",
    "         'time': training_time}\n",
    "\n",
    "#     results = results.append(pd.DataFrame(d, index=[0]), ignore_index=True)\n",
    "    results = pd.concat([results, pd.DataFrame(d, index=[0])], ignore_index=True)\n",
    "    results.to_csv('results/base/fd004.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>std_RMSE</th>\n",
       "      <th>S_score</th>\n",
       "      <th>std_S_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>TW</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.902430</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2310.933325</td>\n",
       "      <td>0.0</td>\n",
       "      <td>357.301849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>179.232490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.837874</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1377.840255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>220.162506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>169.208207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.856868</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2858.685112</td>\n",
       "      <td>0.0</td>\n",
       "      <td>355.581482</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>164.752809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.112819</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1236.360029</td>\n",
       "      <td>0.0</td>\n",
       "      <td>259.622955</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>165.653482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18.711384</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5717.449215</td>\n",
       "      <td>0.0</td>\n",
       "      <td>350.115906</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>349.229202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RMSE  std_RMSE      S_score  std_S_score         MSE  std_MSE nodes  \\\n",
       "0  18.902430       0.0  2310.933325          0.0  357.301849      0.0  [64]   \n",
       "1  14.837874       0.0  1377.840255          0.0  220.162506      0.0  [64]   \n",
       "2  18.856868       0.0  2858.685112          0.0  355.581482      0.0  [64]   \n",
       "3  16.112819       0.0  1236.360029          0.0  259.622955      0.0  [64]   \n",
       "4  18.711384       0.0  5717.449215          0.0  350.115906      0.0  [64]   \n",
       "\n",
       "   dropout activation  batch_size  TW        time  \n",
       "0      0.2       tanh          32  40  179.232490  \n",
       "1      0.2       tanh          32  40  169.208207  \n",
       "2      0.2       tanh          32  40  164.752809  \n",
       "3      0.2       tanh          32  40  165.653482  \n",
       "4      0.2       tanh          32  40  349.229202  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FD002 <a class=\"anchor\" id=\"fd002\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53759, 27) (33991, 26) (259, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# Load data and preprocess\n",
    "train, test, y_test = prepare_data('FD002.txt')\n",
    "print(train.shape, test.shape, y_test.shape)\n",
    "sensor_names = ['T20','T24','T30','T50','P20','P15','P30','Nf','Nc','epr','Ps30','phi',\n",
    "                    'NRf','NRc','BPR','farB','htBleed','Nf_dmd','PCNfR_dmd','W31','W32']\n",
    "remaining_sensors = [\"Ps30\", \"Nf\", \"phi\", \"epr\"] # selection based on main_notebook [1] \"Nf\" [1] \"Ps30\" [1] \"phi\"\n",
    "drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "\n",
    "rul_piecewise = 125\n",
    "train['RUL'].clip(upper=rul_piecewise, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43619, 40, 4) (43619, 1) (259, 40, 4)\n",
      "Epoch 1/20\n",
      "341/341 [==============================] - 4s 9ms/step - loss: 2236.6396 - val_loss: 860.0703\n",
      "Epoch 2/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 632.3809 - val_loss: 335.7626\n",
      "Epoch 3/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 446.0878 - val_loss: 323.0473\n",
      "Epoch 4/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 406.7737 - val_loss: 347.9665\n",
      "Epoch 5/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 380.7448 - val_loss: 302.5203\n",
      "Epoch 6/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 368.7535 - val_loss: 318.2314\n",
      "Epoch 7/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 360.7570 - val_loss: 289.4062\n",
      "Epoch 8/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 352.9235 - val_loss: 283.3463\n",
      "Epoch 9/20\n",
      "341/341 [==============================] - 4s 11ms/step - loss: 341.6447 - val_loss: 286.5387\n",
      "Epoch 10/20\n",
      "341/341 [==============================] - 7s 21ms/step - loss: 340.1083 - val_loss: 284.2273\n",
      "Epoch 11/20\n",
      "341/341 [==============================] - 7s 21ms/step - loss: 335.2086 - val_loss: 275.2146\n",
      "Epoch 12/20\n",
      "341/341 [==============================] - 7s 21ms/step - loss: 330.4875 - val_loss: 282.2957\n",
      "Epoch 13/20\n",
      "341/341 [==============================] - 7s 22ms/step - loss: 326.6052 - val_loss: 312.6443\n",
      "Epoch 14/20\n",
      "341/341 [==============================] - 7s 21ms/step - loss: 318.3932 - val_loss: 267.6143\n",
      "Epoch 15/20\n",
      "341/341 [==============================] - 7s 22ms/step - loss: 318.8814 - val_loss: 308.0165\n",
      "Epoch 16/20\n",
      "341/341 [==============================] - 7s 21ms/step - loss: 314.3333 - val_loss: 269.3803\n",
      "Epoch 17/20\n",
      "341/341 [==============================] - 7s 21ms/step - loss: 314.1977 - val_loss: 269.2411\n",
      "Epoch 18/20\n",
      "341/341 [==============================] - 7s 21ms/step - loss: 307.5070 - val_loss: 258.5770\n",
      "Epoch 19/20\n",
      "341/341 [==============================] - 7s 22ms/step - loss: 303.5731 - val_loss: 279.6862\n",
      "Epoch 20/20\n",
      "341/341 [==============================] - 7s 22ms/step - loss: 305.1843 - val_loss: 280.3105\n",
      "9/9 [==============================] - 1s 7ms/step\n",
      "(43619, 40, 4) (43619, 1) (259, 40, 4)\n",
      "Epoch 1/20\n",
      "341/341 [==============================] - 12s 24ms/step - loss: 2412.6326 - val_loss: 1778.6328\n",
      "Epoch 2/20\n",
      "341/341 [==============================] - 8s 23ms/step - loss: 1067.6980 - val_loss: 372.2478\n",
      "Epoch 3/20\n",
      "341/341 [==============================] - 8s 22ms/step - loss: 424.0416 - val_loss: 359.8002\n",
      "Epoch 4/20\n",
      "341/341 [==============================] - 8s 22ms/step - loss: 390.6566 - val_loss: 387.5718\n",
      "Epoch 5/20\n",
      "341/341 [==============================] - 7s 22ms/step - loss: 370.3558 - val_loss: 422.9388\n",
      "Epoch 6/20\n",
      "341/341 [==============================] - 7s 21ms/step - loss: 360.5968 - val_loss: 430.0512\n",
      "Epoch 7/20\n",
      "341/341 [==============================] - 7s 22ms/step - loss: 351.3860 - val_loss: 391.5057\n",
      "Epoch 8/20\n",
      "341/341 [==============================] - 8s 22ms/step - loss: 348.6420 - val_loss: 433.9803\n",
      "Epoch 9/20\n",
      "341/341 [==============================] - 8s 22ms/step - loss: 342.6228 - val_loss: 414.6710\n",
      "Epoch 10/20\n",
      "341/341 [==============================] - 8s 23ms/step - loss: 336.9984 - val_loss: 442.8880\n",
      "Epoch 11/20\n",
      "341/341 [==============================] - 7s 22ms/step - loss: 334.9041 - val_loss: 446.9933\n",
      "Epoch 12/20\n",
      "341/341 [==============================] - 7s 22ms/step - loss: 329.0080 - val_loss: 413.5786\n",
      "Epoch 13/20\n",
      "341/341 [==============================] - 7s 21ms/step - loss: 327.8910 - val_loss: 454.3326\n",
      "Epoch 14/20\n",
      "341/341 [==============================] - 7s 22ms/step - loss: 322.2101 - val_loss: 442.4109\n",
      "Epoch 15/20\n",
      "341/341 [==============================] - 7s 21ms/step - loss: 321.5542 - val_loss: 428.3742\n",
      "Epoch 16/20\n",
      "341/341 [==============================] - 8s 22ms/step - loss: 319.9256 - val_loss: 432.9258\n",
      "Epoch 17/20\n",
      "341/341 [==============================] - 8s 22ms/step - loss: 315.8217 - val_loss: 433.2453\n",
      "Epoch 18/20\n",
      "341/341 [==============================] - 7s 22ms/step - loss: 313.8445 - val_loss: 465.7370\n",
      "Epoch 19/20\n",
      "341/341 [==============================] - 7s 22ms/step - loss: 310.2045 - val_loss: 422.4187\n",
      "Epoch 20/20\n",
      "341/341 [==============================] - 7s 21ms/step - loss: 314.5171 - val_loss: 362.5939\n",
      "9/9 [==============================] - 1s 8ms/step\n",
      "(43619, 40, 4) (43619, 1) (259, 40, 4)\n",
      "Epoch 1/20\n",
      "341/341 [==============================] - 13s 27ms/step - loss: 2348.5068 - val_loss: 1760.0106\n",
      "Epoch 2/20\n",
      "341/341 [==============================] - 8s 25ms/step - loss: 905.3714 - val_loss: 538.2465\n",
      "Epoch 3/20\n",
      "341/341 [==============================] - 9s 25ms/step - loss: 402.0114 - val_loss: 497.4825\n",
      "Epoch 4/20\n",
      "341/341 [==============================] - 9s 26ms/step - loss: 379.8981 - val_loss: 480.3258\n",
      "Epoch 5/20\n",
      "341/341 [==============================] - 9s 26ms/step - loss: 365.4786 - val_loss: 461.1736\n",
      "Epoch 6/20\n",
      "341/341 [==============================] - 8s 25ms/step - loss: 358.5223 - val_loss: 442.1980\n",
      "Epoch 7/20\n",
      "341/341 [==============================] - 8s 23ms/step - loss: 352.4693 - val_loss: 417.5535\n",
      "Epoch 8/20\n",
      "341/341 [==============================] - 9s 25ms/step - loss: 341.7687 - val_loss: 392.7629\n",
      "Epoch 9/20\n",
      "341/341 [==============================] - 8s 25ms/step - loss: 336.1259 - val_loss: 360.9402\n",
      "Epoch 10/20\n",
      "341/341 [==============================] - 8s 25ms/step - loss: 335.2411 - val_loss: 383.7899\n",
      "Epoch 11/20\n",
      "341/341 [==============================] - 8s 25ms/step - loss: 330.1551 - val_loss: 363.0449\n",
      "Epoch 12/20\n",
      "341/341 [==============================] - 8s 25ms/step - loss: 333.1238 - val_loss: 365.9995\n",
      "Epoch 13/20\n",
      "341/341 [==============================] - 9s 25ms/step - loss: 321.7496 - val_loss: 359.0829\n",
      "Epoch 14/20\n",
      "341/341 [==============================] - 9s 25ms/step - loss: 322.4287 - val_loss: 346.7399\n",
      "Epoch 15/20\n",
      "341/341 [==============================] - 8s 25ms/step - loss: 320.3041 - val_loss: 329.7458\n",
      "Epoch 16/20\n",
      "341/341 [==============================] - 9s 25ms/step - loss: 316.7996 - val_loss: 335.5134\n",
      "Epoch 17/20\n",
      "341/341 [==============================] - 9s 25ms/step - loss: 315.2252 - val_loss: 321.8520\n",
      "Epoch 18/20\n",
      "341/341 [==============================] - 9s 25ms/step - loss: 309.6015 - val_loss: 318.0584\n",
      "Epoch 19/20\n",
      "341/341 [==============================] - 9s 25ms/step - loss: 309.6842 - val_loss: 333.1048\n",
      "Epoch 20/20\n",
      "341/341 [==============================] - 8s 25ms/step - loss: 312.3453 - val_loss: 311.0107\n",
      "9/9 [==============================] - 1s 8ms/step\n",
      "(43619, 40, 4) (43619, 1) (259, 40, 4)\n",
      "Epoch 1/20\n",
      "341/341 [==============================] - 12s 24ms/step - loss: 2393.1924 - val_loss: 1775.1542\n",
      "Epoch 2/20\n",
      "341/341 [==============================] - 7s 21ms/step - loss: 1060.0916 - val_loss: 439.5266\n",
      "Epoch 3/20\n",
      "341/341 [==============================] - 7s 21ms/step - loss: 429.7998 - val_loss: 385.8110\n",
      "Epoch 4/20\n",
      "341/341 [==============================] - 7s 21ms/step - loss: 382.6584 - val_loss: 337.9987\n",
      "Epoch 5/20\n",
      "341/341 [==============================] - 8s 22ms/step - loss: 363.5519 - val_loss: 324.3564\n",
      "Epoch 6/20\n",
      "341/341 [==============================] - 8s 22ms/step - loss: 350.1677 - val_loss: 327.8211\n",
      "Epoch 7/20\n",
      "341/341 [==============================] - 7s 22ms/step - loss: 339.6964 - val_loss: 308.1677\n",
      "Epoch 8/20\n",
      "341/341 [==============================] - 7s 22ms/step - loss: 337.8622 - val_loss: 311.6013\n",
      "Epoch 9/20\n",
      "341/341 [==============================] - 8s 22ms/step - loss: 336.2747 - val_loss: 347.1383\n",
      "Epoch 10/20\n",
      "341/341 [==============================] - 7s 21ms/step - loss: 325.2715 - val_loss: 348.4986\n",
      "Epoch 11/20\n",
      "341/341 [==============================] - 7s 21ms/step - loss: 323.2180 - val_loss: 312.1378\n",
      "Epoch 12/20\n",
      "341/341 [==============================] - 7s 22ms/step - loss: 320.9955 - val_loss: 310.5790\n",
      "Epoch 13/20\n",
      "341/341 [==============================] - 7s 22ms/step - loss: 316.9537 - val_loss: 297.3209\n",
      "Epoch 14/20\n",
      "341/341 [==============================] - 7s 22ms/step - loss: 314.5515 - val_loss: 271.6813\n",
      "Epoch 15/20\n",
      "341/341 [==============================] - 8s 23ms/step - loss: 310.8028 - val_loss: 268.3722\n",
      "Epoch 16/20\n",
      "341/341 [==============================] - 8s 22ms/step - loss: 306.1924 - val_loss: 276.3617\n",
      "Epoch 17/20\n",
      "341/341 [==============================] - 7s 22ms/step - loss: 302.7885 - val_loss: 260.9581\n",
      "Epoch 18/20\n",
      "341/341 [==============================] - 8s 22ms/step - loss: 299.2450 - val_loss: 277.8274\n",
      "Epoch 19/20\n",
      "341/341 [==============================] - 7s 22ms/step - loss: 302.7105 - val_loss: 280.0595\n",
      "Epoch 20/20\n",
      "341/341 [==============================] - 7s 22ms/step - loss: 298.8992 - val_loss: 292.1807\n",
      "9/9 [==============================] - 1s 8ms/step\n",
      "(43619, 40, 4) (43619, 1) (259, 40, 4)\n",
      "Epoch 1/20\n",
      "341/341 [==============================] - 12s 25ms/step - loss: 2344.3616 - val_loss: 1844.4548\n",
      "Epoch 2/20\n",
      "341/341 [==============================] - 7s 22ms/step - loss: 1161.3053 - val_loss: 392.3846\n",
      "Epoch 3/20\n",
      "341/341 [==============================] - 8s 23ms/step - loss: 419.4004 - val_loss: 397.8771\n",
      "Epoch 4/20\n",
      "341/341 [==============================] - 8s 22ms/step - loss: 391.0707 - val_loss: 377.0001\n",
      "Epoch 5/20\n",
      "341/341 [==============================] - 7s 22ms/step - loss: 367.0487 - val_loss: 367.4654\n",
      "Epoch 6/20\n",
      "341/341 [==============================] - 8s 22ms/step - loss: 353.4609 - val_loss: 395.1433\n",
      "Epoch 7/20\n",
      "341/341 [==============================] - 7s 22ms/step - loss: 345.7298 - val_loss: 372.5245\n",
      "Epoch 8/20\n",
      "341/341 [==============================] - 7s 22ms/step - loss: 342.0132 - val_loss: 358.4028\n",
      "Epoch 9/20\n",
      "341/341 [==============================] - 7s 21ms/step - loss: 333.7693 - val_loss: 381.8447\n",
      "Epoch 10/20\n",
      "341/341 [==============================] - 7s 22ms/step - loss: 326.6694 - val_loss: 367.0834\n",
      "Epoch 11/20\n",
      "341/341 [==============================] - 8s 22ms/step - loss: 328.9298 - val_loss: 359.5650\n",
      "Epoch 12/20\n",
      "341/341 [==============================] - 8s 22ms/step - loss: 326.4218 - val_loss: 352.2826\n",
      "Epoch 13/20\n",
      "341/341 [==============================] - 8s 22ms/step - loss: 321.4069 - val_loss: 337.7172\n",
      "Epoch 14/20\n",
      "341/341 [==============================] - 7s 22ms/step - loss: 317.3853 - val_loss: 338.3387\n",
      "Epoch 15/20\n",
      "341/341 [==============================] - 7s 22ms/step - loss: 313.2751 - val_loss: 332.8058\n",
      "Epoch 16/20\n",
      "341/341 [==============================] - 7s 22ms/step - loss: 313.1180 - val_loss: 327.8522\n",
      "Epoch 17/20\n",
      "341/341 [==============================] - 7s 22ms/step - loss: 308.0822 - val_loss: 325.9774\n",
      "Epoch 18/20\n",
      "341/341 [==============================] - 7s 22ms/step - loss: 306.2253 - val_loss: 308.6061\n",
      "Epoch 19/20\n",
      "341/341 [==============================] - 7s 22ms/step - loss: 303.2224 - val_loss: 295.3638\n",
      "Epoch 20/20\n",
      "341/341 [==============================] - 7s 21ms/step - loss: 303.2769 - val_loss: 302.2352\n",
      "9/9 [==============================] - 1s 10ms/step\n",
      "CPU times: total: 5min 3s\n",
      "Wall time: 12min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# RMSE\tstd_RMSE\tS_score\tstd_S_score\tMSE\tstd_MSE\talpha\tnodes\tdropout\tactivation\tbatch_size\tTW\n",
    "# 16.603718\t0.0\t1.343367e+03\t0.0\t275.683502\t0.0\t0.20\t[32]\t0.1\ttanh\t128\t40\n",
    "results002 = pd.DataFrame()\n",
    "\n",
    "for SEED in range(5):  \n",
    "    tf.random.set_seed(SEED)\n",
    "    mse = []\n",
    "    R2_val = []\n",
    "    RMSE = []\n",
    "    score_val = []\n",
    "    \n",
    "    # 0.20\t[64]\t0.3\ttanh\t32\t25\n",
    "    \n",
    "    # parameter's sample\n",
    "    # weights_file = \"weights_file_lstm_optimalmodel_clv.h5\"\n",
    "    alpha = 0.2\n",
    "    sequence_length = 40\n",
    "    epochs = 20\n",
    "    nodes_per_layer = [32]\n",
    "    dropout = 0.1\n",
    "    activation = 'tanh'\n",
    "    batch_size = 128\n",
    "    remaining_sensors = remaining_sensors\n",
    "    # create model\n",
    "    input_shape = (sequence_length, len(remaining_sensors))    \n",
    "    # Data prepration\n",
    "    X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "\n",
    "    # create sequences train, test\n",
    "    train_array = gen_data_wrapper(X_train_interim, sequence_length,remaining_sensors)\n",
    "    label_array = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'])\n",
    "\n",
    "    test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,remaining_sensors, -99.))\n",
    "               for unit_nr in X_test_interim['Unit'].unique())\n",
    "    \n",
    "    test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "    test_rul = rul_piecewise_fct(y_test,rul_piecewise)\n",
    "    print(train_array.shape, label_array.shape, test_array.shape)\n",
    "    model = model_lstm_1layer(input_shape, nodes_per_layer[0], dropout, activation)\n",
    "       \n",
    "    # Model fitting\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        start_time = time.time()\n",
    "        weights_file = model.get_weights()\n",
    "        model.set_weights(weights_file)  # reset optimizer and node weights before every training iteration\n",
    "        history = model.fit(train_array, label_array,\n",
    "                                validation_data=(test_array, test_rul),\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                # callbacks=[cb],\n",
    "                                verbose=1)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        training_time = end_time - start_time\n",
    "        mse.append(history.history['val_loss'][-1])\n",
    "\n",
    "        y_hat_val_split = model.predict(test_array)\n",
    "        R2_val.append(r2_score(test_rul, y_hat_val_split))\n",
    "        RMSE.append(np.sqrt(mean_squared_error(test_rul, y_hat_val_split)))\n",
    "        score_val.append(compute_s_score(test_rul, y_hat_val_split))\n",
    "            \n",
    "        \n",
    "    #  append results\n",
    "    d = {'RMSE' :np.mean(RMSE), 'std_RMSE' :np.std(RMSE),\n",
    "         'S_score' :np.mean(score_val), 'std_S_score' :np.std(score_val),\n",
    "         'MSE':np.mean(mse), 'std_MSE':np.std(mse),\n",
    "         'nodes':str(nodes_per_layer), 'dropout':dropout, \n",
    "         'activation':activation, 'batch_size':batch_size, 'TW' : sequence_length, \n",
    "         'time': training_time}\n",
    "\n",
    "#     results = results.append(pd.DataFrame(d, index=[0]), ignore_index=True)\n",
    "    results002 = pd.concat([results002, pd.DataFrame(d, index=[0])], ignore_index=True)\n",
    "    results002.to_csv('results/base/fd002.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>std_RMSE</th>\n",
       "      <th>S_score</th>\n",
       "      <th>std_S_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>TW</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16.742477</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1304.576739</td>\n",
       "      <td>0.0</td>\n",
       "      <td>280.310547</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "      <td>40</td>\n",
       "      <td>108.490979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19.041899</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8248.457896</td>\n",
       "      <td>0.0</td>\n",
       "      <td>362.593933</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "      <td>40</td>\n",
       "      <td>153.369488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.635494</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1783.964319</td>\n",
       "      <td>0.0</td>\n",
       "      <td>311.010651</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "      <td>40</td>\n",
       "      <td>174.991366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.093295</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1889.205625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>292.180725</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "      <td>40</td>\n",
       "      <td>152.966745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.384912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1440.336592</td>\n",
       "      <td>0.0</td>\n",
       "      <td>302.235168</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "      <td>40</td>\n",
       "      <td>154.377419</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RMSE  std_RMSE      S_score  std_S_score         MSE  std_MSE nodes  \\\n",
       "0  16.742477       0.0  1304.576739          0.0  280.310547      0.0  [32]   \n",
       "1  19.041899       0.0  8248.457896          0.0  362.593933      0.0  [32]   \n",
       "2  17.635494       0.0  1783.964319          0.0  311.010651      0.0  [32]   \n",
       "3  17.093295       0.0  1889.205625          0.0  292.180725      0.0  [32]   \n",
       "4  17.384912       0.0  1440.336592          0.0  302.235168      0.0  [32]   \n",
       "\n",
       "   dropout activation  batch_size  TW        time  \n",
       "0      0.1       tanh         128  40  108.490979  \n",
       "1      0.1       tanh         128  40  153.369488  \n",
       "2      0.1       tanh         128  40  174.991366  \n",
       "3      0.1       tanh         128  40  152.966745  \n",
       "4      0.1       tanh         128  40  154.377419  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FD003 <a class=\"anchor\" id=\"fd003\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24720, 27) (16596, 26) (100, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# Load data and preprocess\n",
    "train, test, y_test = prepare_data('FD003.txt')\n",
    "print(train.shape, test.shape, y_test.shape)\n",
    "sensor_names = ['T20','T24','T30','T50','P20','P15','P30','Nf','Nc','epr','Ps30','phi',\n",
    "                    'NRf','NRc','BPR','farB','htBleed','Nf_dmd','PCNfR_dmd','W31','W32']\n",
    "remaining_sensors = [\"Ps30\", \"NRc\", \"W31\", \"BPR\"] # selection based on main_notebook [1] \"Nf\" [1] \"Ps30\" [1] \"phi\"\n",
    "drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "\n",
    "rul_piecewise = 125\n",
    "train['RUL'].clip(upper=rul_piecewise, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21320, 35, 4) (21320, 1) (100, 35, 4)\n",
      "Epoch 1/20\n",
      "667/667 [==============================] - 16s 18ms/step - loss: 1808.2336 - val_loss: 688.3946\n",
      "Epoch 2/20\n",
      "667/667 [==============================] - 11s 17ms/step - loss: 480.7479 - val_loss: 564.0656\n",
      "Epoch 3/20\n",
      "667/667 [==============================] - 11s 16ms/step - loss: 429.9077 - val_loss: 438.1569\n",
      "Epoch 4/20\n",
      "667/667 [==============================] - 11s 16ms/step - loss: 410.3593 - val_loss: 550.0168\n",
      "Epoch 5/20\n",
      "667/667 [==============================] - 11s 17ms/step - loss: 368.7471 - val_loss: 648.6569\n",
      "Epoch 6/20\n",
      "667/667 [==============================] - 11s 16ms/step - loss: 358.0005 - val_loss: 493.2839\n",
      "Epoch 7/20\n",
      "667/667 [==============================] - 11s 17ms/step - loss: 344.8911 - val_loss: 417.8543\n",
      "Epoch 8/20\n",
      "667/667 [==============================] - 11s 16ms/step - loss: 322.1738 - val_loss: 350.9323\n",
      "Epoch 9/20\n",
      "667/667 [==============================] - 11s 17ms/step - loss: 315.0339 - val_loss: 340.7748\n",
      "Epoch 10/20\n",
      "667/667 [==============================] - 11s 17ms/step - loss: 312.6562 - val_loss: 349.6197\n",
      "Epoch 11/20\n",
      "667/667 [==============================] - 11s 17ms/step - loss: 293.5762 - val_loss: 319.7094\n",
      "Epoch 12/20\n",
      "667/667 [==============================] - 12s 18ms/step - loss: 283.8935 - val_loss: 327.4119\n",
      "Epoch 13/20\n",
      "667/667 [==============================] - 11s 17ms/step - loss: 281.2738 - val_loss: 317.6675\n",
      "Epoch 14/20\n",
      "667/667 [==============================] - 11s 17ms/step - loss: 274.1081 - val_loss: 477.7566\n",
      "Epoch 15/20\n",
      "667/667 [==============================] - 12s 17ms/step - loss: 264.5999 - val_loss: 321.8329\n",
      "Epoch 16/20\n",
      "667/667 [==============================] - 11s 17ms/step - loss: 260.8832 - val_loss: 256.3695\n",
      "Epoch 17/20\n",
      "667/667 [==============================] - 11s 17ms/step - loss: 250.1403 - val_loss: 404.0360\n",
      "Epoch 18/20\n",
      "667/667 [==============================] - 11s 17ms/step - loss: 247.2601 - val_loss: 256.8044\n",
      "Epoch 19/20\n",
      "667/667 [==============================] - 11s 16ms/step - loss: 244.9365 - val_loss: 260.0985\n",
      "Epoch 20/20\n",
      "667/667 [==============================] - 9s 13ms/step - loss: 240.2093 - val_loss: 307.7940\n",
      "4/4 [==============================] - 1s 9ms/step\n",
      "(21320, 35, 4) (21320, 1) (100, 35, 4)\n",
      "Epoch 1/20\n",
      "667/667 [==============================] - 16s 19ms/step - loss: 1974.3760 - val_loss: 1484.8225\n",
      "Epoch 2/20\n",
      "667/667 [==============================] - 11s 17ms/step - loss: 586.8684 - val_loss: 527.5994\n",
      "Epoch 3/20\n",
      "667/667 [==============================] - 11s 17ms/step - loss: 444.5235 - val_loss: 494.3494\n",
      "Epoch 4/20\n",
      "667/667 [==============================] - 11s 16ms/step - loss: 379.9441 - val_loss: 552.1577\n",
      "Epoch 5/20\n",
      "667/667 [==============================] - 7s 10ms/step - loss: 343.0808 - val_loss: 388.0841\n",
      "Epoch 6/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 329.5479 - val_loss: 363.9099\n",
      "Epoch 7/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 315.8494 - val_loss: 420.7811\n",
      "Epoch 8/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 294.5356 - val_loss: 331.2968\n",
      "Epoch 9/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 286.7430 - val_loss: 350.2513\n",
      "Epoch 10/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 274.9318 - val_loss: 283.6338\n",
      "Epoch 11/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 256.0194 - val_loss: 317.7461\n",
      "Epoch 12/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 251.4704 - val_loss: 287.0662\n",
      "Epoch 13/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 241.7975 - val_loss: 259.8074\n",
      "Epoch 14/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 234.8865 - val_loss: 284.9283\n",
      "Epoch 15/20\n",
      "667/667 [==============================] - 4s 7ms/step - loss: 229.3811 - val_loss: 275.9731\n",
      "Epoch 16/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 221.0170 - val_loss: 256.2910\n",
      "Epoch 17/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 215.5258 - val_loss: 335.3699\n",
      "Epoch 18/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 208.0688 - val_loss: 321.0112\n",
      "Epoch 19/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 200.7096 - val_loss: 281.7481\n",
      "Epoch 20/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 201.0685 - val_loss: 236.2471\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "(21320, 35, 4) (21320, 1) (100, 35, 4)\n",
      "Epoch 1/20\n",
      "667/667 [==============================] - 6s 7ms/step - loss: 1891.7097 - val_loss: 831.1437\n",
      "Epoch 2/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 507.3926 - val_loss: 452.9500\n",
      "Epoch 3/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 419.9549 - val_loss: 507.1910\n",
      "Epoch 4/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 372.9388 - val_loss: 390.7617\n",
      "Epoch 5/20\n",
      "667/667 [==============================] - 4s 7ms/step - loss: 352.7392 - val_loss: 363.2605\n",
      "Epoch 6/20\n",
      "667/667 [==============================] - 4s 7ms/step - loss: 335.8202 - val_loss: 467.6998\n",
      "Epoch 7/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 331.2192 - val_loss: 396.2831\n",
      "Epoch 8/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 316.6593 - val_loss: 355.2126\n",
      "Epoch 9/20\n",
      "667/667 [==============================] - 4s 7ms/step - loss: 307.2317 - val_loss: 337.4181\n",
      "Epoch 10/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 298.3001 - val_loss: 326.1134\n",
      "Epoch 11/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 290.7828 - val_loss: 331.1071\n",
      "Epoch 12/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 283.3362 - val_loss: 389.2519\n",
      "Epoch 13/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 281.3502 - val_loss: 284.6884\n",
      "Epoch 14/20\n",
      "667/667 [==============================] - 4s 7ms/step - loss: 265.6040 - val_loss: 327.9617\n",
      "Epoch 15/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 250.6690 - val_loss: 252.5267\n",
      "Epoch 16/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 238.6856 - val_loss: 298.3463\n",
      "Epoch 17/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 237.3575 - val_loss: 262.2495\n",
      "Epoch 18/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 223.3772 - val_loss: 257.6094\n",
      "Epoch 19/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 213.1578 - val_loss: 234.6820\n",
      "Epoch 20/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 206.6622 - val_loss: 236.8971\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "(21320, 35, 4) (21320, 1) (100, 35, 4)\n",
      "Epoch 1/20\n",
      "667/667 [==============================] - 7s 8ms/step - loss: 1946.1666 - val_loss: 901.6435\n",
      "Epoch 2/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 533.3754 - val_loss: 483.0836\n",
      "Epoch 3/20\n",
      "667/667 [==============================] - 4s 7ms/step - loss: 457.0432 - val_loss: 674.0930\n",
      "Epoch 4/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 408.7544 - val_loss: 461.8813\n",
      "Epoch 5/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 372.6505 - val_loss: 409.9823\n",
      "Epoch 6/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 349.9193 - val_loss: 370.6347\n",
      "Epoch 7/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 323.0254 - val_loss: 624.5389\n",
      "Epoch 8/20\n",
      "667/667 [==============================] - 4s 7ms/step - loss: 316.7932 - val_loss: 387.0170\n",
      "Epoch 9/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 305.0361 - val_loss: 318.7365\n",
      "Epoch 10/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 298.3347 - val_loss: 336.0472\n",
      "Epoch 11/20\n",
      "667/667 [==============================] - 4s 7ms/step - loss: 288.1649 - val_loss: 326.9075\n",
      "Epoch 12/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 276.7381 - val_loss: 353.5851\n",
      "Epoch 13/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 273.0336 - val_loss: 303.7235\n",
      "Epoch 14/20\n",
      "667/667 [==============================] - 4s 7ms/step - loss: 266.7534 - val_loss: 288.1896\n",
      "Epoch 15/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 247.5994 - val_loss: 253.5816\n",
      "Epoch 16/20\n",
      "667/667 [==============================] - 4s 7ms/step - loss: 237.2807 - val_loss: 275.4980\n",
      "Epoch 17/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 229.4691 - val_loss: 258.5862\n",
      "Epoch 18/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 217.2733 - val_loss: 275.7996\n",
      "Epoch 19/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 210.2603 - val_loss: 243.0650\n",
      "Epoch 20/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 202.8400 - val_loss: 252.5667\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "(21320, 35, 4) (21320, 1) (100, 35, 4)\n",
      "Epoch 1/20\n",
      "667/667 [==============================] - 6s 7ms/step - loss: 1994.3895 - val_loss: 1521.3333\n",
      "Epoch 2/20\n",
      "667/667 [==============================] - 4s 7ms/step - loss: 603.6851 - val_loss: 766.9236\n",
      "Epoch 3/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 448.1953 - val_loss: 448.8361\n",
      "Epoch 4/20\n",
      "667/667 [==============================] - 4s 7ms/step - loss: 409.1146 - val_loss: 619.3998\n",
      "Epoch 5/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 365.4449 - val_loss: 448.2709\n",
      "Epoch 6/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 336.3042 - val_loss: 330.3365\n",
      "Epoch 7/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 314.7581 - val_loss: 334.9813\n",
      "Epoch 8/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 297.1888 - val_loss: 331.8755\n",
      "Epoch 9/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 290.6132 - val_loss: 332.2495\n",
      "Epoch 10/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 272.0967 - val_loss: 297.7458\n",
      "Epoch 11/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 267.3059 - val_loss: 269.4163\n",
      "Epoch 12/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 261.9240 - val_loss: 273.8081\n",
      "Epoch 13/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 247.8881 - val_loss: 249.1155\n",
      "Epoch 14/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 241.2299 - val_loss: 291.5070\n",
      "Epoch 15/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 231.6521 - val_loss: 241.4872\n",
      "Epoch 16/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 219.1097 - val_loss: 238.1659\n",
      "Epoch 17/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 209.8290 - val_loss: 260.2908\n",
      "Epoch 18/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 204.5514 - val_loss: 249.9440\n",
      "Epoch 19/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 199.1101 - val_loss: 229.8717\n",
      "Epoch 20/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 191.8908 - val_loss: 253.2635\n",
      "4/4 [==============================] - 1s 1ms/step\n",
      "CPU times: total: 4min 7s\n",
      "Wall time: 10min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# \tRMSE\tstd_RMSE\tS_score\tstd_S_score\tMSE\tstd_MSE\talpha\tnodes\tdropout\tactivation\tbatch_size\tTW\n",
    "# 43\t15.404917\t0.0\t8.255290e+02\t0.0\t237.311462\t0.0\t0.10\t[64]\t0.2\ttanh\t32\t35\n",
    "results003 = pd.DataFrame()\n",
    "\n",
    "for SEED in range(5):  \n",
    "    tf.random.set_seed(SEED)\n",
    "    mse = []\n",
    "    R2_val = []\n",
    "    RMSE = []\n",
    "    score_val = []\n",
    "    \n",
    "    # 0.20\t[64]\t0.3\ttanh\t32\t25\n",
    "    \n",
    "    # parameter's sample\n",
    "    # weights_file = \"weights_file_lstm_optimalmodel_clv.h5\"\n",
    "    alpha = 0.1\n",
    "    sequence_length = 35\n",
    "    epochs = 20\n",
    "    nodes_per_layer = [64]\n",
    "    dropout = 0.2\n",
    "    activation = 'tanh'\n",
    "    batch_size = 32\n",
    "    remaining_sensors = remaining_sensors\n",
    "    # create model\n",
    "    input_shape = (sequence_length, len(remaining_sensors))    \n",
    "    # Data prepration\n",
    "    X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "\n",
    "    # create sequences train, test\n",
    "    train_array = gen_data_wrapper(X_train_interim, sequence_length,remaining_sensors)\n",
    "    label_array = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'])\n",
    "\n",
    "    test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,remaining_sensors, -99.))\n",
    "               for unit_nr in X_test_interim['Unit'].unique())\n",
    "    \n",
    "    test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "    test_rul = rul_piecewise_fct(y_test,rul_piecewise)\n",
    "    print(train_array.shape, label_array.shape, test_array.shape)\n",
    "    model = model_lstm_1layer(input_shape, nodes_per_layer[0], dropout, activation)\n",
    "       \n",
    "    # Model fitting\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        start_time = time.time()\n",
    "        weights_file = model.get_weights()\n",
    "        model.set_weights(weights_file)  # reset optimizer and node weights before every training iteration\n",
    "        history = model.fit(train_array, label_array,\n",
    "                                validation_data=(test_array, test_rul),\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                # callbacks=[cb],\n",
    "                                verbose=1)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        training_time = end_time - start_time\n",
    "        mse.append(history.history['val_loss'][-1])\n",
    "\n",
    "        y_hat_val_split = model.predict(test_array)\n",
    "        R2_val.append(r2_score(test_rul, y_hat_val_split))\n",
    "        RMSE.append(np.sqrt(mean_squared_error(test_rul, y_hat_val_split)))\n",
    "        score_val.append(compute_s_score(test_rul, y_hat_val_split))\n",
    "            \n",
    "        \n",
    "    #  append results\n",
    "    d = {'RMSE' :np.mean(RMSE), 'std_RMSE' :np.std(RMSE),\n",
    "         'S_score' :np.mean(score_val), 'std_S_score' :np.std(score_val),\n",
    "         'MSE':np.mean(mse), 'std_MSE':np.std(mse),\n",
    "         'nodes':str(nodes_per_layer), 'dropout':dropout, \n",
    "         'activation':activation, 'batch_size':batch_size, 'TW' : sequence_length, \n",
    "         'time': training_time}\n",
    "\n",
    "#     results = results.append(pd.DataFrame(d, index=[0]), ignore_index=True)\n",
    "    results003 = pd.concat([results003, pd.DataFrame(d, index=[0])], ignore_index=True)\n",
    "    results003.to_csv('results/base/fd003.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>std_RMSE</th>\n",
       "      <th>S_score</th>\n",
       "      <th>std_S_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>TW</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.544059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1048.675473</td>\n",
       "      <td>0.0</td>\n",
       "      <td>307.794037</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "      <td>227.002639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.370333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1056.089298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>236.247131</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "      <td>128.112133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.391460</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1027.064005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>236.897064</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "      <td>93.091394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15.892348</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1188.062839</td>\n",
       "      <td>0.0</td>\n",
       "      <td>252.566742</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "      <td>93.201190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15.914256</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1162.119404</td>\n",
       "      <td>0.0</td>\n",
       "      <td>253.263519</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "      <td>92.851058</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RMSE  std_RMSE      S_score  std_S_score         MSE  std_MSE nodes  \\\n",
       "0  17.544059       0.0  1048.675473          0.0  307.794037      0.0  [64]   \n",
       "1  15.370333       0.0  1056.089298          0.0  236.247131      0.0  [64]   \n",
       "2  15.391460       0.0  1027.064005          0.0  236.897064      0.0  [64]   \n",
       "3  15.892348       0.0  1188.062839          0.0  252.566742      0.0  [64]   \n",
       "4  15.914256       0.0  1162.119404          0.0  253.263519      0.0  [64]   \n",
       "\n",
       "   dropout activation  batch_size  TW        time  \n",
       "0      0.2       tanh          32  35  227.002639  \n",
       "1      0.2       tanh          32  35  128.112133  \n",
       "2      0.2       tanh          32  35   93.091394  \n",
       "3      0.2       tanh          32  35   93.201190  \n",
       "4      0.2       tanh          32  35   92.851058  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FD001 <a class=\"anchor\" id=\"fd001\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20631, 27) (13096, 26) (100, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# Load data and preprocess\n",
    "train, test, y_test = prepare_data('FD001.txt')\n",
    "print(train.shape, test.shape, y_test.shape)\n",
    "sensor_names = ['T20','T24','T30','T50','P20','P15','P30','Nf','Nc','epr','Ps30','phi',\n",
    "                    'NRf','NRc','BPR','farB','htBleed','Nf_dmd','PCNfR_dmd','W31','W32']\n",
    "remaining_sensors = [\"Ps30\", \"Nf\", \"P30\", \"BPR\"] # selection based on main_notebook \n",
    "drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "\n",
    "rul_piecewise = 125\n",
    "train['RUL'].clip(upper=rul_piecewise, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17731, 30, 4) (17731, 1) (100, 30, 4)\n",
      "Epoch 1/20\n",
      "278/278 [==============================] - 6s 15ms/step - loss: 2094.1931 - val_loss: 1618.0138\n",
      "Epoch 2/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 1218.1819 - val_loss: 704.2067\n",
      "Epoch 3/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 671.6970 - val_loss: 451.3782\n",
      "Epoch 4/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 414.8923 - val_loss: 341.0276\n",
      "Epoch 5/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 351.7847 - val_loss: 256.1475\n",
      "Epoch 6/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 309.1059 - val_loss: 297.0459\n",
      "Epoch 7/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 292.2281 - val_loss: 203.4271\n",
      "Epoch 8/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 287.8505 - val_loss: 208.9244\n",
      "Epoch 9/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 272.1981 - val_loss: 199.2759\n",
      "Epoch 10/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 269.6938 - val_loss: 230.4333\n",
      "Epoch 11/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 268.8228 - val_loss: 199.3822\n",
      "Epoch 12/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 266.5687 - val_loss: 253.0650\n",
      "Epoch 13/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 260.7627 - val_loss: 202.6526\n",
      "Epoch 14/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 257.8602 - val_loss: 294.3881\n",
      "Epoch 15/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 268.7884 - val_loss: 221.9613\n",
      "Epoch 16/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 258.2159 - val_loss: 210.5917\n",
      "Epoch 17/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 263.0499 - val_loss: 227.5882\n",
      "Epoch 18/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 257.5280 - val_loss: 184.8262\n",
      "Epoch 19/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 257.3256 - val_loss: 514.9196\n",
      "Epoch 20/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 258.0364 - val_loss: 187.9990\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "(17731, 30, 4) (17731, 1) (100, 30, 4)\n",
      "Epoch 1/20\n",
      "278/278 [==============================] - 6s 16ms/step - loss: 2078.9536 - val_loss: 1606.9803\n",
      "Epoch 2/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 1219.7988 - val_loss: 747.9568\n",
      "Epoch 3/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 656.5989 - val_loss: 1891.0814\n",
      "Epoch 4/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 489.5693 - val_loss: 380.1104\n",
      "Epoch 5/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 359.1964 - val_loss: 257.2903\n",
      "Epoch 6/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 307.6697 - val_loss: 228.8339\n",
      "Epoch 7/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 302.0051 - val_loss: 365.8041\n",
      "Epoch 8/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 291.2435 - val_loss: 231.3754\n",
      "Epoch 9/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 285.0749 - val_loss: 248.8928\n",
      "Epoch 10/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 276.4378 - val_loss: 307.5818\n",
      "Epoch 11/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 277.4029 - val_loss: 237.1333\n",
      "Epoch 12/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 268.1517 - val_loss: 218.2430\n",
      "Epoch 13/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 270.2302 - val_loss: 270.6755\n",
      "Epoch 14/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 262.8786 - val_loss: 205.1135\n",
      "Epoch 15/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 260.2126 - val_loss: 339.2585\n",
      "Epoch 16/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 257.0455 - val_loss: 220.3410\n",
      "Epoch 17/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 259.1434 - val_loss: 222.7914\n",
      "Epoch 18/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 256.9933 - val_loss: 200.4616\n",
      "Epoch 19/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 250.3539 - val_loss: 297.5879\n",
      "Epoch 20/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 254.5756 - val_loss: 220.6628\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "(17731, 30, 4) (17731, 1) (100, 30, 4)\n",
      "Epoch 1/20\n",
      "278/278 [==============================] - 5s 14ms/step - loss: 2094.0459 - val_loss: 1729.9303\n",
      "Epoch 2/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 1309.6001 - val_loss: 750.0071\n",
      "Epoch 3/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 749.3345 - val_loss: 693.9478\n",
      "Epoch 4/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 624.6177 - val_loss: 624.2625\n",
      "Epoch 5/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 396.1165 - val_loss: 571.0096\n",
      "Epoch 6/20\n",
      "278/278 [==============================] - 5s 16ms/step - loss: 383.1449 - val_loss: 556.1239\n",
      "Epoch 7/20\n",
      "278/278 [==============================] - 4s 13ms/step - loss: 361.3888 - val_loss: 296.1885\n",
      "Epoch 8/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 294.9594 - val_loss: 208.6813\n",
      "Epoch 9/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 281.5725 - val_loss: 231.6975\n",
      "Epoch 10/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 279.0105 - val_loss: 236.5249\n",
      "Epoch 11/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 275.4070 - val_loss: 204.9567\n",
      "Epoch 12/20\n",
      "278/278 [==============================] - 4s 13ms/step - loss: 275.3206 - val_loss: 226.2235\n",
      "Epoch 13/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 272.7993 - val_loss: 213.2337\n",
      "Epoch 14/20\n",
      "278/278 [==============================] - 4s 13ms/step - loss: 264.8187 - val_loss: 235.5180\n",
      "Epoch 15/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 267.7016 - val_loss: 202.3827\n",
      "Epoch 16/20\n",
      "278/278 [==============================] - 4s 13ms/step - loss: 272.6542 - val_loss: 267.1535\n",
      "Epoch 17/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 275.1472 - val_loss: 261.9128\n",
      "Epoch 18/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 260.4059 - val_loss: 212.1127\n",
      "Epoch 19/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 258.1665 - val_loss: 215.2800\n",
      "Epoch 20/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 252.7763 - val_loss: 203.0189\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "(17731, 30, 4) (17731, 1) (100, 30, 4)\n",
      "Epoch 1/20\n",
      "278/278 [==============================] - 6s 15ms/step - loss: 2110.8745 - val_loss: 1565.2750\n",
      "Epoch 2/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 1210.1388 - val_loss: 745.7397\n",
      "Epoch 3/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 729.4255 - val_loss: 692.4369\n",
      "Epoch 4/20\n",
      "278/278 [==============================] - 4s 13ms/step - loss: 507.6820 - val_loss: 366.0959\n",
      "Epoch 5/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 360.7318 - val_loss: 304.0369\n",
      "Epoch 6/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 350.4071 - val_loss: 310.2050\n",
      "Epoch 7/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 307.1693 - val_loss: 320.1892\n",
      "Epoch 8/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 301.5501 - val_loss: 321.2110\n",
      "Epoch 9/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 290.2199 - val_loss: 224.8135\n",
      "Epoch 10/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 286.7387 - val_loss: 213.6616\n",
      "Epoch 11/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 274.9398 - val_loss: 210.9548\n",
      "Epoch 12/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 277.9974 - val_loss: 347.6423\n",
      "Epoch 13/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 269.4263 - val_loss: 276.3042\n",
      "Epoch 14/20\n",
      "278/278 [==============================] - 4s 13ms/step - loss: 273.5287 - val_loss: 234.4571\n",
      "Epoch 15/20\n",
      "278/278 [==============================] - 4s 13ms/step - loss: 273.4059 - val_loss: 272.8022\n",
      "Epoch 16/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 270.8100 - val_loss: 218.2015\n",
      "Epoch 17/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 275.4136 - val_loss: 209.2249\n",
      "Epoch 18/20\n",
      "278/278 [==============================] - 4s 13ms/step - loss: 270.1133 - val_loss: 212.7900\n",
      "Epoch 19/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 267.8000 - val_loss: 209.7949\n",
      "Epoch 20/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 261.6378 - val_loss: 211.5235\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "(17731, 30, 4) (17731, 1) (100, 30, 4)\n",
      "Epoch 1/20\n",
      "278/278 [==============================] - 6s 16ms/step - loss: 2124.2063 - val_loss: 1588.0614\n",
      "Epoch 2/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 1243.9152 - val_loss: 738.4001\n",
      "Epoch 3/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 704.0798 - val_loss: 578.8834\n",
      "Epoch 4/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 430.2003 - val_loss: 236.2921\n",
      "Epoch 5/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 335.2400 - val_loss: 1091.2075\n",
      "Epoch 6/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 341.1683 - val_loss: 279.3336\n",
      "Epoch 7/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 277.5127 - val_loss: 322.5653\n",
      "Epoch 8/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 280.8879 - val_loss: 276.1951\n",
      "Epoch 9/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 273.4796 - val_loss: 210.1035\n",
      "Epoch 10/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 270.3138 - val_loss: 221.2235\n",
      "Epoch 11/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 267.1096 - val_loss: 255.1645\n",
      "Epoch 12/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 262.3855 - val_loss: 218.8061\n",
      "Epoch 13/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 255.1153 - val_loss: 215.5038\n",
      "Epoch 14/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 253.6401 - val_loss: 203.3079\n",
      "Epoch 15/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 249.5297 - val_loss: 191.7167\n",
      "Epoch 16/20\n",
      "278/278 [==============================] - 4s 13ms/step - loss: 252.2125 - val_loss: 194.6310\n",
      "Epoch 17/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 244.1014 - val_loss: 217.1675\n",
      "Epoch 18/20\n",
      "278/278 [==============================] - 4s 13ms/step - loss: 244.7660 - val_loss: 221.9786\n",
      "Epoch 19/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 246.7326 - val_loss: 251.0719\n",
      "Epoch 20/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 242.1459 - val_loss: 206.3216\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "CPU times: total: 1min 36s\n",
      "Wall time: 6min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# RMSE\tstd_RMSE\tS_score\tstd_S_score\tMSE\tstd_MSE\talpha\tnodes\tdropout\tactivation\tbatch_size\tTW\n",
    "# 21\t13.768992\t0.0\t3.830708e+02\t0.0\t189.585159\t0.0\t0.10\t[128]\t0.2\ttanh\t64\t30\n",
    "results001 = pd.DataFrame()\n",
    "\n",
    "for SEED in range(5):  \n",
    "    set_seed(SEED)\n",
    "    mse = []\n",
    "    R2_val = []\n",
    "    RMSE = []\n",
    "    score_val = []\n",
    "    \n",
    "    # 0.20\t[64]\t0.3\ttanh\t32\t25\n",
    "    \n",
    "    # parameter's sample\n",
    "    # weights_file = \"weights_file_lstm_optimalmodel_clv.h5\"\n",
    "    alpha = 0.1\n",
    "    sequence_length = 30\n",
    "    epochs = 20\n",
    "    nodes_per_layer = [128]\n",
    "    dropout = 0.2\n",
    "    activation = 'tanh'\n",
    "    batch_size = 64\n",
    "    remaining_sensors = remaining_sensors\n",
    "    # create model\n",
    "    input_shape = (sequence_length, len(remaining_sensors))    \n",
    "    # Data prepration\n",
    "    X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "\n",
    "    # create sequences train, test\n",
    "    train_array = gen_data_wrapper(X_train_interim, sequence_length,remaining_sensors)\n",
    "    label_array = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'])\n",
    "\n",
    "    test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,remaining_sensors, -99.))\n",
    "               for unit_nr in X_test_interim['Unit'].unique())\n",
    "    \n",
    "    test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "    test_rul = rul_piecewise_fct(y_test,rul_piecewise)\n",
    "    print(train_array.shape, label_array.shape, test_array.shape)\n",
    "    model = model_lstm_1layer(input_shape, nodes_per_layer[0], dropout, activation)\n",
    "       \n",
    "    # Model fitting\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        start_time = time.time()\n",
    "        weights_file = model.get_weights()\n",
    "        model.set_weights(weights_file)  # reset optimizer and node weights before every training iteration\n",
    "        history = model.fit(train_array, label_array,\n",
    "                                validation_data=(test_array, test_rul),\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                # callbacks=[cb],\n",
    "                                verbose=1)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        training_time = end_time - start_time\n",
    "        mse.append(history.history['val_loss'][-1])\n",
    "\n",
    "        y_hat_val_split = model.predict(test_array)\n",
    "        R2_val.append(r2_score(test_rul, y_hat_val_split))\n",
    "        RMSE.append(np.sqrt(mean_squared_error(test_rul, y_hat_val_split)))\n",
    "        score_val.append(compute_s_score(test_rul, y_hat_val_split))\n",
    "            \n",
    "        \n",
    "    #  append results\n",
    "    d = {'RMSE' :np.mean(RMSE), 'std_RMSE' :np.std(RMSE),\n",
    "         'S_score' :np.mean(score_val), 'std_S_score' :np.std(score_val),\n",
    "         'MSE':np.mean(mse), 'std_MSE':np.std(mse),\n",
    "         'nodes':str(nodes_per_layer), 'dropout':dropout, \n",
    "         'activation':activation, 'batch_size':batch_size, 'TW' : sequence_length, \n",
    "         'time': training_time}\n",
    "\n",
    "#     results = results.append(pd.DataFrame(d, index=[0]), ignore_index=True)\n",
    "    results001 = pd.concat([results001, pd.DataFrame(d, index=[0])], ignore_index=True)\n",
    "    results001.to_csv('results/base/fd001.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>std_RMSE</th>\n",
       "      <th>S_score</th>\n",
       "      <th>std_S_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>TW</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.711273</td>\n",
       "      <td>0.0</td>\n",
       "      <td>345.463846</td>\n",
       "      <td>0.0</td>\n",
       "      <td>187.998978</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "      <td>81.581375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.854724</td>\n",
       "      <td>0.0</td>\n",
       "      <td>364.273577</td>\n",
       "      <td>0.0</td>\n",
       "      <td>220.662811</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "      <td>82.244397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.248470</td>\n",
       "      <td>0.0</td>\n",
       "      <td>362.494277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>203.018890</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "      <td>79.451121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.543846</td>\n",
       "      <td>0.0</td>\n",
       "      <td>458.032573</td>\n",
       "      <td>0.0</td>\n",
       "      <td>211.523483</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "      <td>79.214682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14.363899</td>\n",
       "      <td>0.0</td>\n",
       "      <td>437.720490</td>\n",
       "      <td>0.0</td>\n",
       "      <td>206.321594</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "      <td>80.568281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RMSE  std_RMSE     S_score  std_S_score         MSE  std_MSE  nodes  \\\n",
       "0  13.711273       0.0  345.463846          0.0  187.998978      0.0  [128]   \n",
       "1  14.854724       0.0  364.273577          0.0  220.662811      0.0  [128]   \n",
       "2  14.248470       0.0  362.494277          0.0  203.018890      0.0  [128]   \n",
       "3  14.543846       0.0  458.032573          0.0  211.523483      0.0  [128]   \n",
       "4  14.363899       0.0  437.720490          0.0  206.321594      0.0  [128]   \n",
       "\n",
       "   dropout activation  batch_size  TW       time  \n",
       "0      0.2       tanh          64  30  81.581375  \n",
       "1      0.2       tanh          64  30  82.244397  \n",
       "2      0.2       tanh          64  30  79.451121  \n",
       "3      0.2       tanh          64  30  79.214682  \n",
       "4      0.2       tanh          64  30  80.568281  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results001"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
