{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sommaire: <a class=\"anchor\" id=\"sommaire\"></a>\n",
    "* [Sommaire](#sommaire)\n",
    "* [Package Loading](#package)\n",
    "* [Model selection](#model_selection)\n",
    "    * [FD001](#fd001)\n",
    "    * [FD002](#fd002)\n",
    "    * [FD003](#fd003)\n",
    "    * [FD004](#fd004)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package's loading <a id='package'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "from lime.lime_tabular import RecurrentTabularExplainer\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error, r2_score \n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn import preprocessing\n",
    "from keras import backend as K\n",
    "from sklearn.preprocessing import MinMaxScaler , StandardScaler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Activation, GRU\n",
    "from scipy import optimize\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "\n",
    "from sp_modif.model_function import *\n",
    "from sp_modif.methods import *\n",
    "from sp_modif.data_prep import *\n",
    "from sp_modif.evaluator import *\n",
    "from sp_modif.SHAP import *\n",
    "from sp_modif.L2X import *\n",
    "from methods import *\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEED = 0\n",
    "def set_seed(seed=SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    tf.random.set_seed(SEED)\n",
    "\n",
    "# Appeler la fonction pour fixer le seed\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FD004 <a class=\"anchor\" id=\"fd004\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61249, 27) (41214, 26) (248, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# Load data and preprocess\n",
    "with tf.device('/device:GPU:0'):\n",
    "    train, test, y_test = prepare_data('FD004.txt')\n",
    "    print(train.shape, test.shape, y_test.shape)\n",
    "    sensor_names = ['T20','T24','T30','T50','P20','P15','P30','Nf','Nc','epr','Ps30','phi',\n",
    "                    'NRf','NRc','BPR','farB','htBleed','Nf_dmd','PCNfR_dmd','W31','W32']\n",
    "\n",
    "    remaining_sensors = [\"Ps30\", \"Nf\", \"phi\", \"BPR\", \"farB\"] # selection based on main_notebook\n",
    "\n",
    "    drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "\n",
    "rul_piecewise = 120\n",
    "train['RUL'].clip(upper=rul_piecewise, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1layers\n",
    "def model_lstm_1layer(input_shape, nodes_per_layer, dropout, activation):\n",
    "    \n",
    "    cb = keras.callbacks.EarlyStopping(monitor='loss', patience=4)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units = nodes_per_layer, activation=activation, \n",
    "                  input_shape=input_shape))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(256))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer=Adam(learning_rate=0.001))\n",
    "    # model.save_weights(weights_file)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51538, 40, 5) (51538, 1) (248, 40, 5)\n",
      "Epoch 1/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 1007.3642 - val_loss: 1328.0765\n",
      "Epoch 2/20\n",
      "1611/1611 [==============================] - 10s 6ms/step - loss: 455.5491 - val_loss: 903.4106\n",
      "Epoch 3/20\n",
      "1611/1611 [==============================] - 10s 7ms/step - loss: 363.3373 - val_loss: 509.7916\n",
      "Epoch 4/20\n",
      "1611/1611 [==============================] - 10s 6ms/step - loss: 314.2787 - val_loss: 459.2633\n",
      "Epoch 5/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 298.5221 - val_loss: 375.7044\n",
      "Epoch 6/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 279.5657 - val_loss: 393.4356\n",
      "Epoch 7/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 259.8807 - val_loss: 302.1973\n",
      "Epoch 8/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 243.9541 - val_loss: 319.0983\n",
      "Epoch 9/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 233.7947 - val_loss: 295.6146\n",
      "Epoch 10/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 227.2847 - val_loss: 291.1745\n",
      "Epoch 11/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 220.3006 - val_loss: 286.0516\n",
      "Epoch 12/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 217.3517 - val_loss: 304.7195\n",
      "Epoch 13/20\n",
      "1611/1611 [==============================] - 12s 8ms/step - loss: 212.4227 - val_loss: 287.5859\n",
      "Epoch 14/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 208.4663 - val_loss: 251.2137\n",
      "Epoch 15/20\n",
      "1611/1611 [==============================] - 12s 8ms/step - loss: 206.7656 - val_loss: 278.4070\n",
      "Epoch 16/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 205.7390 - val_loss: 250.2789\n",
      "Epoch 17/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 204.4950 - val_loss: 259.0097\n",
      "Epoch 18/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 199.8225 - val_loss: 249.2148\n",
      "Epoch 19/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 200.3501 - val_loss: 238.8479\n",
      "Epoch 20/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 198.5592 - val_loss: 227.5038\n",
      "8/8 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51538, 40, 5) (51538, 1) (248, 40, 5)\n",
      "Epoch 1/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 994.6793 - val_loss: 1104.8896\n",
      "Epoch 2/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 449.0446 - val_loss: 805.1175\n",
      "Epoch 3/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 391.5176 - val_loss: 606.9626\n",
      "Epoch 4/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 316.0338 - val_loss: 329.8018\n",
      "Epoch 5/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 296.5393 - val_loss: 400.7310\n",
      "Epoch 6/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 278.9533 - val_loss: 327.1832\n",
      "Epoch 7/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 266.5649 - val_loss: 292.1869\n",
      "Epoch 8/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 252.3264 - val_loss: 257.4317\n",
      "Epoch 9/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 240.3982 - val_loss: 239.5925\n",
      "Epoch 10/20\n",
      "1611/1611 [==============================] - 12s 8ms/step - loss: 230.0259 - val_loss: 229.6958\n",
      "Epoch 11/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 222.8822 - val_loss: 228.2783\n",
      "Epoch 12/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 218.8563 - val_loss: 250.9755\n",
      "Epoch 13/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 213.9969 - val_loss: 246.3504\n",
      "Epoch 14/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 212.8851 - val_loss: 246.8406\n",
      "Epoch 15/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 208.7209 - val_loss: 235.4722\n",
      "Epoch 16/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 207.7997 - val_loss: 228.6382\n",
      "Epoch 17/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 205.2813 - val_loss: 225.9402\n",
      "Epoch 18/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 203.3288 - val_loss: 225.4365\n",
      "Epoch 19/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 202.2095 - val_loss: 229.7642\n",
      "Epoch 20/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 200.7182 - val_loss: 222.5234\n",
      "8/8 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51538, 40, 5) (51538, 1) (248, 40, 5)\n",
      "Epoch 1/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 1005.1373 - val_loss: 1119.7898\n",
      "Epoch 2/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 440.6535 - val_loss: 666.8292\n",
      "Epoch 3/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 375.2309 - val_loss: 324.8863\n",
      "Epoch 4/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 323.5302 - val_loss: 289.1518\n",
      "Epoch 5/20\n",
      "1611/1611 [==============================] - 12s 8ms/step - loss: 293.7157 - val_loss: 264.5674\n",
      "Epoch 6/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 276.4119 - val_loss: 273.2298\n",
      "Epoch 7/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 262.9656 - val_loss: 284.4217\n",
      "Epoch 8/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 248.2249 - val_loss: 258.8352\n",
      "Epoch 9/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 236.3213 - val_loss: 363.0738\n",
      "Epoch 10/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 228.0180 - val_loss: 349.1395\n",
      "Epoch 11/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 221.3067 - val_loss: 336.8487\n",
      "Epoch 12/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 218.6142 - val_loss: 340.7795\n",
      "Epoch 13/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 215.5271 - val_loss: 334.0074\n",
      "Epoch 14/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 213.7668 - val_loss: 316.3020\n",
      "Epoch 15/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 209.3597 - val_loss: 338.0477\n",
      "Epoch 16/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 209.3691 - val_loss: 377.7503\n",
      "Epoch 17/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 206.1440 - val_loss: 388.5368\n",
      "Epoch 18/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 204.3000 - val_loss: 375.1585\n",
      "Epoch 19/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 201.2736 - val_loss: 384.1866\n",
      "Epoch 20/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 201.5162 - val_loss: 361.9356\n",
      "8/8 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51538, 40, 5) (51538, 1) (248, 40, 5)\n",
      "Epoch 1/20\n",
      "1611/1611 [==============================] - 13s 8ms/step - loss: 981.6541 - val_loss: 564.4487\n",
      "Epoch 2/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 447.9604 - val_loss: 651.7050\n",
      "Epoch 3/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 383.2815 - val_loss: 370.7835\n",
      "Epoch 4/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 317.7560 - val_loss: 301.7954\n",
      "Epoch 5/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 298.4744 - val_loss: 333.3961\n",
      "Epoch 6/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 281.8705 - val_loss: 318.7209\n",
      "Epoch 7/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 265.3545 - val_loss: 269.8325\n",
      "Epoch 8/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 251.0957 - val_loss: 280.0313\n",
      "Epoch 9/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 237.1234 - val_loss: 290.8649\n",
      "Epoch 10/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 231.4159 - val_loss: 294.8712\n",
      "Epoch 11/20\n",
      "1611/1611 [==============================] - 13s 8ms/step - loss: 225.7532 - val_loss: 370.3519\n",
      "Epoch 12/20\n",
      "1611/1611 [==============================] - 12s 8ms/step - loss: 219.4212 - val_loss: 340.9488\n",
      "Epoch 13/20\n",
      "1611/1611 [==============================] - 12s 8ms/step - loss: 215.5828 - val_loss: 299.7004\n",
      "Epoch 14/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 214.3961 - val_loss: 298.3197\n",
      "Epoch 15/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 212.4666 - val_loss: 268.3527\n",
      "Epoch 16/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 209.3502 - val_loss: 287.8945\n",
      "Epoch 17/20\n",
      "1611/1611 [==============================] - 14s 8ms/step - loss: 208.4628 - val_loss: 296.6494\n",
      "Epoch 18/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 205.3557 - val_loss: 280.5874\n",
      "Epoch 19/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 205.5463 - val_loss: 260.2021\n",
      "Epoch 20/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 203.0585 - val_loss: 235.5063\n",
      "8/8 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51538, 40, 5) (51538, 1) (248, 40, 5)\n",
      "Epoch 1/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 1045.3427 - val_loss: 830.9023\n",
      "Epoch 2/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 442.2701 - val_loss: 860.0965\n",
      "Epoch 3/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 385.9459 - val_loss: 685.2007\n",
      "Epoch 4/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 318.3638 - val_loss: 593.9271\n",
      "Epoch 5/20\n",
      "1611/1611 [==============================] - 12s 8ms/step - loss: 295.5399 - val_loss: 344.9670\n",
      "Epoch 6/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 275.0363 - val_loss: 422.4583\n",
      "Epoch 7/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 259.7030 - val_loss: 371.8749\n",
      "Epoch 8/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 248.9927 - val_loss: 404.6609\n",
      "Epoch 9/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 240.0711 - val_loss: 468.7724\n",
      "Epoch 10/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 231.4482 - val_loss: 309.8862\n",
      "Epoch 11/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 225.8598 - val_loss: 258.4960\n",
      "Epoch 12/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 219.3717 - val_loss: 249.0052\n",
      "Epoch 13/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 217.1483 - val_loss: 257.4297\n",
      "Epoch 14/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 214.0240 - val_loss: 261.1975\n",
      "Epoch 15/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 210.1172 - val_loss: 247.8030\n",
      "Epoch 16/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 210.0839 - val_loss: 249.7852\n",
      "Epoch 17/20\n",
      "1611/1611 [==============================] - 12s 8ms/step - loss: 207.5825 - val_loss: 254.8071\n",
      "Epoch 18/20\n",
      "1611/1611 [==============================] - 12s 8ms/step - loss: 204.2165 - val_loss: 257.3855\n",
      "Epoch 19/20\n",
      "1611/1611 [==============================] - 12s 8ms/step - loss: 204.0810 - val_loss: 259.9808\n",
      "Epoch 20/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 202.8896 - val_loss: 245.0911\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "CPU times: user 19min 27s, sys: 59.9 s, total: 20min 27s\n",
      "Wall time: 19min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# RMSE\tstd_RMSE\tS_score\tstd_S_score\tMSE\tstd_MSE\tnodes\tdropout\tactivation\tbatch_size\tTW\n",
    "# 1\t15.449126\t0.0\t1.242658e+03\t0.0\t238.675491\t0.0\t[64]\t0.2\ttanh\t32\t40\n",
    "# RMSE\tstd_RMSE\tS_score\tstd_S_score\tMSE\tstd_MSE\talpha\tnodes\tdropout\tactivation\tbatch_size\tTW\n",
    "# 2\t16.100150\t0.0\t1.752538e+03\t0.0\t259.214813\t0.0\t0.30\t[64]\t0.2\ttanh\t32\t40\n",
    "results = pd.DataFrame()\n",
    "\n",
    "for SEED in range(5):  \n",
    "    tf.random.set_seed(SEED)\n",
    "    mse = []\n",
    "    R2_val = []\n",
    "    RMSE = []\n",
    "    score_val = []\n",
    "    \n",
    "    # 0.20\t[64]\t0.3\ttanh\t32\t25\n",
    "    \n",
    "    # parameter's sample\n",
    "    weights_file = \"weights_file_lstm_optimalmodel_clv.h5\"\n",
    "    alpha = 0.3\n",
    "    sequence_length = 40\n",
    "    epochs = 20\n",
    "    nodes_per_layer = [64]\n",
    "    dropout = 0.2\n",
    "    activation = 'tanh'\n",
    "    batch_size = 32\n",
    "    remaining_sensors = remaining_sensors\n",
    "    # create model\n",
    "    input_shape = (sequence_length, len(remaining_sensors))\n",
    "    model = model_lstm_1layer(input_shape, nodes_per_layer[0], dropout, activation)\n",
    "    \n",
    "    # Data prepration\n",
    "    X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "\n",
    "    # create sequences train, test\n",
    "    train_array = gen_data_wrapper(X_train_interim, sequence_length,remaining_sensors)\n",
    "    label_array = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'])\n",
    "\n",
    "    test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,remaining_sensors, -99.))\n",
    "               for unit_nr in X_test_interim['Unit'].unique())\n",
    "    \n",
    "    test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "    test_rul = rul_piecewise_fct(y_test,rul_piecewise)\n",
    "    print(train_array.shape, label_array.shape, test_array.shape)\n",
    "            \n",
    "    # Model fitting\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        start_time = time.time()\n",
    "        weights_file = model.get_weights()\n",
    "        model.set_weights(weights_file)  # reset optimizer and node weights before every training iteration\n",
    "        history = model.fit(train_array, label_array,\n",
    "                                validation_data=(test_array, test_rul),\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                # callbacks=[cb],\n",
    "                                verbose=1)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        training_time = end_time - start_time\n",
    "        mse.append(history.history['val_loss'][-1])\n",
    "\n",
    "        y_hat_val_split = model.predict(test_array)\n",
    "        R2_val.append(r2_score(test_rul, y_hat_val_split))\n",
    "        RMSE.append(np.sqrt(mean_squared_error(test_rul, y_hat_val_split)))\n",
    "        score_val.append(compute_s_score(test_rul, y_hat_val_split))\n",
    "            \n",
    "        \n",
    "    #  append results\n",
    "    d = {'RMSE' :np.mean(RMSE), 'std_RMSE' :np.std(RMSE),\n",
    "         'S_score' :np.mean(score_val), 'std_S_score' :np.std(score_val),\n",
    "         'MSE':np.mean(mse), 'std_MSE':np.std(mse),\n",
    "         'nodes':str(nodes_per_layer), 'dropout':dropout, \n",
    "         'activation':activation, 'batch_size':batch_size, 'TW' : sequence_length, \n",
    "         'time': training_time}\n",
    "\n",
    "#     results = results.append(pd.DataFrame(d, index=[0]), ignore_index=True)\n",
    "    results = pd.concat([results, pd.DataFrame(d, index=[0])], ignore_index=True)\n",
    "    results.to_csv('results/base/fd004.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>std_RMSE</th>\n",
       "      <th>S_score</th>\n",
       "      <th>std_S_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>TW</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15.083231</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1416.246871</td>\n",
       "      <td>0.0</td>\n",
       "      <td>227.503845</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>225.823854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.917217</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1350.649618</td>\n",
       "      <td>0.0</td>\n",
       "      <td>222.523392</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>229.871984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.024605</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2900.514974</td>\n",
       "      <td>0.0</td>\n",
       "      <td>361.935608</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>230.356510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15.346217</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1048.587734</td>\n",
       "      <td>0.0</td>\n",
       "      <td>235.506348</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>237.527807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15.655384</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1472.123818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>245.091095</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>235.396072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RMSE  std_RMSE      S_score  std_S_score         MSE  std_MSE nodes  \\\n",
       "0  15.083231       0.0  1416.246871          0.0  227.503845      0.0  [64]   \n",
       "1  14.917217       0.0  1350.649618          0.0  222.523392      0.0  [64]   \n",
       "2  19.024605       0.0  2900.514974          0.0  361.935608      0.0  [64]   \n",
       "3  15.346217       0.0  1048.587734          0.0  235.506348      0.0  [64]   \n",
       "4  15.655384       0.0  1472.123818          0.0  245.091095      0.0  [64]   \n",
       "\n",
       "   dropout activation  batch_size  TW        time  \n",
       "0      0.2       tanh          32  40  225.823854  \n",
       "1      0.2       tanh          32  40  229.871984  \n",
       "2      0.2       tanh          32  40  230.356510  \n",
       "3      0.2       tanh          32  40  237.527807  \n",
       "4      0.2       tanh          32  40  235.396072  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FD002 <a class=\"anchor\" id=\"fd002\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53759, 27) (33991, 26) (259, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# Load data and preprocess\n",
    "train, test, y_test = prepare_data('FD002.txt')\n",
    "print(train.shape, test.shape, y_test.shape)\n",
    "sensor_names = ['T20','T24','T30','T50','P20','P15','P30','Nf','Nc','epr','Ps30','phi',\n",
    "                    'NRf','NRc','BPR','farB','htBleed','Nf_dmd','PCNfR_dmd','W31','W32']\n",
    "remaining_sensors = [\"Ps30\", \"Nf\", \"phi\", \"epr\"] # selection based on main_notebook [1] \"Nf\" [1] \"Ps30\" [1] \"phi\"\n",
    "drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "\n",
    "rul_piecewise = 125\n",
    "train['RUL'].clip(upper=rul_piecewise, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43619, 40, 4) (43619, 1) (259, 40, 4)\n",
      "Epoch 1/20\n",
      "341/341 [==============================] - 4s 9ms/step - loss: 2236.6399 - val_loss: 860.0704\n",
      "Epoch 2/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 632.3915 - val_loss: 335.7599\n",
      "Epoch 3/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 446.0885 - val_loss: 323.0456\n",
      "Epoch 4/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 406.7784 - val_loss: 348.0060\n",
      "Epoch 5/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 380.7405 - val_loss: 302.4825\n",
      "Epoch 6/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 368.7476 - val_loss: 318.1848\n",
      "Epoch 7/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 360.7546 - val_loss: 289.3883\n",
      "Epoch 8/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 352.9236 - val_loss: 283.3387\n",
      "Epoch 9/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 341.6450 - val_loss: 286.5251\n",
      "Epoch 10/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 340.1089 - val_loss: 284.2105\n",
      "Epoch 11/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 335.2099 - val_loss: 275.2052\n",
      "Epoch 12/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 330.4894 - val_loss: 282.2795\n",
      "Epoch 13/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 326.6069 - val_loss: 312.6275\n",
      "Epoch 14/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 318.3952 - val_loss: 267.6027\n",
      "Epoch 15/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 318.8837 - val_loss: 308.0148\n",
      "Epoch 16/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 314.3348 - val_loss: 269.3747\n",
      "Epoch 17/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 314.1987 - val_loss: 269.2358\n",
      "Epoch 18/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 307.5088 - val_loss: 258.5591\n",
      "Epoch 19/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 303.5747 - val_loss: 279.6474\n",
      "Epoch 20/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 305.1863 - val_loss: 280.3040\n",
      "9/9 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43619, 40, 4) (43619, 1) (259, 40, 4)\n",
      "Epoch 1/20\n",
      "341/341 [==============================] - 4s 10ms/step - loss: 2412.6331 - val_loss: 1778.6327\n",
      "Epoch 2/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 1067.7266 - val_loss: 372.5059\n",
      "Epoch 3/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 423.9863 - val_loss: 360.1047\n",
      "Epoch 4/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 390.6329 - val_loss: 385.9352\n",
      "Epoch 5/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 370.3245 - val_loss: 422.2668\n",
      "Epoch 6/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 360.5595 - val_loss: 429.8504\n",
      "Epoch 7/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 351.3489 - val_loss: 394.6701\n",
      "Epoch 8/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 348.6140 - val_loss: 437.7506\n",
      "Epoch 9/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 342.6008 - val_loss: 416.0578\n",
      "Epoch 10/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 336.9779 - val_loss: 442.8977\n",
      "Epoch 11/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 334.8899 - val_loss: 446.9541\n",
      "Epoch 12/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 328.9974 - val_loss: 413.7572\n",
      "Epoch 13/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 327.8879 - val_loss: 454.8456\n",
      "Epoch 14/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 322.1981 - val_loss: 445.2818\n",
      "Epoch 15/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 321.5517 - val_loss: 431.6632\n",
      "Epoch 16/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 319.9282 - val_loss: 435.4188\n",
      "Epoch 17/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 315.8489 - val_loss: 437.2541\n",
      "Epoch 18/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 313.8647 - val_loss: 470.4486\n",
      "Epoch 19/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 310.2292 - val_loss: 430.9648\n",
      "Epoch 20/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 314.5530 - val_loss: 375.7777\n",
      "9/9 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43619, 40, 4) (43619, 1) (259, 40, 4)\n",
      "Epoch 1/20\n",
      "341/341 [==============================] - 4s 9ms/step - loss: 2348.5066 - val_loss: 1760.0104\n",
      "Epoch 2/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 905.3906 - val_loss: 538.0271\n",
      "Epoch 3/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 402.0147 - val_loss: 497.5231\n",
      "Epoch 4/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 379.9032 - val_loss: 480.3297\n",
      "Epoch 5/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 365.4847 - val_loss: 461.1993\n",
      "Epoch 6/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 358.5267 - val_loss: 442.4182\n",
      "Epoch 7/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 352.4693 - val_loss: 417.7172\n",
      "Epoch 8/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 341.7758 - val_loss: 392.9417\n",
      "Epoch 9/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 336.1290 - val_loss: 361.3725\n",
      "Epoch 10/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 335.2461 - val_loss: 384.3633\n",
      "Epoch 11/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 330.1585 - val_loss: 363.1767\n",
      "Epoch 12/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 333.1298 - val_loss: 365.9569\n",
      "Epoch 13/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 321.7569 - val_loss: 359.0782\n",
      "Epoch 14/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 322.4348 - val_loss: 346.8219\n",
      "Epoch 15/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 320.3102 - val_loss: 329.8136\n",
      "Epoch 16/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 316.8035 - val_loss: 335.6013\n",
      "Epoch 17/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 315.2298 - val_loss: 321.9313\n",
      "Epoch 18/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 309.6044 - val_loss: 318.1364\n",
      "Epoch 19/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 309.6872 - val_loss: 333.2196\n",
      "Epoch 20/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 312.3482 - val_loss: 311.1096\n",
      "9/9 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43619, 40, 4) (43619, 1) (259, 40, 4)\n",
      "Epoch 1/20\n",
      "341/341 [==============================] - 4s 9ms/step - loss: 2393.1924 - val_loss: 1775.1542\n",
      "Epoch 2/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 1060.2963 - val_loss: 439.3153\n",
      "Epoch 3/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 429.9979 - val_loss: 386.2940\n",
      "Epoch 4/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 382.7807 - val_loss: 337.7810\n",
      "Epoch 5/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 363.5873 - val_loss: 323.6670\n",
      "Epoch 6/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 350.1389 - val_loss: 327.8008\n",
      "Epoch 7/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 339.6621 - val_loss: 307.6828\n",
      "Epoch 8/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 337.8290 - val_loss: 311.1031\n",
      "Epoch 9/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 336.2205 - val_loss: 346.6529\n",
      "Epoch 10/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 325.2146 - val_loss: 348.3587\n",
      "Epoch 11/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 323.1756 - val_loss: 311.0858\n",
      "Epoch 12/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 320.9852 - val_loss: 309.1925\n",
      "Epoch 13/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 316.9194 - val_loss: 296.0526\n",
      "Epoch 14/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 314.5063 - val_loss: 270.4543\n",
      "Epoch 15/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 310.7520 - val_loss: 267.9482\n",
      "Epoch 16/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 306.1442 - val_loss: 275.7955\n",
      "Epoch 17/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 302.7379 - val_loss: 260.0704\n",
      "Epoch 18/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 299.1792 - val_loss: 276.5576\n",
      "Epoch 19/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 302.6666 - val_loss: 279.4329\n",
      "Epoch 20/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 298.8624 - val_loss: 294.6371\n",
      "9/9 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43619, 40, 4) (43619, 1) (259, 40, 4)\n",
      "Epoch 1/20\n",
      "341/341 [==============================] - 4s 8ms/step - loss: 2344.3616 - val_loss: 1844.4548\n",
      "Epoch 2/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 1161.2739 - val_loss: 390.6000\n",
      "Epoch 3/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 419.1754 - val_loss: 393.2776\n",
      "Epoch 4/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 391.0252 - val_loss: 378.7570\n",
      "Epoch 5/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 366.8130 - val_loss: 368.2174\n",
      "Epoch 6/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 353.4683 - val_loss: 397.9230\n",
      "Epoch 7/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 345.8092 - val_loss: 369.9594\n",
      "Epoch 8/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 341.9616 - val_loss: 359.5326\n",
      "Epoch 9/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 333.7926 - val_loss: 386.5799\n",
      "Epoch 10/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 326.7023 - val_loss: 368.8883\n",
      "Epoch 11/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 329.0290 - val_loss: 359.5367\n",
      "Epoch 12/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 326.5447 - val_loss: 351.4746\n",
      "Epoch 13/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 321.3650 - val_loss: 338.3096\n",
      "Epoch 14/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 317.5802 - val_loss: 338.3398\n",
      "Epoch 15/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 313.3471 - val_loss: 331.1120\n",
      "Epoch 16/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 313.1023 - val_loss: 325.1451\n",
      "Epoch 17/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 308.1083 - val_loss: 323.4234\n",
      "Epoch 18/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 306.2451 - val_loss: 306.2063\n",
      "Epoch 19/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 303.3025 - val_loss: 293.5175\n",
      "Epoch 20/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 303.2964 - val_loss: 299.9835\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "CPU times: user 4min 48s, sys: 28.8 s, total: 5min 17s\n",
      "Wall time: 5min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# RMSE\tstd_RMSE\tS_score\tstd_S_score\tMSE\tstd_MSE\talpha\tnodes\tdropout\tactivation\tbatch_size\tTW\n",
    "# 16.603718\t0.0\t1.343367e+03\t0.0\t275.683502\t0.0\t0.20\t[32]\t0.1\ttanh\t128\t40\n",
    "results002 = pd.DataFrame()\n",
    "\n",
    "for SEED in range(5):  \n",
    "    tf.random.set_seed(SEED)\n",
    "    mse = []\n",
    "    R2_val = []\n",
    "    RMSE = []\n",
    "    score_val = []\n",
    "    \n",
    "    # 0.20\t[64]\t0.3\ttanh\t32\t25\n",
    "    \n",
    "    # parameter's sample\n",
    "    # weights_file = \"weights_file_lstm_optimalmodel_clv.h5\"\n",
    "    alpha = 0.2\n",
    "    sequence_length = 40\n",
    "    epochs = 20\n",
    "    nodes_per_layer = [32]\n",
    "    dropout = 0.1\n",
    "    activation = 'tanh'\n",
    "    batch_size = 128\n",
    "    remaining_sensors = remaining_sensors\n",
    "    # create model\n",
    "    input_shape = (sequence_length, len(remaining_sensors))    \n",
    "    # Data prepration\n",
    "    X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "\n",
    "    # create sequences train, test\n",
    "    train_array = gen_data_wrapper(X_train_interim, sequence_length,remaining_sensors)\n",
    "    label_array = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'])\n",
    "\n",
    "    test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,remaining_sensors, -99.))\n",
    "               for unit_nr in X_test_interim['Unit'].unique())\n",
    "    \n",
    "    test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "    test_rul = rul_piecewise_fct(y_test,rul_piecewise)\n",
    "    print(train_array.shape, label_array.shape, test_array.shape)\n",
    "    model = model_lstm_1layer(input_shape, nodes_per_layer[0], dropout, activation)\n",
    "       \n",
    "    # Model fitting\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        start_time = time.time()\n",
    "        weights_file = model.get_weights()\n",
    "        model.set_weights(weights_file)  # reset optimizer and node weights before every training iteration\n",
    "        history = model.fit(train_array, label_array,\n",
    "                                validation_data=(test_array, test_rul),\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                # callbacks=[cb],\n",
    "                                verbose=1)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        training_time = end_time - start_time\n",
    "        mse.append(history.history['val_loss'][-1])\n",
    "\n",
    "        y_hat_val_split = model.predict(test_array)\n",
    "        R2_val.append(r2_score(test_rul, y_hat_val_split))\n",
    "        RMSE.append(np.sqrt(mean_squared_error(test_rul, y_hat_val_split)))\n",
    "        score_val.append(compute_s_score(test_rul, y_hat_val_split))\n",
    "            \n",
    "        \n",
    "    #  append results\n",
    "    d = {'RMSE' :np.mean(RMSE), 'std_RMSE' :np.std(RMSE),\n",
    "         'S_score' :np.mean(score_val), 'std_S_score' :np.std(score_val),\n",
    "         'MSE':np.mean(mse), 'std_MSE':np.std(mse),\n",
    "         'nodes':str(nodes_per_layer), 'dropout':dropout, \n",
    "         'activation':activation, 'batch_size':batch_size, 'TW' : sequence_length, \n",
    "         'time': training_time}\n",
    "\n",
    "#     results = results.append(pd.DataFrame(d, index=[0]), ignore_index=True)\n",
    "    results002 = pd.concat([results002, pd.DataFrame(d, index=[0])], ignore_index=True)\n",
    "    results002.to_csv('results/base/fd002.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>std_RMSE</th>\n",
       "      <th>S_score</th>\n",
       "      <th>std_S_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>TW</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16.742281</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1304.559511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>280.303986</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "      <td>40</td>\n",
       "      <td>61.363432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19.384987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9952.746447</td>\n",
       "      <td>0.0</td>\n",
       "      <td>375.777710</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "      <td>40</td>\n",
       "      <td>62.251372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.638299</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1785.090727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>311.109650</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "      <td>40</td>\n",
       "      <td>60.280839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.164997</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2051.300692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>294.637146</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "      <td>40</td>\n",
       "      <td>55.732981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.320034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1426.378111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>299.983521</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "      <td>40</td>\n",
       "      <td>55.506745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RMSE  std_RMSE      S_score  std_S_score         MSE  std_MSE nodes  \\\n",
       "0  16.742281       0.0  1304.559511          0.0  280.303986      0.0  [32]   \n",
       "1  19.384987       0.0  9952.746447          0.0  375.777710      0.0  [32]   \n",
       "2  17.638299       0.0  1785.090727          0.0  311.109650      0.0  [32]   \n",
       "3  17.164997       0.0  2051.300692          0.0  294.637146      0.0  [32]   \n",
       "4  17.320034       0.0  1426.378111          0.0  299.983521      0.0  [32]   \n",
       "\n",
       "   dropout activation  batch_size  TW       time  \n",
       "0      0.1       tanh         128  40  61.363432  \n",
       "1      0.1       tanh         128  40  62.251372  \n",
       "2      0.1       tanh         128  40  60.280839  \n",
       "3      0.1       tanh         128  40  55.732981  \n",
       "4      0.1       tanh         128  40  55.506745  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FD003 <a class=\"anchor\" id=\"fd003\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24720, 27) (16596, 26) (100, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# Load data and preprocess\n",
    "train, test, y_test = prepare_data('FD003.txt')\n",
    "print(train.shape, test.shape, y_test.shape)\n",
    "sensor_names = ['T20','T24','T30','T50','P20','P15','P30','Nf','Nc','epr','Ps30','phi',\n",
    "                    'NRf','NRc','BPR','farB','htBleed','Nf_dmd','PCNfR_dmd','W31','W32']\n",
    "remaining_sensors = [\"Ps30\", \"NRc\", \"W31\", \"BPR\"] # selection based on main_notebook [1] \"Nf\" [1] \"Ps30\" [1] \"phi\"\n",
    "drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "\n",
    "rul_piecewise = 125\n",
    "train['RUL'].clip(upper=rul_piecewise, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21320, 35, 4) (21320, 1) (100, 35, 4)\n",
      "Epoch 1/20\n",
      "667/667 [==============================] - 5s 6ms/step - loss: 1809.3916 - val_loss: 627.0729\n",
      "Epoch 2/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 476.0309 - val_loss: 546.0072\n",
      "Epoch 3/20\n",
      "667/667 [==============================] - 4s 7ms/step - loss: 428.4308 - val_loss: 418.3187\n",
      "Epoch 4/20\n",
      "667/667 [==============================] - 4s 7ms/step - loss: 397.5644 - val_loss: 534.5612\n",
      "Epoch 5/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 354.9685 - val_loss: 517.6132\n",
      "Epoch 6/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 338.8925 - val_loss: 385.0986\n",
      "Epoch 7/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 328.4911 - val_loss: 380.4737\n",
      "Epoch 8/20\n",
      "667/667 [==============================] - 4s 7ms/step - loss: 328.2462 - val_loss: 351.5383\n",
      "Epoch 9/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 310.2026 - val_loss: 346.5166\n",
      "Epoch 10/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 309.4579 - val_loss: 338.8087\n",
      "Epoch 11/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 289.6968 - val_loss: 318.5159\n",
      "Epoch 12/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 278.0544 - val_loss: 340.3091\n",
      "Epoch 13/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 273.8850 - val_loss: 328.6561\n",
      "Epoch 14/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 268.2656 - val_loss: 388.2747\n",
      "Epoch 15/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 253.4452 - val_loss: 257.3010\n",
      "Epoch 16/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 248.6750 - val_loss: 284.4387\n",
      "Epoch 17/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 238.1518 - val_loss: 321.3987\n",
      "Epoch 18/20\n",
      "667/667 [==============================] - 4s 7ms/step - loss: 230.8805 - val_loss: 265.8199\n",
      "Epoch 19/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 231.8635 - val_loss: 259.2244\n",
      "Epoch 20/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 218.8658 - val_loss: 231.8885\n",
      "4/4 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21320, 35, 4) (21320, 1) (100, 35, 4)\n",
      "Epoch 1/20\n",
      "667/667 [==============================] - 5s 6ms/step - loss: 1959.3773 - val_loss: 1284.4646\n",
      "Epoch 2/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 512.9575 - val_loss: 534.6699\n",
      "Epoch 3/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 415.3179 - val_loss: 451.7582\n",
      "Epoch 4/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 368.0338 - val_loss: 504.9758\n",
      "Epoch 5/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 335.9818 - val_loss: 459.8291\n",
      "Epoch 6/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 333.8476 - val_loss: 408.7294\n",
      "Epoch 7/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 323.1582 - val_loss: 516.1066\n",
      "Epoch 8/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 306.9634 - val_loss: 347.6187\n",
      "Epoch 9/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 300.6796 - val_loss: 343.1248\n",
      "Epoch 10/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 293.8029 - val_loss: 334.8302\n",
      "Epoch 11/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 274.6517 - val_loss: 325.3284\n",
      "Epoch 12/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 270.0127 - val_loss: 293.4892\n",
      "Epoch 13/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 258.6269 - val_loss: 251.0237\n",
      "Epoch 14/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 255.2900 - val_loss: 292.9923\n",
      "Epoch 15/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 250.6792 - val_loss: 391.1759\n",
      "Epoch 16/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 239.6557 - val_loss: 318.8029\n",
      "Epoch 17/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 231.6162 - val_loss: 326.3521\n",
      "Epoch 18/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 224.5186 - val_loss: 279.5188\n",
      "Epoch 19/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 217.1559 - val_loss: 278.1673\n",
      "Epoch 20/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 216.1429 - val_loss: 232.7490\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "(21320, 35, 4) (21320, 1) (100, 35, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "667/667 [==============================] - 5s 6ms/step - loss: 1888.2330 - val_loss: 706.7651\n",
      "Epoch 2/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 497.0775 - val_loss: 435.4746\n",
      "Epoch 3/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 419.4802 - val_loss: 654.4635\n",
      "Epoch 4/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 366.1762 - val_loss: 470.4344\n",
      "Epoch 5/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 355.1180 - val_loss: 352.7591\n",
      "Epoch 6/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 336.1967 - val_loss: 471.1773\n",
      "Epoch 7/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 334.3104 - val_loss: 389.3570\n",
      "Epoch 8/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 317.4855 - val_loss: 359.4142\n",
      "Epoch 9/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 311.0001 - val_loss: 346.6952\n",
      "Epoch 10/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 301.2757 - val_loss: 339.0142\n",
      "Epoch 11/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 294.9182 - val_loss: 312.7732\n",
      "Epoch 12/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 286.5693 - val_loss: 364.6932\n",
      "Epoch 13/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 281.2741 - val_loss: 278.3047\n",
      "Epoch 14/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 266.4488 - val_loss: 319.4044\n",
      "Epoch 15/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 253.3222 - val_loss: 258.1976\n",
      "Epoch 16/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 242.9198 - val_loss: 283.8609\n",
      "Epoch 17/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 238.2584 - val_loss: 276.4525\n",
      "Epoch 18/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 225.2287 - val_loss: 257.3896\n",
      "Epoch 19/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 215.4590 - val_loss: 242.5242\n",
      "Epoch 20/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 207.1290 - val_loss: 264.7688\n",
      "4/4 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21320, 35, 4) (21320, 1) (100, 35, 4)\n",
      "Epoch 1/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 1946.0426 - val_loss: 894.0097\n",
      "Epoch 2/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 528.4478 - val_loss: 485.9036\n",
      "Epoch 3/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 454.2690 - val_loss: 753.2277\n",
      "Epoch 4/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 406.4329 - val_loss: 443.8639\n",
      "Epoch 5/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 358.9470 - val_loss: 427.8487\n",
      "Epoch 6/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 339.7233 - val_loss: 359.7825\n",
      "Epoch 7/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 321.8696 - val_loss: 581.3495\n",
      "Epoch 8/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 310.6410 - val_loss: 378.3747\n",
      "Epoch 9/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 295.5874 - val_loss: 308.0919\n",
      "Epoch 10/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 290.5657 - val_loss: 303.4630\n",
      "Epoch 11/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 279.4646 - val_loss: 335.5248\n",
      "Epoch 12/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 267.0987 - val_loss: 313.6276\n",
      "Epoch 13/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 261.9629 - val_loss: 300.9824\n",
      "Epoch 14/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 250.2408 - val_loss: 269.9701\n",
      "Epoch 15/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 231.1691 - val_loss: 253.7264\n",
      "Epoch 16/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 226.6756 - val_loss: 264.8079\n",
      "Epoch 17/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 219.3446 - val_loss: 259.7076\n",
      "Epoch 18/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 209.5261 - val_loss: 267.8708\n",
      "Epoch 19/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 204.2981 - val_loss: 237.1114\n",
      "Epoch 20/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 196.4656 - val_loss: 259.2326\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "(21320, 35, 4) (21320, 1) (100, 35, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "667/667 [==============================] - 5s 6ms/step - loss: 1827.8146 - val_loss: 593.7330\n",
      "Epoch 2/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 487.5656 - val_loss: 481.2301\n",
      "Epoch 3/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 393.5088 - val_loss: 417.6632\n",
      "Epoch 4/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 356.6049 - val_loss: 519.1750\n",
      "Epoch 5/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 336.2125 - val_loss: 419.7113\n",
      "Epoch 6/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 324.0346 - val_loss: 350.1489\n",
      "Epoch 7/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 305.5600 - val_loss: 327.7593\n",
      "Epoch 8/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 290.0201 - val_loss: 361.1849\n",
      "Epoch 9/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 281.4741 - val_loss: 312.9975\n",
      "Epoch 10/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 271.4062 - val_loss: 278.5565\n",
      "Epoch 11/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 267.0735 - val_loss: 263.6822\n",
      "Epoch 12/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 262.6733 - val_loss: 284.5909\n",
      "Epoch 13/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 252.0832 - val_loss: 263.1508\n",
      "Epoch 14/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 241.6901 - val_loss: 272.1668\n",
      "Epoch 15/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 234.2740 - val_loss: 299.2135\n",
      "Epoch 16/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 228.3992 - val_loss: 246.9773\n",
      "Epoch 17/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 217.5882 - val_loss: 241.1895\n",
      "Epoch 18/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 210.4099 - val_loss: 255.1012\n",
      "Epoch 19/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 207.9808 - val_loss: 224.3251\n",
      "Epoch 20/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 200.4610 - val_loss: 251.5573\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "CPU times: user 6min 48s, sys: 18.3 s, total: 7min 6s\n",
      "Wall time: 6min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# \tRMSE\tstd_RMSE\tS_score\tstd_S_score\tMSE\tstd_MSE\talpha\tnodes\tdropout\tactivation\tbatch_size\tTW\n",
    "# 43\t15.404917\t0.0\t8.255290e+02\t0.0\t237.311462\t0.0\t0.10\t[64]\t0.2\ttanh\t32\t35\n",
    "results003 = pd.DataFrame()\n",
    "\n",
    "for SEED in range(5):  \n",
    "    tf.random.set_seed(SEED)\n",
    "    mse = []\n",
    "    R2_val = []\n",
    "    RMSE = []\n",
    "    score_val = []\n",
    "    \n",
    "    # 0.20\t[64]\t0.3\ttanh\t32\t25\n",
    "    \n",
    "    # parameter's sample\n",
    "    # weights_file = \"weights_file_lstm_optimalmodel_clv.h5\"\n",
    "    alpha = 0.1\n",
    "    sequence_length = 35\n",
    "    epochs = 20\n",
    "    nodes_per_layer = [64]\n",
    "    dropout = 0.2\n",
    "    activation = 'tanh'\n",
    "    batch_size = 32\n",
    "    remaining_sensors = remaining_sensors\n",
    "    # create model\n",
    "    input_shape = (sequence_length, len(remaining_sensors))    \n",
    "    # Data prepration\n",
    "    X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "\n",
    "    # create sequences train, test\n",
    "    train_array = gen_data_wrapper(X_train_interim, sequence_length,remaining_sensors)\n",
    "    label_array = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'])\n",
    "\n",
    "    test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,remaining_sensors, -99.))\n",
    "               for unit_nr in X_test_interim['Unit'].unique())\n",
    "    \n",
    "    test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "    test_rul = rul_piecewise_fct(y_test,rul_piecewise)\n",
    "    print(train_array.shape, label_array.shape, test_array.shape)\n",
    "    model = model_lstm_1layer(input_shape, nodes_per_layer[0], dropout, activation)\n",
    "       \n",
    "    # Model fitting\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        start_time = time.time()\n",
    "        weights_file = model.get_weights()\n",
    "        model.set_weights(weights_file)  # reset optimizer and node weights before every training iteration\n",
    "        history = model.fit(train_array, label_array,\n",
    "                                validation_data=(test_array, test_rul),\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                # callbacks=[cb],\n",
    "                                verbose=1)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        training_time = end_time - start_time\n",
    "        mse.append(history.history['val_loss'][-1])\n",
    "\n",
    "        y_hat_val_split = model.predict(test_array)\n",
    "        R2_val.append(r2_score(test_rul, y_hat_val_split))\n",
    "        RMSE.append(np.sqrt(mean_squared_error(test_rul, y_hat_val_split)))\n",
    "        score_val.append(compute_s_score(test_rul, y_hat_val_split))\n",
    "            \n",
    "        \n",
    "    #  append results\n",
    "    d = {'RMSE' :np.mean(RMSE), 'std_RMSE' :np.std(RMSE),\n",
    "         'S_score' :np.mean(score_val), 'std_S_score' :np.std(score_val),\n",
    "         'MSE':np.mean(mse), 'std_MSE':np.std(mse),\n",
    "         'nodes':str(nodes_per_layer), 'dropout':dropout, \n",
    "         'activation':activation, 'batch_size':batch_size, 'TW' : sequence_length, \n",
    "         'time': training_time}\n",
    "\n",
    "#     results = results.append(pd.DataFrame(d, index=[0]), ignore_index=True)\n",
    "    results003 = pd.concat([results003, pd.DataFrame(d, index=[0])], ignore_index=True)\n",
    "    results003.to_csv('results/base/fd003.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>std_RMSE</th>\n",
       "      <th>S_score</th>\n",
       "      <th>std_S_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>TW</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15.227885</td>\n",
       "      <td>0.0</td>\n",
       "      <td>742.982769</td>\n",
       "      <td>0.0</td>\n",
       "      <td>231.888474</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "      <td>84.279276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.256113</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1119.886405</td>\n",
       "      <td>0.0</td>\n",
       "      <td>232.748962</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "      <td>78.741477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16.271719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1376.477150</td>\n",
       "      <td>0.0</td>\n",
       "      <td>264.768829</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "      <td>77.551248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.100702</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1310.040013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>259.232605</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "      <td>77.406572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15.860561</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1515.197137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>251.557327</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "      <td>78.349120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RMSE  std_RMSE      S_score  std_S_score         MSE  std_MSE nodes  \\\n",
       "0  15.227885       0.0   742.982769          0.0  231.888474      0.0  [64]   \n",
       "1  15.256113       0.0  1119.886405          0.0  232.748962      0.0  [64]   \n",
       "2  16.271719       0.0  1376.477150          0.0  264.768829      0.0  [64]   \n",
       "3  16.100702       0.0  1310.040013          0.0  259.232605      0.0  [64]   \n",
       "4  15.860561       0.0  1515.197137          0.0  251.557327      0.0  [64]   \n",
       "\n",
       "   dropout activation  batch_size  TW       time  \n",
       "0      0.2       tanh          32  35  84.279276  \n",
       "1      0.2       tanh          32  35  78.741477  \n",
       "2      0.2       tanh          32  35  77.551248  \n",
       "3      0.2       tanh          32  35  77.406572  \n",
       "4      0.2       tanh          32  35  78.349120  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FD001 <a class=\"anchor\" id=\"fd001\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20631, 27) (13096, 26) (100, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# Load data and preprocess\n",
    "train, test, y_test = prepare_data('FD001.txt')\n",
    "print(train.shape, test.shape, y_test.shape)\n",
    "sensor_names = ['T20','T24','T30','T50','P20','P15','P30','Nf','Nc','epr','Ps30','phi',\n",
    "                    'NRf','NRc','BPR','farB','htBleed','Nf_dmd','PCNfR_dmd','W31','W32']\n",
    "remaining_sensors = [\"Ps30\", \"Nf\", \"P30\", \"BPR\"] # selection based on main_notebook \n",
    "drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "\n",
    "rul_piecewise = 125\n",
    "train['RUL'].clip(upper=rul_piecewise, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17731, 30, 4) (17731, 1) (100, 30, 4)\n",
      "Epoch 1/20\n",
      "278/278 [==============================] - 5s 16ms/step - loss: 2094.1931 - val_loss: 1618.0125\n",
      "Epoch 2/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 1216.7513 - val_loss: 701.2263\n",
      "Epoch 3/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 684.6442 - val_loss: 430.1585\n",
      "Epoch 4/20\n",
      "278/278 [==============================] - 5s 16ms/step - loss: 395.6197 - val_loss: 269.1258\n",
      "Epoch 5/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 333.9962 - val_loss: 234.5949\n",
      "Epoch 6/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 315.4727 - val_loss: 242.5693\n",
      "Epoch 7/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 293.2221 - val_loss: 234.5160\n",
      "Epoch 8/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 289.0195 - val_loss: 240.1270\n",
      "Epoch 9/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 272.7834 - val_loss: 211.5993\n",
      "Epoch 10/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 270.4138 - val_loss: 209.0664\n",
      "Epoch 11/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 268.8469 - val_loss: 201.9522\n",
      "Epoch 12/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 263.5716 - val_loss: 218.5646\n",
      "Epoch 13/20\n",
      "278/278 [==============================] - 5s 16ms/step - loss: 260.0588 - val_loss: 197.2213\n",
      "Epoch 14/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 254.0367 - val_loss: 306.2424\n",
      "Epoch 15/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 265.7289 - val_loss: 219.8473\n",
      "Epoch 16/20\n",
      "278/278 [==============================] - 5s 16ms/step - loss: 256.2245 - val_loss: 202.7061\n",
      "Epoch 17/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 262.5152 - val_loss: 245.2004\n",
      "Epoch 18/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 258.9381 - val_loss: 190.4556\n",
      "Epoch 19/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 256.9735 - val_loss: 367.9646\n",
      "Epoch 20/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 253.3004 - val_loss: 189.0175\n",
      "4/4 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17731, 30, 4) (17731, 1) (100, 30, 4)\n",
      "Epoch 1/20\n",
      "278/278 [==============================] - 5s 16ms/step - loss: 2078.9531 - val_loss: 1606.9794\n",
      "Epoch 2/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 1220.6165 - val_loss: 759.2280\n",
      "Epoch 3/20\n",
      "278/278 [==============================] - 5s 16ms/step - loss: 633.4005 - val_loss: 481.1546\n",
      "Epoch 4/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 432.7019 - val_loss: 517.8420\n",
      "Epoch 5/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 352.8484 - val_loss: 229.2370\n",
      "Epoch 6/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 308.6838 - val_loss: 242.9386\n",
      "Epoch 7/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 286.3264 - val_loss: 496.5279\n",
      "Epoch 8/20\n",
      "278/278 [==============================] - 5s 16ms/step - loss: 311.5746 - val_loss: 230.1150\n",
      "Epoch 9/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 285.4235 - val_loss: 270.9725\n",
      "Epoch 10/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 273.6106 - val_loss: 298.6067\n",
      "Epoch 11/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 276.1212 - val_loss: 216.6938\n",
      "Epoch 12/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 269.7479 - val_loss: 213.5727\n",
      "Epoch 13/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 267.8220 - val_loss: 248.7298\n",
      "Epoch 14/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 263.2815 - val_loss: 220.8702\n",
      "Epoch 15/20\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 265.1206 - val_loss: 276.4460\n",
      "Epoch 16/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 259.2953 - val_loss: 243.6112\n",
      "Epoch 17/20\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 257.5627 - val_loss: 220.9771\n",
      "Epoch 18/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 266.3781 - val_loss: 222.9655\n",
      "Epoch 19/20\n",
      "278/278 [==============================] - 5s 19ms/step - loss: 253.1618 - val_loss: 376.3720\n",
      "Epoch 20/20\n",
      "278/278 [==============================] - 5s 19ms/step - loss: 258.1356 - val_loss: 238.3522\n",
      "4/4 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17731, 30, 4) (17731, 1) (100, 30, 4)\n",
      "Epoch 1/20\n",
      "278/278 [==============================] - 5s 16ms/step - loss: 2094.0459 - val_loss: 1729.9291\n",
      "Epoch 2/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 1309.5995 - val_loss: 749.9687\n",
      "Epoch 3/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 747.7368 - val_loss: 693.5231\n",
      "Epoch 4/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 678.0728 - val_loss: 510.0824\n",
      "Epoch 5/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 414.9129 - val_loss: 939.8067\n",
      "Epoch 6/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 363.2160 - val_loss: 341.2820\n",
      "Epoch 7/20\n",
      "278/278 [==============================] - 5s 16ms/step - loss: 311.7357 - val_loss: 384.8770\n",
      "Epoch 8/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 291.5698 - val_loss: 217.9052\n",
      "Epoch 9/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 277.6761 - val_loss: 237.6449\n",
      "Epoch 10/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 278.5867 - val_loss: 252.9384\n",
      "Epoch 11/20\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 273.9001 - val_loss: 203.0262\n",
      "Epoch 12/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 269.1165 - val_loss: 214.0331\n",
      "Epoch 13/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 266.3112 - val_loss: 260.2408\n",
      "Epoch 14/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 265.7875 - val_loss: 221.7475\n",
      "Epoch 15/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 265.7626 - val_loss: 201.8480\n",
      "Epoch 16/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 273.5358 - val_loss: 226.3591\n",
      "Epoch 17/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 271.0573 - val_loss: 268.6886\n",
      "Epoch 18/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 262.3854 - val_loss: 216.4814\n",
      "Epoch 19/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 256.9982 - val_loss: 215.2633\n",
      "Epoch 20/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 251.7938 - val_loss: 199.8924\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "(17731, 30, 4) (17731, 1) (100, 30, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "278/278 [==============================] - 5s 15ms/step - loss: 2110.8755 - val_loss: 1565.2759\n",
      "Epoch 2/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 1210.2472 - val_loss: 749.0840\n",
      "Epoch 3/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 728.4390 - val_loss: 701.4253\n",
      "Epoch 4/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 492.8962 - val_loss: 317.3288\n",
      "Epoch 5/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 348.7446 - val_loss: 308.9222\n",
      "Epoch 6/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 324.0099 - val_loss: 259.0289\n",
      "Epoch 7/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 302.8770 - val_loss: 292.0851\n",
      "Epoch 8/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 294.5700 - val_loss: 341.5771\n",
      "Epoch 9/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 290.2554 - val_loss: 243.2938\n",
      "Epoch 10/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 282.5752 - val_loss: 216.5613\n",
      "Epoch 11/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 272.5053 - val_loss: 223.8488\n",
      "Epoch 12/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 274.6294 - val_loss: 378.2812\n",
      "Epoch 13/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 265.8940 - val_loss: 242.7614\n",
      "Epoch 14/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 271.5932 - val_loss: 219.2391\n",
      "Epoch 15/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 275.5325 - val_loss: 257.1039\n",
      "Epoch 16/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 270.8724 - val_loss: 216.4944\n",
      "Epoch 17/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 273.8191 - val_loss: 204.2164\n",
      "Epoch 18/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 271.4509 - val_loss: 216.6102\n",
      "Epoch 19/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 267.3225 - val_loss: 203.9916\n",
      "Epoch 20/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 262.7890 - val_loss: 202.6674\n",
      "4/4 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17731, 30, 4) (17731, 1) (100, 30, 4)\n",
      "Epoch 1/20\n",
      "278/278 [==============================] - 5s 15ms/step - loss: 2124.2063 - val_loss: 1588.0610\n",
      "Epoch 2/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 1256.3977 - val_loss: 722.7924\n",
      "Epoch 3/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 738.5599 - val_loss: 756.6016\n",
      "Epoch 4/20\n",
      "278/278 [==============================] - 5s 19ms/step - loss: 666.0049 - val_loss: 456.2169\n",
      "Epoch 5/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 457.7518 - val_loss: 420.1334\n",
      "Epoch 6/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 352.0724 - val_loss: 252.0757\n",
      "Epoch 7/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 320.5235 - val_loss: 221.1909\n",
      "Epoch 8/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 300.2021 - val_loss: 437.0053\n",
      "Epoch 9/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 299.6741 - val_loss: 216.6460\n",
      "Epoch 10/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 301.4364 - val_loss: 220.1520\n",
      "Epoch 11/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 283.4087 - val_loss: 251.9600\n",
      "Epoch 12/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 276.2222 - val_loss: 240.7516\n",
      "Epoch 13/20\n",
      "278/278 [==============================] - 5s 16ms/step - loss: 278.4939 - val_loss: 216.4634\n",
      "Epoch 14/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 271.4201 - val_loss: 206.3292\n",
      "Epoch 15/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 274.4836 - val_loss: 205.6006\n",
      "Epoch 16/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 271.8141 - val_loss: 208.3161\n",
      "Epoch 17/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 263.3819 - val_loss: 222.4185\n",
      "Epoch 18/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 263.6825 - val_loss: 211.0468\n",
      "Epoch 19/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 260.2422 - val_loss: 200.6702\n",
      "Epoch 20/20\n",
      "278/278 [==============================] - 5s 16ms/step - loss: 262.9647 - val_loss: 228.9551\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "CPU times: user 10min 26s, sys: 1min 53s, total: 12min 20s\n",
      "Wall time: 7min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# RMSE\tstd_RMSE\tS_score\tstd_S_score\tMSE\tstd_MSE\talpha\tnodes\tdropout\tactivation\tbatch_size\tTW\n",
    "# 21\t13.768992\t0.0\t3.830708e+02\t0.0\t189.585159\t0.0\t0.10\t[128]\t0.2\ttanh\t64\t30\n",
    "results001 = pd.DataFrame()\n",
    "\n",
    "for SEED in range(5):  \n",
    "    set_seed(SEED)\n",
    "    mse = []\n",
    "    R2_val = []\n",
    "    RMSE = []\n",
    "    score_val = []\n",
    "    \n",
    "    # 0.20\t[64]\t0.3\ttanh\t32\t25\n",
    "    \n",
    "    # parameter's sample\n",
    "    # weights_file = \"weights_file_lstm_optimalmodel_clv.h5\"\n",
    "    alpha = 0.1\n",
    "    sequence_length = 30\n",
    "    epochs = 20\n",
    "    nodes_per_layer = [128]\n",
    "    dropout = 0.2\n",
    "    activation = 'tanh'\n",
    "    batch_size = 64\n",
    "    remaining_sensors = remaining_sensors\n",
    "    # create model\n",
    "    input_shape = (sequence_length, len(remaining_sensors))    \n",
    "    # Data prepration\n",
    "    X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "\n",
    "    # create sequences train, test\n",
    "    train_array = gen_data_wrapper(X_train_interim, sequence_length,remaining_sensors)\n",
    "    label_array = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'])\n",
    "\n",
    "    test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,remaining_sensors, -99.))\n",
    "               for unit_nr in X_test_interim['Unit'].unique())\n",
    "    \n",
    "    test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "    test_rul = rul_piecewise_fct(y_test,rul_piecewise)\n",
    "    print(train_array.shape, label_array.shape, test_array.shape)\n",
    "    model = model_lstm_1layer(input_shape, nodes_per_layer[0], dropout, activation)\n",
    "       \n",
    "    # Model fitting\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        start_time = time.time()\n",
    "        weights_file = model.get_weights()\n",
    "        model.set_weights(weights_file)  # reset optimizer and node weights before every training iteration\n",
    "        history = model.fit(train_array, label_array,\n",
    "                                validation_data=(test_array, test_rul),\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                # callbacks=[cb],\n",
    "                                verbose=1)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        training_time = end_time - start_time\n",
    "        mse.append(history.history['val_loss'][-1])\n",
    "\n",
    "        y_hat_val_split = model.predict(test_array)\n",
    "        R2_val.append(r2_score(test_rul, y_hat_val_split))\n",
    "        RMSE.append(np.sqrt(mean_squared_error(test_rul, y_hat_val_split)))\n",
    "        score_val.append(compute_s_score(test_rul, y_hat_val_split))\n",
    "            \n",
    "        \n",
    "    #  append results\n",
    "    d = {'RMSE' :np.mean(RMSE), 'std_RMSE' :np.std(RMSE),\n",
    "         'S_score' :np.mean(score_val), 'std_S_score' :np.std(score_val),\n",
    "         'MSE':np.mean(mse), 'std_MSE':np.std(mse),\n",
    "         'nodes':str(nodes_per_layer), 'dropout':dropout, \n",
    "         'activation':activation, 'batch_size':batch_size, 'TW' : sequence_length, \n",
    "         'time': training_time}\n",
    "\n",
    "#     results = results.append(pd.DataFrame(d, index=[0]), ignore_index=True)\n",
    "    results001 = pd.concat([results001, pd.DataFrame(d, index=[0])], ignore_index=True)\n",
    "    results001.to_csv('results/base/fd001.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>std_RMSE</th>\n",
       "      <th>S_score</th>\n",
       "      <th>std_S_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>TW</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.748364</td>\n",
       "      <td>0.0</td>\n",
       "      <td>356.682616</td>\n",
       "      <td>0.0</td>\n",
       "      <td>189.017532</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "      <td>88.835431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.438658</td>\n",
       "      <td>0.0</td>\n",
       "      <td>407.824525</td>\n",
       "      <td>0.0</td>\n",
       "      <td>238.352188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "      <td>93.002537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.138332</td>\n",
       "      <td>0.0</td>\n",
       "      <td>369.935529</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.892426</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "      <td>92.465764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.236129</td>\n",
       "      <td>0.0</td>\n",
       "      <td>403.798504</td>\n",
       "      <td>0.0</td>\n",
       "      <td>202.667404</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "      <td>87.789842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15.131261</td>\n",
       "      <td>0.0</td>\n",
       "      <td>489.257022</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.955078</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "      <td>88.103285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RMSE  std_RMSE     S_score  std_S_score         MSE  std_MSE  nodes  \\\n",
       "0  13.748364       0.0  356.682616          0.0  189.017532      0.0  [128]   \n",
       "1  15.438658       0.0  407.824525          0.0  238.352188      0.0  [128]   \n",
       "2  14.138332       0.0  369.935529          0.0  199.892426      0.0  [128]   \n",
       "3  14.236129       0.0  403.798504          0.0  202.667404      0.0  [128]   \n",
       "4  15.131261       0.0  489.257022          0.0  228.955078      0.0  [128]   \n",
       "\n",
       "   dropout activation  batch_size  TW       time  \n",
       "0      0.2       tanh          64  30  88.835431  \n",
       "1      0.2       tanh          64  30  93.002537  \n",
       "2      0.2       tanh          64  30  92.465764  \n",
       "3      0.2       tanh          64  30  87.789842  \n",
       "4      0.2       tanh          64  30  88.103285  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results001"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
