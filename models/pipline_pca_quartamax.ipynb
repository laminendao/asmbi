{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\ProgramData\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "from lime.lime_tabular import RecurrentTabularExplainer\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error, r2_score \n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn import preprocessing\n",
    "from keras import backend as K\n",
    "from sklearn.preprocessing import MinMaxScaler , StandardScaler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Activation, GRU\n",
    "from scipy import optimize\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sp_modif.model_function import *\n",
    "from sp_modif.methods import *\n",
    "from sp_modif.data_prep import *\n",
    "from sp_modif.evaluator import *\n",
    "from sp_modif.SHAP import *\n",
    "from sp_modif.L2X import *\n",
    "from methods import *\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEED = 0\n",
    "def set_seed(seed=SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    tf.random.set_seed(SEED)\n",
    "\n",
    "# Appeler la fonction pour fixer le seed\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61249, 27) (41214, 26) (248, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# Load data and preprocess\n",
    "train, test, y_test = prepare_data('FD004.txt')\n",
    "print(train.shape, test.shape, y_test.shape)\n",
    "sensor_names = ['T20','T24','T30','T50','P20','P15','P30','Nf','Nc','epr','Ps30','phi',\n",
    "                    'NRf','NRc','BPR','farB','htBleed','Nf_dmd','PCNfR_dmd','W31','W32']\n",
    "\n",
    "remaining_sensors = ['T24','T30','T50', 'P15', 'P30','Nf','Nc', 'epr','Ps30','phi',\n",
    "                    'NRf','NRc','BPR', 'farB','htBleed','W31','W32']\n",
    "drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "\n",
    "rul_piecewise = 120\n",
    "train['RUL'].clip(upper=rul_piecewise, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_lstm_1layer(input_shape, nodes_per_layer, dropout, activation):\n",
    "    \n",
    "    cb = keras.callbacks.EarlyStopping(monitor='loss', patience=4)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units = nodes_per_layer, activation=activation, \n",
    "                  input_shape=input_shape))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(256))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer=Adam(learning_rate=0.001))\n",
    "    # model.save_weights(weights_file)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_cr_train, X_cr_test= StandardScaler().fit_transform(X_train_interim[remaining_sensors]), StandardScaler().fit_transform(X_test_interim[remaining_sensors])\n",
    "# pca = PCA()\n",
    "\n",
    "# component_train , component_test = pca.fit(X_cr_train).transform(X_cr_train), pca.transform(X_cr_test)\n",
    "# # print(pca.explained_variance_, np_component) # choos component which lambda >1 # kaiser\n",
    "\n",
    "# np_component = len(pca.explained_variance_[pca.explained_variance_>1])\n",
    "# print(pca.explained_variance_,'\\n', \"Nb components: \", np_component) # choos component which lambda >1 # kaiser\n",
    "# comp = ['comp' + str(i) for i in range(1,np_component+1)]\n",
    "# X_train_interim[comp],  X_test_interim[comp]= component_train[:, :np_component], component_test[:, :np_component]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\ProgramData\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "[7.95914066e+00 5.20347330e+00 1.05754225e+00 9.73443909e-01\n",
      " 6.01841369e-01 3.89076541e-01 1.95285565e-01 1.42521636e-01\n",
      " 1.18388892e-01 9.96185263e-02 9.54720058e-02 7.55025638e-02\n",
      " 3.73317439e-02 2.33308417e-02 1.38792003e-02 9.96708843e-03\n",
      " 4.46146340e-03] \n",
      " Nb components:  3\n",
      "(51538, 40, 3) (51538, 1) (248, 40, 3)\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:From c:\\ProgramData\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      " 460/1611 [=======>......................] - ETA: 14s - loss: 992.3143"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:60\u001b[0m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\src\\engine\\training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1805\u001b[0m ):\n\u001b[0;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1325\u001b[0m     args,\n\u001b[0;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1327\u001b[0m     executing_eagerly)\n\u001b[0;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1501\u001b[0m   )\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results_pca = pd.DataFrame()\n",
    "for SEED in range(5):  \n",
    "    tf.random.set_seed(SEED)\n",
    "    mse = []\n",
    "    R2_val = []\n",
    "    RMSE = []\n",
    "    score_val = []\n",
    "    \n",
    "    # 0.20\t[64]\t0.3\ttanh\t32\t25\n",
    "    \n",
    "    # parameter's sample\n",
    "    # weights_file = \"weights_file_lstm_optimalmodel_all.h5\"\n",
    "    alpha = 0.3\n",
    "    sequence_length = 40\n",
    "    epochs = 20\n",
    "    nodes_per_layer = [64]\n",
    "    dropout = 0.2\n",
    "    activation = 'tanh'\n",
    "    batch_size = 32\n",
    "    remaining_sensors = remaining_sensors\n",
    "    # create model\n",
    "    input_shape = (sequence_length, len(remaining_sensors))\n",
    "    model = model_lstm_1layer(input_shape, nodes_per_layer[0], dropout, activation)\n",
    "    \n",
    "    # Data prepration\n",
    "    X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "    \n",
    "    #PCA data reduction \n",
    "    X_cr_train, X_cr_test= StandardScaler().fit_transform(X_train_interim[remaining_sensors]), StandardScaler().fit_transform(X_test_interim[remaining_sensors])\n",
    "    pca = PCA()\n",
    "    component_train , component_test = pca.fit(X_cr_train).transform(X_cr_train), pca.transform(X_cr_test)\n",
    "    # print(pca.explained_variance_, np_component) # choos component which lambda >1 # kaiser\n",
    "\n",
    "    np_component = len(pca.explained_variance_[pca.explained_variance_>1])\n",
    "    print(pca.explained_variance_,'\\n', \"Nb components: \", np_component) # choos component which lambda >1 # kaiser\n",
    "    comp = ['comp' + str(i) for i in range(1,np_component+1)]\n",
    "    X_train_interim[comp],  X_test_interim[comp]= component_train[:, :np_component], component_test[:, :np_component]\n",
    "    \n",
    "\n",
    "    # create sequences train, test\n",
    "    train_array = gen_data_wrapper(X_train_interim, sequence_length, comp)\n",
    "    label_array = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'])\n",
    "\n",
    "    test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,comp, -99.))\n",
    "               for unit_nr in X_test_interim['Unit'].unique())\n",
    "    \n",
    "    test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "    test_rul = rul_piecewise_fct(y_test,rul_piecewise)\n",
    "    \n",
    "    input_shape = (sequence_length, len(comp))\n",
    "    model = model_lstm_1layer(input_shape, nodes_per_layer[0], dropout, activation)\n",
    "    print(train_array.shape, label_array.shape, test_array.shape)\n",
    "            \n",
    "    # Model fitting\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        start_time = time.time()\n",
    "        model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n",
    "        weights_file = model.get_weights()\n",
    "        model.set_weights(weights_file)  # reset optimizer and node weights before every training iteration\n",
    "        history = model.fit(train_array, label_array,\n",
    "                                validation_data=(test_array, test_rul),\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                # callbacks=[cb],\n",
    "                                verbose=1)\n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "        mse.append(history.history['val_loss'][-1])\n",
    "\n",
    "        y_hat_val_split = model.predict(test_array)\n",
    "        R2_val.append(r2_score(test_rul, y_hat_val_split))\n",
    "        RMSE.append(np.sqrt(mean_squared_error(test_rul, y_hat_val_split)))\n",
    "        score_val.append(compute_s_score(test_rul, y_hat_val_split))\n",
    "            \n",
    "        \n",
    "    #  append results\n",
    "    d = {'RMSE' :np.mean(RMSE), 'std_RMSE' :np.std(RMSE),\n",
    "         'S_score' :np.mean(score_val), 'std_S_score' :np.std(score_val),\n",
    "         'MSE':np.mean(mse), 'std_MSE':np.std(mse),\n",
    "         'nodes':str(nodes_per_layer), 'dropout':dropout, \n",
    "         'activation':activation, 'batch_size':batch_size, 'TW' : sequence_length,\n",
    "         'time': training_time}\n",
    "\n",
    "#     results = results.append(pd.DataFrame(d, index=[0]), ignore_index=True)\n",
    "    results_pca = pd.concat([results_pca, pd.DataFrame(d, index=[0])], ignore_index=True)\n",
    "    results_pca.to_csv('results/pca/fd004.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>std_RMSE</th>\n",
       "      <th>S_score</th>\n",
       "      <th>std_S_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>TW</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33.517714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20798.519974</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1123.437134</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>184.166592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30.657440</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11337.076398</td>\n",
       "      <td>0.0</td>\n",
       "      <td>939.878723</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>184.555245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27.935600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7179.744395</td>\n",
       "      <td>0.0</td>\n",
       "      <td>780.397766</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>179.203301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33.057681</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14701.550987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1092.810303</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>180.289082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31.380157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11612.759988</td>\n",
       "      <td>0.0</td>\n",
       "      <td>984.714294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>184.788765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RMSE  std_RMSE       S_score  std_S_score          MSE  std_MSE nodes  \\\n",
       "0  33.517714       0.0  20798.519974          0.0  1123.437134      0.0  [64]   \n",
       "1  30.657440       0.0  11337.076398          0.0   939.878723      0.0  [64]   \n",
       "2  27.935600       0.0   7179.744395          0.0   780.397766      0.0  [64]   \n",
       "3  33.057681       0.0  14701.550987          0.0  1092.810303      0.0  [64]   \n",
       "4  31.380157       0.0  11612.759988          0.0   984.714294      0.0  [64]   \n",
       "\n",
       "   dropout activation  batch_size  TW        time  \n",
       "0      0.2       tanh          32  40  184.166592  \n",
       "1      0.2       tanh          32  40  184.555245  \n",
       "2      0.2       tanh          32  40  179.203301  \n",
       "3      0.2       tanh          32  40  180.289082  \n",
       "4      0.2       tanh          32  40  184.788765  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FD002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53759, 27) (33991, 26) (259, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# Load data and preprocess\n",
    "train, test, y_test = prepare_data('FD002.txt')\n",
    "print(train.shape, test.shape, y_test.shape)\n",
    "sensor_names = ['T20','T24','T30','T50','P20','P15','P30','Nf','Nc','epr','Ps30','phi',\n",
    "                    'NRf','NRc','BPR','farB','htBleed','Nf_dmd','PCNfR_dmd','W31','W32']\n",
    "\n",
    "remaining_sensors = ['T24','T30','T50', 'P15', 'P30','Nf','Nc', 'epr','Ps30','phi',\n",
    "                    'NRf','NRc','BPR','htBleed','W31','W32']\n",
    "drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "\n",
    "rul_piecewise = 125\n",
    "train['RUL'].clip(upper=rul_piecewise, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.04271508e+01 2.66149542e+00 1.05555422e+00 4.67459514e-01\n",
      " 2.81050559e-01 2.11386654e-01 1.92544371e-01 1.64287263e-01\n",
      " 1.32272442e-01 1.16578816e-01 1.04679947e-01 7.46965656e-02\n",
      " 4.85539287e-02 3.45854321e-02 1.97697647e-02 8.23190831e-03] \n",
      " Nb components:  3\n",
      "(43619, 40, 3) (43619, 1) (259, 40, 3)\n",
      "Epoch 1/20\n",
      "341/341 [==============================] - 3s 7ms/step - loss: 1384.3842 - val_loss: 876.1518\n",
      "Epoch 2/20\n",
      "341/341 [==============================] - 2s 6ms/step - loss: 433.4120 - val_loss: 823.0498\n",
      "Epoch 3/20\n",
      "341/341 [==============================] - 2s 6ms/step - loss: 424.6640 - val_loss: 789.3731\n",
      "Epoch 4/20\n",
      "341/341 [==============================] - 2s 6ms/step - loss: 387.0952 - val_loss: 873.8283\n",
      "Epoch 5/20\n",
      "341/341 [==============================] - 2s 6ms/step - loss: 300.4011 - val_loss: 745.8162\n",
      "Epoch 6/20\n",
      "341/341 [==============================] - 2s 6ms/step - loss: 258.9048 - val_loss: 634.3585\n",
      "Epoch 7/20\n",
      "341/341 [==============================] - 2s 6ms/step - loss: 247.3683 - val_loss: 561.0745\n",
      "Epoch 8/20\n",
      "341/341 [==============================] - 2s 6ms/step - loss: 238.7275 - val_loss: 556.8059\n",
      "Epoch 9/20\n",
      "341/341 [==============================] - 2s 6ms/step - loss: 238.9057 - val_loss: 585.7537\n",
      "Epoch 10/20\n",
      "341/341 [==============================] - 2s 7ms/step - loss: 237.1591 - val_loss: 577.5851\n",
      "Epoch 11/20\n",
      "341/341 [==============================] - 2s 6ms/step - loss: 232.6417 - val_loss: 597.3887\n",
      "Epoch 12/20\n",
      "341/341 [==============================] - 2s 6ms/step - loss: 230.3390 - val_loss: 548.5800\n",
      "Epoch 13/20\n",
      "341/341 [==============================] - 2s 6ms/step - loss: 229.6414 - val_loss: 573.8098\n",
      "Epoch 14/20\n",
      "341/341 [==============================] - 2s 6ms/step - loss: 225.6916 - val_loss: 580.0823\n",
      "Epoch 15/20\n",
      "341/341 [==============================] - 2s 6ms/step - loss: 225.4310 - val_loss: 547.9427\n",
      "Epoch 16/20\n",
      "341/341 [==============================] - 2s 6ms/step - loss: 222.3907 - val_loss: 588.7170\n",
      "Epoch 17/20\n",
      "341/341 [==============================] - 2s 7ms/step - loss: 220.9351 - val_loss: 561.1433\n",
      "Epoch 18/20\n",
      "341/341 [==============================] - 2s 6ms/step - loss: 219.8834 - val_loss: 586.1384\n",
      "Epoch 19/20\n",
      "341/341 [==============================] - 2s 6ms/step - loss: 215.9854 - val_loss: 574.3770\n",
      "Epoch 20/20\n",
      "341/341 [==============================] - 2s 6ms/step - loss: 216.9912 - val_loss: 554.1687\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "[1.04271508e+01 2.66149542e+00 1.05555422e+00 4.67459514e-01\n",
      " 2.81050559e-01 2.11386654e-01 1.92544371e-01 1.64287263e-01\n",
      " 1.32272442e-01 1.16578816e-01 1.04679947e-01 7.46965656e-02\n",
      " 4.85539287e-02 3.45854321e-02 1.97697647e-02 8.23190831e-03] \n",
      " Nb components:  3\n",
      "(43619, 40, 3) (43619, 1) (259, 40, 3)\n",
      "Epoch 1/20\n",
      "341/341 [==============================] - 3s 7ms/step - loss: 1460.1169 - val_loss: 798.9950\n",
      "Epoch 2/20\n",
      "341/341 [==============================] - 2s 6ms/step - loss: 440.6393 - val_loss: 794.3800\n",
      "Epoch 3/20\n",
      "341/341 [==============================] - 2s 6ms/step - loss: 431.9056 - val_loss: 762.0864\n",
      "Epoch 4/20\n",
      "341/341 [==============================] - 2s 6ms/step - loss: 428.8854 - val_loss: 862.8931\n",
      "Epoch 5/20\n",
      "341/341 [==============================] - 2s 6ms/step - loss: 426.1221 - val_loss: 808.8055\n",
      "Epoch 6/20\n",
      "341/341 [==============================] - 2s 6ms/step - loss: 397.8127 - val_loss: 760.9892\n",
      "Epoch 7/20\n",
      "341/341 [==============================] - 2s 7ms/step - loss: 280.5617 - val_loss: 836.6212\n",
      "Epoch 8/20\n",
      "341/341 [==============================] - 2s 7ms/step - loss: 251.4516 - val_loss: 800.7253\n",
      "Epoch 9/20\n",
      "341/341 [==============================] - 2s 7ms/step - loss: 245.5134 - val_loss: 725.5422\n",
      "Epoch 10/20\n",
      "341/341 [==============================] - 2s 7ms/step - loss: 241.9852 - val_loss: 654.1192\n",
      "Epoch 11/20\n",
      "341/341 [==============================] - 2s 7ms/step - loss: 238.1407 - val_loss: 703.3031\n",
      "Epoch 12/20\n",
      "341/341 [==============================] - 2s 7ms/step - loss: 236.0569 - val_loss: 669.9686\n",
      "Epoch 13/20\n",
      "341/341 [==============================] - 2s 7ms/step - loss: 233.8594 - val_loss: 694.7496\n",
      "Epoch 14/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 229.5765 - val_loss: 654.9034\n",
      "Epoch 15/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 229.1574 - val_loss: 733.2073\n",
      "Epoch 16/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 226.9438 - val_loss: 686.2677\n",
      "Epoch 17/20\n",
      "341/341 [==============================] - 3s 7ms/step - loss: 225.8623 - val_loss: 715.3604\n",
      "Epoch 18/20\n",
      "341/341 [==============================] - 2s 7ms/step - loss: 224.0304 - val_loss: 798.8035\n",
      "Epoch 19/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 221.9988 - val_loss: 693.7404\n",
      "Epoch 20/20\n",
      "341/341 [==============================] - 3s 7ms/step - loss: 221.7795 - val_loss: 708.0387\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "[1.04271508e+01 2.66149542e+00 1.05555422e+00 4.67459514e-01\n",
      " 2.81050559e-01 2.11386654e-01 1.92544371e-01 1.64287263e-01\n",
      " 1.32272442e-01 1.16578816e-01 1.04679947e-01 7.46965656e-02\n",
      " 4.85539287e-02 3.45854321e-02 1.97697647e-02 8.23190831e-03] \n",
      " Nb components:  3\n",
      "(43619, 40, 3) (43619, 1) (259, 40, 3)\n",
      "Epoch 1/20\n",
      "341/341 [==============================] - 4s 8ms/step - loss: 1375.9553 - val_loss: 866.4975\n",
      "Epoch 2/20\n",
      "341/341 [==============================] - 2s 7ms/step - loss: 389.0464 - val_loss: 758.0341\n",
      "Epoch 3/20\n",
      "341/341 [==============================] - 3s 7ms/step - loss: 351.4838 - val_loss: 766.9044\n",
      "Epoch 4/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 300.6168 - val_loss: 759.7846\n",
      "Epoch 5/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 272.3353 - val_loss: 766.3838\n",
      "Epoch 6/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 252.0682 - val_loss: 736.9410\n",
      "Epoch 7/20\n",
      "341/341 [==============================] - 2s 7ms/step - loss: 248.1113 - val_loss: 678.9899\n",
      "Epoch 8/20\n",
      "341/341 [==============================] - 2s 7ms/step - loss: 241.6031 - val_loss: 674.0952\n",
      "Epoch 9/20\n",
      "341/341 [==============================] - 2s 7ms/step - loss: 240.2036 - val_loss: 720.0824\n",
      "Epoch 10/20\n",
      "341/341 [==============================] - 2s 7ms/step - loss: 238.8313 - val_loss: 712.1783\n",
      "Epoch 11/20\n",
      "341/341 [==============================] - 2s 7ms/step - loss: 235.3637 - val_loss: 785.8952\n",
      "Epoch 12/20\n",
      "341/341 [==============================] - 2s 7ms/step - loss: 234.1628 - val_loss: 726.4702\n",
      "Epoch 13/20\n",
      "341/341 [==============================] - 2s 7ms/step - loss: 230.4964 - val_loss: 718.3364\n",
      "Epoch 14/20\n",
      "341/341 [==============================] - 2s 7ms/step - loss: 229.9588 - val_loss: 732.4760\n",
      "Epoch 15/20\n",
      "341/341 [==============================] - 2s 7ms/step - loss: 227.7705 - val_loss: 686.7009\n",
      "Epoch 16/20\n",
      "341/341 [==============================] - 2s 7ms/step - loss: 226.0996 - val_loss: 668.6094\n",
      "Epoch 17/20\n",
      "341/341 [==============================] - 2s 7ms/step - loss: 225.3027 - val_loss: 680.4966\n",
      "Epoch 18/20\n",
      "341/341 [==============================] - 2s 6ms/step - loss: 221.2708 - val_loss: 670.5560\n",
      "Epoch 19/20\n",
      "341/341 [==============================] - 2s 7ms/step - loss: 220.7626 - val_loss: 650.6189\n",
      "Epoch 20/20\n",
      "341/341 [==============================] - 2s 7ms/step - loss: 219.0243 - val_loss: 622.4547\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "[1.04271508e+01 2.66149542e+00 1.05555422e+00 4.67459514e-01\n",
      " 2.81050559e-01 2.11386654e-01 1.92544371e-01 1.64287263e-01\n",
      " 1.32272442e-01 1.16578816e-01 1.04679947e-01 7.46965656e-02\n",
      " 4.85539287e-02 3.45854321e-02 1.97697647e-02 8.23190831e-03] \n",
      " Nb components:  3\n",
      "(43619, 40, 3) (43619, 1) (259, 40, 3)\n",
      "Epoch 1/20\n",
      "341/341 [==============================] - 4s 8ms/step - loss: 1556.3500 - val_loss: 936.6337\n",
      "Epoch 2/20\n",
      "341/341 [==============================] - 2s 7ms/step - loss: 411.2877 - val_loss: 842.2570\n",
      "Epoch 3/20\n",
      "341/341 [==============================] - 2s 7ms/step - loss: 402.1641 - val_loss: 836.5876\n",
      "Epoch 4/20\n",
      "341/341 [==============================] - 2s 7ms/step - loss: 411.2144 - val_loss: 843.1787\n",
      "Epoch 5/20\n",
      "341/341 [==============================] - 2s 7ms/step - loss: 387.9792 - val_loss: 796.7607\n",
      "Epoch 6/20\n",
      "341/341 [==============================] - 2s 7ms/step - loss: 387.8377 - val_loss: 891.3377\n",
      "Epoch 7/20\n",
      "341/341 [==============================] - 2s 7ms/step - loss: 375.1231 - val_loss: 700.4009\n",
      "Epoch 8/20\n",
      "341/341 [==============================] - 2s 7ms/step - loss: 336.0469 - val_loss: 773.0343\n",
      "Epoch 9/20\n",
      "341/341 [==============================] - 2s 7ms/step - loss: 313.1384 - val_loss: 878.8029\n",
      "Epoch 10/20\n",
      "341/341 [==============================] - 2s 7ms/step - loss: 308.4041 - val_loss: 935.5980\n",
      "Epoch 11/20\n",
      "341/341 [==============================] - 2s 7ms/step - loss: 291.6913 - val_loss: 840.7689\n",
      "Epoch 12/20\n",
      "341/341 [==============================] - 2s 7ms/step - loss: 279.2624 - val_loss: 798.3574\n",
      "Epoch 13/20\n",
      "341/341 [==============================] - 2s 7ms/step - loss: 272.9755 - val_loss: 786.6682\n",
      "Epoch 14/20\n",
      "341/341 [==============================] - 2s 7ms/step - loss: 275.3453 - val_loss: 799.7522\n",
      "Epoch 15/20\n",
      "341/341 [==============================] - 2s 7ms/step - loss: 267.3015 - val_loss: 783.4977\n",
      "Epoch 16/20\n",
      "341/341 [==============================] - 2s 7ms/step - loss: 260.6101 - val_loss: 765.3019\n",
      "Epoch 17/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 251.2656 - val_loss: 789.8914\n",
      "Epoch 18/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 243.1185 - val_loss: 805.5024\n",
      "Epoch 19/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 240.5454 - val_loss: 715.9290\n",
      "Epoch 20/20\n",
      "341/341 [==============================] - 2s 7ms/step - loss: 236.5528 - val_loss: 719.9666\n",
      "9/9 [==============================] - 0s 5ms/step\n",
      "[1.04271508e+01 2.66149542e+00 1.05555422e+00 4.67459514e-01\n",
      " 2.81050559e-01 2.11386654e-01 1.92544371e-01 1.64287263e-01\n",
      " 1.32272442e-01 1.16578816e-01 1.04679947e-01 7.46965656e-02\n",
      " 4.85539287e-02 3.45854321e-02 1.97697647e-02 8.23190831e-03] \n",
      " Nb components:  3\n",
      "(43619, 40, 3) (43619, 1) (259, 40, 3)\n",
      "Epoch 1/20\n",
      "341/341 [==============================] - 4s 8ms/step - loss: 1417.3318 - val_loss: 816.7598\n",
      "Epoch 2/20\n",
      "341/341 [==============================] - 2s 7ms/step - loss: 415.0840 - val_loss: 855.0056\n",
      "Epoch 3/20\n",
      "341/341 [==============================] - 2s 7ms/step - loss: 425.3702 - val_loss: 747.7908\n",
      "Epoch 4/20\n",
      "341/341 [==============================] - 2s 7ms/step - loss: 353.0941 - val_loss: 836.5168\n",
      "Epoch 5/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 282.3171 - val_loss: 831.9901\n",
      "Epoch 6/20\n",
      "341/341 [==============================] - 2s 7ms/step - loss: 259.3670 - val_loss: 836.2079\n",
      "Epoch 7/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 250.4256 - val_loss: 872.1711\n",
      "Epoch 8/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 246.2963 - val_loss: 828.8420\n",
      "Epoch 9/20\n",
      "341/341 [==============================] - 3s 7ms/step - loss: 243.0182 - val_loss: 783.8512\n",
      "Epoch 10/20\n",
      "341/341 [==============================] - 2s 7ms/step - loss: 240.1928 - val_loss: 778.6636\n",
      "Epoch 11/20\n",
      "341/341 [==============================] - 2s 7ms/step - loss: 238.0753 - val_loss: 768.5667\n",
      "Epoch 12/20\n",
      "341/341 [==============================] - 2s 6ms/step - loss: 235.3404 - val_loss: 718.8558\n",
      "Epoch 13/20\n",
      "341/341 [==============================] - 2s 6ms/step - loss: 233.7598 - val_loss: 816.5776\n",
      "Epoch 14/20\n",
      "341/341 [==============================] - 2s 6ms/step - loss: 230.6187 - val_loss: 765.7064\n",
      "Epoch 15/20\n",
      "341/341 [==============================] - 2s 7ms/step - loss: 230.3209 - val_loss: 762.6438\n",
      "Epoch 16/20\n",
      "341/341 [==============================] - 2s 6ms/step - loss: 228.6803 - val_loss: 800.7385\n",
      "Epoch 17/20\n",
      "341/341 [==============================] - 2s 6ms/step - loss: 226.0235 - val_loss: 719.8666\n",
      "Epoch 18/20\n",
      "341/341 [==============================] - 2s 6ms/step - loss: 225.1419 - val_loss: 707.9732\n",
      "Epoch 19/20\n",
      "341/341 [==============================] - 2s 7ms/step - loss: 223.7526 - val_loss: 708.7758\n",
      "Epoch 20/20\n",
      "341/341 [==============================] - 2s 6ms/step - loss: 221.3755 - val_loss: 709.0779\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "CPU times: total: 1min 10s\n",
      "Wall time: 4min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results_pca002 = pd.DataFrame()\n",
    "for SEED in range(5):  \n",
    "    tf.random.set_seed(SEED)\n",
    "    mse = []\n",
    "    R2_val = []\n",
    "    RMSE = []\n",
    "    score_val = []\n",
    "    \n",
    "    # 0.20\t[64]\t0.3\ttanh\t32\t25\n",
    "    \n",
    "    # parameter's sample\n",
    "    # weights_file = \"weights_file_lstm_optimalmodel_all.h5\"\n",
    "    alpha = 0.2\n",
    "    sequence_length = 40\n",
    "    epochs = 20\n",
    "    nodes_per_layer = [32]\n",
    "    dropout = 0.1\n",
    "    activation = 'tanh'\n",
    "    batch_size = 128\n",
    "    remaining_sensors = remaining_sensors\n",
    "    # create model\n",
    "    input_shape = (sequence_length, len(remaining_sensors))\n",
    "    model = model_lstm_1layer(input_shape, nodes_per_layer[0], dropout, activation)\n",
    "    \n",
    "    # Data prepration\n",
    "    X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "    \n",
    "    #PCA data reduction \n",
    "    X_cr_train, X_cr_test= StandardScaler().fit_transform(X_train_interim[remaining_sensors]), StandardScaler().fit_transform(X_test_interim[remaining_sensors])\n",
    "    pca = PCA()\n",
    "    component_train , component_test = pca.fit(X_cr_train).transform(X_cr_train), pca.transform(X_cr_test)\n",
    "    # print(pca.explained_variance_, np_component) # choos component which lambda >1 # kaiser\n",
    "\n",
    "    np_component = len(pca.explained_variance_[pca.explained_variance_>1])\n",
    "    print(pca.explained_variance_,'\\n', \"Nb components: \", np_component) # choos component which lambda >1 # kaiser\n",
    "    comp = ['comp' + str(i) for i in range(1,np_component+1)]\n",
    "    X_train_interim[comp],  X_test_interim[comp]= component_train[:, :np_component], component_test[:, :np_component]\n",
    "    \n",
    "\n",
    "    # create sequences train, test\n",
    "    train_array = gen_data_wrapper(X_train_interim, sequence_length, comp)\n",
    "    label_array = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'])\n",
    "\n",
    "    test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,comp, -99.))\n",
    "               for unit_nr in X_test_interim['Unit'].unique())\n",
    "    \n",
    "    test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "    test_rul = rul_piecewise_fct(y_test,rul_piecewise)\n",
    "    \n",
    "    input_shape = (sequence_length, len(comp))\n",
    "    model = model_lstm_1layer(input_shape, nodes_per_layer[0], dropout, activation)\n",
    "    print(train_array.shape, label_array.shape, test_array.shape)\n",
    "            \n",
    "    # Model fitting\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        start_time = time.time()\n",
    "        model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n",
    "        weights_file = model.get_weights()\n",
    "        model.set_weights(weights_file)  # reset optimizer and node weights before every training iteration\n",
    "        history = model.fit(train_array, label_array,\n",
    "                                validation_data=(test_array, test_rul),\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                # callbacks=[cb],\n",
    "                                verbose=1)\n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "        mse.append(history.history['val_loss'][-1])\n",
    "\n",
    "        y_hat_val_split = model.predict(test_array)\n",
    "        R2_val.append(r2_score(test_rul, y_hat_val_split))\n",
    "        RMSE.append(np.sqrt(mean_squared_error(test_rul, y_hat_val_split)))\n",
    "        score_val.append(compute_s_score(test_rul, y_hat_val_split))\n",
    "            \n",
    "        \n",
    "    #  append results\n",
    "    d = {'RMSE' :np.mean(RMSE), 'std_RMSE' :np.std(RMSE),\n",
    "         'S_score' :np.mean(score_val), 'std_S_score' :np.std(score_val),\n",
    "         'MSE':np.mean(mse), 'std_MSE':np.std(mse),\n",
    "         'nodes':str(nodes_per_layer), 'dropout':dropout, \n",
    "         'activation':activation, 'batch_size':batch_size, 'TW' : sequence_length,\n",
    "         'time': training_time}\n",
    "\n",
    "#     results = results.append(pd.DataFrame(d, index=[0]), ignore_index=True)\n",
    "    results_pca002 = pd.concat([results_pca002, pd.DataFrame(d, index=[0])], ignore_index=True)\n",
    "    results_pca002.to_csv('results/pca/fd002.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>std_RMSE</th>\n",
       "      <th>S_score</th>\n",
       "      <th>std_S_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>TW</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23.540787</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3249.123771</td>\n",
       "      <td>0.0</td>\n",
       "      <td>554.168701</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "      <td>40</td>\n",
       "      <td>44.072514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26.608995</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10540.041642</td>\n",
       "      <td>0.0</td>\n",
       "      <td>708.038696</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "      <td>40</td>\n",
       "      <td>48.091069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24.949041</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4426.846722</td>\n",
       "      <td>0.0</td>\n",
       "      <td>622.454651</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "      <td>40</td>\n",
       "      <td>50.391524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26.832192</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4031.455326</td>\n",
       "      <td>0.0</td>\n",
       "      <td>719.966553</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "      <td>40</td>\n",
       "      <td>49.096392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26.628517</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8183.105448</td>\n",
       "      <td>0.0</td>\n",
       "      <td>709.077881</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "      <td>40</td>\n",
       "      <td>48.870997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RMSE  std_RMSE       S_score  std_S_score         MSE  std_MSE nodes  \\\n",
       "0  23.540787       0.0   3249.123771          0.0  554.168701      0.0  [32]   \n",
       "1  26.608995       0.0  10540.041642          0.0  708.038696      0.0  [32]   \n",
       "2  24.949041       0.0   4426.846722          0.0  622.454651      0.0  [32]   \n",
       "3  26.832192       0.0   4031.455326          0.0  719.966553      0.0  [32]   \n",
       "4  26.628517       0.0   8183.105448          0.0  709.077881      0.0  [32]   \n",
       "\n",
       "   dropout activation  batch_size  TW       time  \n",
       "0      0.1       tanh         128  40  44.072514  \n",
       "1      0.1       tanh         128  40  48.091069  \n",
       "2      0.1       tanh         128  40  50.391524  \n",
       "3      0.1       tanh         128  40  49.096392  \n",
       "4      0.1       tanh         128  40  48.870997  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_pca002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FD003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24720, 27) (16596, 26) (100, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# Load data and preprocess\n",
    "train, test, y_test = prepare_data('FD003.txt')\n",
    "print(train.shape, test.shape, y_test.shape)\n",
    "sensor_names = ['T20','T24','T30','T50','P20','P15','P30','Nf','Nc','epr','Ps30','phi',\n",
    "                    'NRf','NRc','BPR','farB','htBleed','Nf_dmd','PCNfR_dmd','W31','W32']\n",
    "\n",
    "remaining_sensors = ['T24','T30','T50', 'P15', 'P30','Nf','Nc', 'epr','Ps30','phi',\n",
    "                    'NRf','NRc','BPR','htBleed','W31','W32']\n",
    "drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "\n",
    "rul_piecewise = 125\n",
    "train['RUL'].clip(upper=rul_piecewise, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.19095667e+00 5.49399656e+00 8.81347307e-01 7.81564822e-01\n",
      " 3.51238057e-01 1.43235420e-01 4.09913921e-02 3.53324081e-02\n",
      " 2.44339253e-02 1.75337353e-02 1.33853528e-02 1.23424994e-02\n",
      " 6.63828566e-03 3.52672765e-03 3.10180225e-03 1.02230366e-03] \n",
      " Nb components:  2\n",
      "(21320, 35, 2) (21320, 1) (100, 35, 2)\n",
      "Epoch 1/20\n",
      "667/667 [==============================] - 5s 6ms/step - loss: 836.8309 - val_loss: 944.6252\n",
      "Epoch 2/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 289.8248 - val_loss: 1114.4414\n",
      "Epoch 3/20\n",
      "667/667 [==============================] - 3s 5ms/step - loss: 212.7500 - val_loss: 959.8607\n",
      "Epoch 4/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 192.4150 - val_loss: 878.5651\n",
      "Epoch 5/20\n",
      "667/667 [==============================] - 4s 5ms/step - loss: 183.3982 - val_loss: 832.0654\n",
      "Epoch 6/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 180.6049 - val_loss: 874.3455\n",
      "Epoch 7/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 173.5380 - val_loss: 951.0679\n",
      "Epoch 8/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 170.6400 - val_loss: 718.4102\n",
      "Epoch 9/20\n",
      "667/667 [==============================] - 4s 5ms/step - loss: 165.5586 - val_loss: 835.9971\n",
      "Epoch 10/20\n",
      "667/667 [==============================] - 4s 5ms/step - loss: 159.6880 - val_loss: 764.1189\n",
      "Epoch 11/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 156.3004 - val_loss: 875.2067\n",
      "Epoch 12/20\n",
      "667/667 [==============================] - 4s 5ms/step - loss: 155.2384 - val_loss: 806.8122\n",
      "Epoch 13/20\n",
      "667/667 [==============================] - 4s 5ms/step - loss: 151.9572 - val_loss: 743.9881\n",
      "Epoch 14/20\n",
      "667/667 [==============================] - 4s 5ms/step - loss: 150.9742 - val_loss: 768.1682\n",
      "Epoch 15/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 146.9276 - val_loss: 686.0037\n",
      "Epoch 16/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 144.2425 - val_loss: 704.4647\n",
      "Epoch 17/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 141.4144 - val_loss: 742.2422\n",
      "Epoch 18/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 138.9749 - val_loss: 757.3510\n",
      "Epoch 19/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 137.4823 - val_loss: 711.4275\n",
      "Epoch 20/20\n",
      "667/667 [==============================] - 4s 5ms/step - loss: 135.9808 - val_loss: 738.1802\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "[8.19095667e+00 5.49399656e+00 8.81347307e-01 7.81564822e-01\n",
      " 3.51238057e-01 1.43235420e-01 4.09913921e-02 3.53324081e-02\n",
      " 2.44339253e-02 1.75337353e-02 1.33853528e-02 1.23424994e-02\n",
      " 6.63828566e-03 3.52672765e-03 3.10180225e-03 1.02230366e-03] \n",
      " Nb components:  2\n",
      "(21320, 35, 2) (21320, 1) (100, 35, 2)\n",
      "Epoch 1/20\n",
      "667/667 [==============================] - 5s 6ms/step - loss: 871.3484 - val_loss: 1264.0946\n",
      "Epoch 2/20\n",
      "667/667 [==============================] - 4s 5ms/step - loss: 325.4044 - val_loss: 1069.9731\n",
      "Epoch 3/20\n",
      "667/667 [==============================] - 3s 5ms/step - loss: 248.1033 - val_loss: 1062.0975\n",
      "Epoch 4/20\n",
      "667/667 [==============================] - 4s 5ms/step - loss: 204.5336 - val_loss: 1080.6292\n",
      "Epoch 5/20\n",
      "667/667 [==============================] - 4s 5ms/step - loss: 190.4118 - val_loss: 1024.2748\n",
      "Epoch 6/20\n",
      "667/667 [==============================] - 3s 5ms/step - loss: 184.2433 - val_loss: 859.3016\n",
      "Epoch 7/20\n",
      "667/667 [==============================] - 3s 5ms/step - loss: 177.8422 - val_loss: 736.0818\n",
      "Epoch 8/20\n",
      "667/667 [==============================] - 4s 5ms/step - loss: 167.4729 - val_loss: 931.7216\n",
      "Epoch 9/20\n",
      "667/667 [==============================] - 4s 5ms/step - loss: 178.0766 - val_loss: 824.2667\n",
      "Epoch 10/20\n",
      "667/667 [==============================] - 3s 5ms/step - loss: 160.6780 - val_loss: 892.9830\n",
      "Epoch 11/20\n",
      "667/667 [==============================] - 4s 5ms/step - loss: 154.9885 - val_loss: 927.1850\n",
      "Epoch 12/20\n",
      "667/667 [==============================] - 3s 5ms/step - loss: 152.1544 - val_loss: 754.7381\n",
      "Epoch 13/20\n",
      "667/667 [==============================] - 4s 5ms/step - loss: 146.5138 - val_loss: 754.8372\n",
      "Epoch 14/20\n",
      "667/667 [==============================] - 4s 5ms/step - loss: 144.1522 - val_loss: 769.6403\n",
      "Epoch 15/20\n",
      "667/667 [==============================] - 4s 5ms/step - loss: 143.4108 - val_loss: 781.4481\n",
      "Epoch 16/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 140.4199 - val_loss: 784.7996\n",
      "Epoch 17/20\n",
      "667/667 [==============================] - 3s 5ms/step - loss: 135.7538 - val_loss: 696.6691\n",
      "Epoch 18/20\n",
      "667/667 [==============================] - 4s 5ms/step - loss: 134.6774 - val_loss: 808.7589\n",
      "Epoch 19/20\n",
      "667/667 [==============================] - 3s 5ms/step - loss: 132.5482 - val_loss: 699.1320\n",
      "Epoch 20/20\n",
      "667/667 [==============================] - 4s 5ms/step - loss: 130.6462 - val_loss: 763.6888\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "[8.19095667e+00 5.49399656e+00 8.81347307e-01 7.81564822e-01\n",
      " 3.51238057e-01 1.43235420e-01 4.09913921e-02 3.53324081e-02\n",
      " 2.44339253e-02 1.75337353e-02 1.33853528e-02 1.23424994e-02\n",
      " 6.63828566e-03 3.52672765e-03 3.10180225e-03 1.02230366e-03] \n",
      " Nb components:  2\n",
      "(21320, 35, 2) (21320, 1) (100, 35, 2)\n",
      "Epoch 1/20\n",
      "667/667 [==============================] - 5s 6ms/step - loss: 870.2594 - val_loss: 990.8940\n",
      "Epoch 2/20\n",
      "667/667 [==============================] - 3s 5ms/step - loss: 266.4536 - val_loss: 916.6393\n",
      "Epoch 3/20\n",
      "667/667 [==============================] - 3s 5ms/step - loss: 203.4665 - val_loss: 784.1457\n",
      "Epoch 4/20\n",
      "667/667 [==============================] - 3s 5ms/step - loss: 193.0265 - val_loss: 890.9320\n",
      "Epoch 5/20\n",
      "667/667 [==============================] - 4s 5ms/step - loss: 183.6679 - val_loss: 957.4260\n",
      "Epoch 6/20\n",
      "667/667 [==============================] - 3s 5ms/step - loss: 179.0565 - val_loss: 941.8049\n",
      "Epoch 7/20\n",
      "667/667 [==============================] - 3s 5ms/step - loss: 173.8165 - val_loss: 827.7164\n",
      "Epoch 8/20\n",
      "667/667 [==============================] - 3s 5ms/step - loss: 171.1232 - val_loss: 940.7034\n",
      "Epoch 9/20\n",
      "667/667 [==============================] - 3s 5ms/step - loss: 165.6175 - val_loss: 752.4520\n",
      "Epoch 10/20\n",
      "667/667 [==============================] - 3s 5ms/step - loss: 160.6181 - val_loss: 763.4620\n",
      "Epoch 11/20\n",
      "667/667 [==============================] - 3s 5ms/step - loss: 157.0078 - val_loss: 819.9588\n",
      "Epoch 12/20\n",
      "667/667 [==============================] - 3s 5ms/step - loss: 154.6622 - val_loss: 720.4451\n",
      "Epoch 13/20\n",
      "667/667 [==============================] - 3s 5ms/step - loss: 151.3334 - val_loss: 820.4397\n",
      "Epoch 14/20\n",
      "667/667 [==============================] - 3s 5ms/step - loss: 147.9324 - val_loss: 737.6376\n",
      "Epoch 15/20\n",
      "667/667 [==============================] - 3s 5ms/step - loss: 149.0712 - val_loss: 695.4966\n",
      "Epoch 16/20\n",
      "667/667 [==============================] - 3s 5ms/step - loss: 142.1339 - val_loss: 688.3388\n",
      "Epoch 17/20\n",
      "667/667 [==============================] - 3s 5ms/step - loss: 143.6875 - val_loss: 804.9412\n",
      "Epoch 18/20\n",
      "667/667 [==============================] - 4s 5ms/step - loss: 139.5658 - val_loss: 659.2143\n",
      "Epoch 19/20\n",
      "667/667 [==============================] - 3s 5ms/step - loss: 136.3800 - val_loss: 756.3987\n",
      "Epoch 20/20\n",
      "667/667 [==============================] - 3s 5ms/step - loss: 134.9151 - val_loss: 705.4673\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "[8.19095667e+00 5.49399656e+00 8.81347307e-01 7.81564822e-01\n",
      " 3.51238057e-01 1.43235420e-01 4.09913921e-02 3.53324081e-02\n",
      " 2.44339253e-02 1.75337353e-02 1.33853528e-02 1.23424994e-02\n",
      " 6.63828566e-03 3.52672765e-03 3.10180225e-03 1.02230366e-03] \n",
      " Nb components:  2\n",
      "(21320, 35, 2) (21320, 1) (100, 35, 2)\n",
      "Epoch 1/20\n",
      "667/667 [==============================] - 5s 6ms/step - loss: 878.6287 - val_loss: 1457.4213\n",
      "Epoch 2/20\n",
      "667/667 [==============================] - 3s 5ms/step - loss: 315.2050 - val_loss: 1235.2123\n",
      "Epoch 3/20\n",
      "667/667 [==============================] - 4s 5ms/step - loss: 234.3548 - val_loss: 1101.5878\n",
      "Epoch 4/20\n",
      "667/667 [==============================] - 3s 5ms/step - loss: 197.6350 - val_loss: 1019.9833\n",
      "Epoch 5/20\n",
      "667/667 [==============================] - 3s 5ms/step - loss: 186.0987 - val_loss: 978.9929\n",
      "Epoch 6/20\n",
      "667/667 [==============================] - 3s 5ms/step - loss: 182.2670 - val_loss: 981.0732\n",
      "Epoch 7/20\n",
      "667/667 [==============================] - 3s 5ms/step - loss: 177.7139 - val_loss: 919.9444\n",
      "Epoch 8/20\n",
      "667/667 [==============================] - 4s 5ms/step - loss: 169.7271 - val_loss: 748.3656\n",
      "Epoch 9/20\n",
      "667/667 [==============================] - 3s 5ms/step - loss: 167.1950 - val_loss: 883.6426\n",
      "Epoch 10/20\n",
      "667/667 [==============================] - 3s 5ms/step - loss: 165.4212 - val_loss: 851.3831\n",
      "Epoch 11/20\n",
      "667/667 [==============================] - 3s 5ms/step - loss: 161.6480 - val_loss: 923.8270\n",
      "Epoch 12/20\n",
      "667/667 [==============================] - 3s 5ms/step - loss: 155.8247 - val_loss: 717.2908\n",
      "Epoch 13/20\n",
      "667/667 [==============================] - 3s 5ms/step - loss: 152.5312 - val_loss: 832.1443\n",
      "Epoch 14/20\n",
      "667/667 [==============================] - 3s 5ms/step - loss: 150.7034 - val_loss: 823.7430\n",
      "Epoch 15/20\n",
      "667/667 [==============================] - 3s 5ms/step - loss: 145.2525 - val_loss: 797.0776\n",
      "Epoch 16/20\n",
      "667/667 [==============================] - 3s 5ms/step - loss: 140.4224 - val_loss: 793.7237\n",
      "Epoch 17/20\n",
      "667/667 [==============================] - 3s 5ms/step - loss: 138.5120 - val_loss: 748.3091\n",
      "Epoch 18/20\n",
      "667/667 [==============================] - 3s 5ms/step - loss: 137.7879 - val_loss: 770.8091\n",
      "Epoch 19/20\n",
      "667/667 [==============================] - 3s 5ms/step - loss: 136.5275 - val_loss: 796.6677\n",
      "Epoch 20/20\n",
      "667/667 [==============================] - 3s 5ms/step - loss: 132.0157 - val_loss: 881.4465\n",
      "4/4 [==============================] - 0s 0s/step\n",
      "[8.19095667e+00 5.49399656e+00 8.81347307e-01 7.81564822e-01\n",
      " 3.51238057e-01 1.43235420e-01 4.09913921e-02 3.53324081e-02\n",
      " 2.44339253e-02 1.75337353e-02 1.33853528e-02 1.23424994e-02\n",
      " 6.63828566e-03 3.52672765e-03 3.10180225e-03 1.02230366e-03] \n",
      " Nb components:  2\n",
      "(21320, 35, 2) (21320, 1) (100, 35, 2)\n",
      "Epoch 1/20\n",
      "667/667 [==============================] - 5s 6ms/step - loss: 866.8098 - val_loss: 1344.9794\n",
      "Epoch 2/20\n",
      "667/667 [==============================] - 3s 5ms/step - loss: 256.4071 - val_loss: 879.0283\n",
      "Epoch 3/20\n",
      "667/667 [==============================] - 3s 5ms/step - loss: 208.5192 - val_loss: 885.9451\n",
      "Epoch 4/20\n",
      "667/667 [==============================] - 4s 5ms/step - loss: 192.7369 - val_loss: 911.5170\n",
      "Epoch 5/20\n",
      "667/667 [==============================] - 4s 5ms/step - loss: 178.5781 - val_loss: 810.0723\n",
      "Epoch 6/20\n",
      "667/667 [==============================] - 4s 5ms/step - loss: 178.3516 - val_loss: 965.7986\n",
      "Epoch 7/20\n",
      "667/667 [==============================] - 3s 5ms/step - loss: 172.6986 - val_loss: 651.5446\n",
      "Epoch 8/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 167.7273 - val_loss: 807.7855\n",
      "Epoch 9/20\n",
      "667/667 [==============================] - 4s 5ms/step - loss: 163.8087 - val_loss: 823.3336\n",
      "Epoch 10/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 156.1326 - val_loss: 867.9001\n",
      "Epoch 11/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 154.0166 - val_loss: 796.0851\n",
      "Epoch 12/20\n",
      "667/667 [==============================] - 3s 5ms/step - loss: 150.6814 - val_loss: 899.5867\n",
      "Epoch 13/20\n",
      "667/667 [==============================] - 3s 5ms/step - loss: 144.9578 - val_loss: 817.0655\n",
      "Epoch 14/20\n",
      "667/667 [==============================] - 4s 5ms/step - loss: 144.3491 - val_loss: 727.1571\n",
      "Epoch 15/20\n",
      "667/667 [==============================] - 3s 5ms/step - loss: 142.4800 - val_loss: 720.4955\n",
      "Epoch 16/20\n",
      "667/667 [==============================] - 3s 5ms/step - loss: 139.2297 - val_loss: 726.8761\n",
      "Epoch 17/20\n",
      "667/667 [==============================] - 4s 5ms/step - loss: 135.0110 - val_loss: 901.2857\n",
      "Epoch 18/20\n",
      "667/667 [==============================] - 3s 5ms/step - loss: 136.1448 - val_loss: 740.6981\n",
      "Epoch 19/20\n",
      "667/667 [==============================] - 4s 5ms/step - loss: 137.4272 - val_loss: 716.5376\n",
      "Epoch 20/20\n",
      "667/667 [==============================] - 3s 5ms/step - loss: 131.5763 - val_loss: 733.2812\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "CPU times: total: 1min 20s\n",
      "Wall time: 6min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results_pca003 = pd.DataFrame()\n",
    "for SEED in range(5):  \n",
    "    tf.random.set_seed(SEED)\n",
    "    mse = []\n",
    "    R2_val = []\n",
    "    RMSE = []\n",
    "    score_val = []\n",
    "    \n",
    "    # 0.20\t[64]\t0.3\ttanh\t32\t25\n",
    "    \n",
    "    # parameter's sample\n",
    "    # weights_file = \"weights_file_lstm_optimalmodel_all.h5\"\n",
    "    alpha = 0.1\n",
    "    sequence_length = 35\n",
    "    epochs = 20\n",
    "    nodes_per_layer = [64]\n",
    "    dropout = 0.2\n",
    "    activation = 'tanh'\n",
    "    batch_size = 32\n",
    "    remaining_sensors = remaining_sensors\n",
    "    \n",
    "    # Data prepration\n",
    "    X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "    \n",
    "    #PCA data reduction \n",
    "    X_cr_train, X_cr_test= StandardScaler().fit_transform(X_train_interim[remaining_sensors]), StandardScaler().fit_transform(X_test_interim[remaining_sensors])\n",
    "    pca = PCA()\n",
    "    component_train , component_test = pca.fit(X_cr_train).transform(X_cr_train), pca.transform(X_cr_test)\n",
    "    # print(pca.explained_variance_, np_component) # choos component which lambda >1 # kaiser\n",
    "\n",
    "    np_component = len(pca.explained_variance_[pca.explained_variance_>1])\n",
    "    print(pca.explained_variance_,'\\n', \"Nb components: \", np_component) # choos component which lambda >1 # kaiser\n",
    "    comp = ['comp' + str(i) for i in range(1,np_component+1)]\n",
    "    X_train_interim[comp],  X_test_interim[comp]= component_train[:, :np_component], component_test[:, :np_component]\n",
    "    \n",
    "\n",
    "    # create sequences train, test\n",
    "    train_array = gen_data_wrapper(X_train_interim, sequence_length, comp)\n",
    "    label_array = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'])\n",
    "\n",
    "    test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,comp, -99.))\n",
    "               for unit_nr in X_test_interim['Unit'].unique())\n",
    "    \n",
    "    test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "    test_rul = rul_piecewise_fct(y_test,rul_piecewise)\n",
    "    \n",
    "    input_shape = (sequence_length, len(comp))\n",
    "    model = model_lstm_1layer(input_shape, nodes_per_layer[0], dropout, activation)\n",
    "    print(train_array.shape, label_array.shape, test_array.shape)\n",
    "            \n",
    "    # Model fitting\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        start_time = time.time()\n",
    "        model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n",
    "        weights_file = model.get_weights()\n",
    "        model.set_weights(weights_file)  # reset optimizer and node weights before every training iteration\n",
    "        history = model.fit(train_array, label_array,\n",
    "                                validation_data=(test_array, test_rul),\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                # callbacks=[cb],\n",
    "                                verbose=1)\n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "        mse.append(history.history['val_loss'][-1])\n",
    "\n",
    "        y_hat_val_split = model.predict(test_array)\n",
    "        R2_val.append(r2_score(test_rul, y_hat_val_split))\n",
    "        RMSE.append(np.sqrt(mean_squared_error(test_rul, y_hat_val_split)))\n",
    "        score_val.append(compute_s_score(test_rul, y_hat_val_split))\n",
    "            \n",
    "        \n",
    "    #  append results\n",
    "    d = {'RMSE' :np.mean(RMSE), 'std_RMSE' :np.std(RMSE),\n",
    "         'S_score' :np.mean(score_val), 'std_S_score' :np.std(score_val),\n",
    "         'MSE':np.mean(mse), 'std_MSE':np.std(mse),\n",
    "         'nodes':str(nodes_per_layer), 'dropout':dropout, \n",
    "         'activation':activation, 'batch_size':batch_size, 'TW' : sequence_length,\n",
    "         'time': training_time}\n",
    "\n",
    "#     results = results.append(pd.DataFrame(d, index=[0]), ignore_index=True)\n",
    "    results_pca003 = pd.concat([results_pca003, pd.DataFrame(d, index=[0])], ignore_index=True)\n",
    "    results_pca003.to_csv('results/pca/fd003.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>std_RMSE</th>\n",
       "      <th>S_score</th>\n",
       "      <th>std_S_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>TW</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27.169472</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1606.809450</td>\n",
       "      <td>0.0</td>\n",
       "      <td>738.180176</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "      <td>74.372573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27.634920</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1340.860313</td>\n",
       "      <td>0.0</td>\n",
       "      <td>763.688843</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "      <td>72.090802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26.560636</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1262.115565</td>\n",
       "      <td>0.0</td>\n",
       "      <td>705.467346</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "      <td>68.616933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29.689165</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1721.257225</td>\n",
       "      <td>0.0</td>\n",
       "      <td>881.446533</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "      <td>69.500571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27.079166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1642.169688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>733.281250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "      <td>71.682696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RMSE  std_RMSE      S_score  std_S_score         MSE  std_MSE nodes  \\\n",
       "0  27.169472       0.0  1606.809450          0.0  738.180176      0.0  [64]   \n",
       "1  27.634920       0.0  1340.860313          0.0  763.688843      0.0  [64]   \n",
       "2  26.560636       0.0  1262.115565          0.0  705.467346      0.0  [64]   \n",
       "3  29.689165       0.0  1721.257225          0.0  881.446533      0.0  [64]   \n",
       "4  27.079166       0.0  1642.169688          0.0  733.281250      0.0  [64]   \n",
       "\n",
       "   dropout activation  batch_size  TW       time  \n",
       "0      0.2       tanh          32  35  74.372573  \n",
       "1      0.2       tanh          32  35  72.090802  \n",
       "2      0.2       tanh          32  35  68.616933  \n",
       "3      0.2       tanh          32  35  69.500571  \n",
       "4      0.2       tanh          32  35  71.682696  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_pca003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FD001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20631, 27) (13096, 26) (100, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# Load data and preprocess\n",
    "train, test, y_test = prepare_data('FD001.txt')\n",
    "print(train.shape, test.shape, y_test.shape)\n",
    "sensor_names = ['T20','T24','T30','T50','P20','P15','P30','Nf','Nc','epr','Ps30','phi',\n",
    "                    'NRf','NRc','BPR','farB','htBleed','Nf_dmd','PCNfR_dmd','W31','W32']\n",
    "remaining_sensors = ['T24','T30','T50', 'P15', 'P30','Nf','Nc','Ps30','phi',\n",
    "                    'NRf','NRc','BPR','htBleed','W31','W32']\n",
    "drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "\n",
    "rul_piecewise = 125\n",
    "train['RUL'].clip(upper=rul_piecewise, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.16218316e+01 2.22745151e+00 7.53919107e-01 7.60689125e-02\n",
      " 5.80733626e-02 4.69453854e-02 4.15078334e-02 3.47383080e-02\n",
      " 3.32981565e-02 2.89969632e-02 2.53785317e-02 1.84836056e-02\n",
      " 1.69423744e-02 1.35008525e-02 3.59058504e-03] \n",
      " Nb components:  2\n",
      "(17731, 30, 2) (17731, 1) (100, 30, 2)\n",
      "Epoch 1/20\n",
      "278/278 [==============================] - 6s 18ms/step - loss: 1091.9460 - val_loss: 993.8204\n",
      "Epoch 2/20\n",
      "278/278 [==============================] - 4s 13ms/step - loss: 224.6014 - val_loss: 880.8183\n",
      "Epoch 3/20\n",
      "278/278 [==============================] - 4s 13ms/step - loss: 192.0079 - val_loss: 687.6577\n",
      "Epoch 4/20\n",
      "278/278 [==============================] - 3s 12ms/step - loss: 187.4294 - val_loss: 767.1495\n",
      "Epoch 5/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 174.4967 - val_loss: 792.8438\n",
      "Epoch 6/20\n",
      "278/278 [==============================] - 3s 12ms/step - loss: 170.6644 - val_loss: 663.6057\n",
      "Epoch 7/20\n",
      "278/278 [==============================] - 3s 10ms/step - loss: 172.4901 - val_loss: 724.1043\n",
      "Epoch 8/20\n",
      "278/278 [==============================] - 3s 10ms/step - loss: 162.9078 - val_loss: 804.3661\n",
      "Epoch 9/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 162.1567 - val_loss: 830.8133\n",
      "Epoch 10/20\n",
      "278/278 [==============================] - 3s 10ms/step - loss: 160.6317 - val_loss: 853.1472\n",
      "Epoch 11/20\n",
      "278/278 [==============================] - 3s 10ms/step - loss: 152.0023 - val_loss: 800.5947\n",
      "Epoch 12/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 152.9207 - val_loss: 922.8359\n",
      "Epoch 13/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 150.6951 - val_loss: 958.9855\n",
      "Epoch 14/20\n",
      "278/278 [==============================] - 3s 10ms/step - loss: 148.0786 - val_loss: 773.6228\n",
      "Epoch 15/20\n",
      "278/278 [==============================] - 3s 10ms/step - loss: 147.4338 - val_loss: 996.9255\n",
      "Epoch 16/20\n",
      "278/278 [==============================] - 3s 10ms/step - loss: 150.0048 - val_loss: 765.7772\n",
      "Epoch 17/20\n",
      "278/278 [==============================] - 3s 10ms/step - loss: 143.4277 - val_loss: 906.4135\n",
      "Epoch 18/20\n",
      "278/278 [==============================] - 3s 10ms/step - loss: 138.2467 - val_loss: 881.7985\n",
      "Epoch 19/20\n",
      "278/278 [==============================] - 3s 10ms/step - loss: 136.5658 - val_loss: 936.5581\n",
      "Epoch 20/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 132.6329 - val_loss: 898.1289\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "[1.16218316e+01 2.22745151e+00 7.53919107e-01 7.60689125e-02\n",
      " 5.80733626e-02 4.69453854e-02 4.15078334e-02 3.47383080e-02\n",
      " 3.32981565e-02 2.89969632e-02 2.53785317e-02 1.84836056e-02\n",
      " 1.69423744e-02 1.35008525e-02 3.59058504e-03] \n",
      " Nb components:  2\n",
      "(17731, 30, 2) (17731, 1) (100, 30, 2)\n",
      "Epoch 1/20\n",
      "278/278 [==============================] - 4s 11ms/step - loss: 1100.1071 - val_loss: 1153.1545\n",
      "Epoch 2/20\n",
      "278/278 [==============================] - 3s 10ms/step - loss: 217.4684 - val_loss: 785.4532\n",
      "Epoch 3/20\n",
      "278/278 [==============================] - 3s 10ms/step - loss: 189.2479 - val_loss: 555.6754\n",
      "Epoch 4/20\n",
      "278/278 [==============================] - 3s 10ms/step - loss: 181.4359 - val_loss: 773.5963\n",
      "Epoch 5/20\n",
      "278/278 [==============================] - 3s 10ms/step - loss: 174.4355 - val_loss: 630.0936\n",
      "Epoch 6/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 168.9748 - val_loss: 728.0293\n",
      "Epoch 7/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 163.8810 - val_loss: 735.5632\n",
      "Epoch 8/20\n",
      "278/278 [==============================] - 3s 10ms/step - loss: 160.6069 - val_loss: 823.2004\n",
      "Epoch 9/20\n",
      "278/278 [==============================] - 3s 10ms/step - loss: 161.6643 - val_loss: 691.8312\n",
      "Epoch 10/20\n",
      "278/278 [==============================] - 3s 10ms/step - loss: 153.8473 - val_loss: 800.3984\n",
      "Epoch 11/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 156.7354 - val_loss: 790.2548\n",
      "Epoch 12/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 151.9056 - val_loss: 723.6401\n",
      "Epoch 13/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 148.9743 - val_loss: 859.0850\n",
      "Epoch 14/20\n",
      "278/278 [==============================] - 3s 10ms/step - loss: 146.0843 - val_loss: 825.6949\n",
      "Epoch 15/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 144.2646 - val_loss: 739.8670\n",
      "Epoch 16/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 142.1574 - val_loss: 843.6420\n",
      "Epoch 17/20\n",
      "278/278 [==============================] - 3s 10ms/step - loss: 135.8269 - val_loss: 857.5476\n",
      "Epoch 18/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 135.6960 - val_loss: 934.9664\n",
      "Epoch 19/20\n",
      "278/278 [==============================] - 3s 10ms/step - loss: 131.5508 - val_loss: 775.1266\n",
      "Epoch 20/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 127.6990 - val_loss: 832.0885\n",
      "4/4 [==============================] - 0s 0s/step\n",
      "[1.16218316e+01 2.22745151e+00 7.53919107e-01 7.60689125e-02\n",
      " 5.80733626e-02 4.69453854e-02 4.15078334e-02 3.47383080e-02\n",
      " 3.32981565e-02 2.89969632e-02 2.53785317e-02 1.84836056e-02\n",
      " 1.69423744e-02 1.35008525e-02 3.59058504e-03] \n",
      " Nb components:  2\n",
      "(17731, 30, 2) (17731, 1) (100, 30, 2)\n",
      "Epoch 1/20\n",
      "278/278 [==============================] - 4s 11ms/step - loss: 1170.8821 - val_loss: 1167.8439\n",
      "Epoch 2/20\n",
      "278/278 [==============================] - 3s 10ms/step - loss: 444.6333 - val_loss: 1145.7039\n",
      "Epoch 3/20\n",
      "278/278 [==============================] - 3s 10ms/step - loss: 472.4609 - val_loss: 955.2648\n",
      "Epoch 4/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 241.0593 - val_loss: 984.5386\n",
      "Epoch 5/20\n",
      "278/278 [==============================] - 3s 10ms/step - loss: 213.8667 - val_loss: 932.2952\n",
      "Epoch 6/20\n",
      "278/278 [==============================] - 3s 10ms/step - loss: 203.0894 - val_loss: 852.5136\n",
      "Epoch 7/20\n",
      "278/278 [==============================] - 3s 10ms/step - loss: 189.8178 - val_loss: 599.8365\n",
      "Epoch 8/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 191.1189 - val_loss: 956.4289\n",
      "Epoch 9/20\n",
      "278/278 [==============================] - 3s 10ms/step - loss: 180.8203 - val_loss: 798.9120\n",
      "Epoch 10/20\n",
      "278/278 [==============================] - 3s 10ms/step - loss: 178.8482 - val_loss: 783.3610\n",
      "Epoch 11/20\n",
      "278/278 [==============================] - 3s 10ms/step - loss: 178.8869 - val_loss: 642.9565\n",
      "Epoch 12/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 176.6390 - val_loss: 808.2296\n",
      "Epoch 13/20\n",
      "278/278 [==============================] - 3s 10ms/step - loss: 175.8571 - val_loss: 789.1000\n",
      "Epoch 14/20\n",
      "278/278 [==============================] - 3s 10ms/step - loss: 172.3445 - val_loss: 1014.4276\n",
      "Epoch 15/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 168.8431 - val_loss: 709.0361\n",
      "Epoch 16/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 162.4929 - val_loss: 758.9576\n",
      "Epoch 17/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 157.0864 - val_loss: 789.1302\n",
      "Epoch 18/20\n",
      "278/278 [==============================] - 3s 10ms/step - loss: 158.5013 - val_loss: 836.3320\n",
      "Epoch 19/20\n",
      "278/278 [==============================] - 3s 10ms/step - loss: 152.6757 - val_loss: 816.0632\n",
      "Epoch 20/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 156.4126 - val_loss: 787.3466\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "[1.16218316e+01 2.22745151e+00 7.53919107e-01 7.60689125e-02\n",
      " 5.80733626e-02 4.69453854e-02 4.15078334e-02 3.47383080e-02\n",
      " 3.32981565e-02 2.89969632e-02 2.53785317e-02 1.84836056e-02\n",
      " 1.69423744e-02 1.35008525e-02 3.59058504e-03] \n",
      " Nb components:  2\n",
      "(17731, 30, 2) (17731, 1) (100, 30, 2)\n",
      "Epoch 1/20\n",
      "278/278 [==============================] - 4s 13ms/step - loss: 1106.7518 - val_loss: 1104.9806\n",
      "Epoch 2/20\n",
      "278/278 [==============================] - 3s 10ms/step - loss: 221.7606 - val_loss: 886.9553\n",
      "Epoch 3/20\n",
      "278/278 [==============================] - 3s 10ms/step - loss: 194.1648 - val_loss: 805.6320\n",
      "Epoch 4/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 189.6561 - val_loss: 961.6819\n",
      "Epoch 5/20\n",
      "278/278 [==============================] - 3s 10ms/step - loss: 178.6871 - val_loss: 793.7977\n",
      "Epoch 6/20\n",
      "278/278 [==============================] - 3s 10ms/step - loss: 177.9328 - val_loss: 725.9621\n",
      "Epoch 7/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 170.4878 - val_loss: 747.6244\n",
      "Epoch 8/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 168.6939 - val_loss: 1006.4252\n",
      "Epoch 9/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 165.2650 - val_loss: 858.8205\n",
      "Epoch 10/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 161.2479 - val_loss: 746.6833\n",
      "Epoch 11/20\n",
      "278/278 [==============================] - 3s 12ms/step - loss: 159.0076 - val_loss: 750.3912\n",
      "Epoch 12/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 159.2987 - val_loss: 766.6962\n",
      "Epoch 13/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 152.0682 - val_loss: 738.5545\n",
      "Epoch 14/20\n",
      "278/278 [==============================] - 3s 12ms/step - loss: 151.7979 - val_loss: 814.5353\n",
      "Epoch 15/20\n",
      "278/278 [==============================] - 3s 12ms/step - loss: 150.5547 - val_loss: 648.1250\n",
      "Epoch 16/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 151.4535 - val_loss: 863.9540\n",
      "Epoch 17/20\n",
      "278/278 [==============================] - 3s 12ms/step - loss: 142.6792 - val_loss: 893.5520\n",
      "Epoch 18/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 141.5819 - val_loss: 833.3047\n",
      "Epoch 19/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 140.1879 - val_loss: 853.2841\n",
      "Epoch 20/20\n",
      "278/278 [==============================] - 3s 12ms/step - loss: 148.4303 - val_loss: 837.5359\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "[1.16218316e+01 2.22745151e+00 7.53919107e-01 7.60689125e-02\n",
      " 5.80733626e-02 4.69453854e-02 4.15078334e-02 3.47383080e-02\n",
      " 3.32981565e-02 2.89969632e-02 2.53785317e-02 1.84836056e-02\n",
      " 1.69423744e-02 1.35008525e-02 3.59058504e-03] \n",
      " Nb components:  2\n",
      "(17731, 30, 2) (17731, 1) (100, 30, 2)\n",
      "Epoch 1/20\n",
      "278/278 [==============================] - 4s 12ms/step - loss: 1126.6289 - val_loss: 900.7766\n",
      "Epoch 2/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 220.2926 - val_loss: 1120.5793\n",
      "Epoch 3/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 195.7609 - val_loss: 667.2241\n",
      "Epoch 4/20\n",
      "278/278 [==============================] - 3s 10ms/step - loss: 181.1488 - val_loss: 807.1113\n",
      "Epoch 5/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 177.6113 - val_loss: 883.7792\n",
      "Epoch 6/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 174.7022 - val_loss: 888.8145\n",
      "Epoch 7/20\n",
      "278/278 [==============================] - 3s 12ms/step - loss: 165.7505 - val_loss: 803.2195\n",
      "Epoch 8/20\n",
      "278/278 [==============================] - 4s 13ms/step - loss: 163.1833 - val_loss: 926.6280\n",
      "Epoch 9/20\n",
      "278/278 [==============================] - 3s 12ms/step - loss: 162.4624 - val_loss: 799.3563\n",
      "Epoch 10/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 155.9911 - val_loss: 716.3898\n",
      "Epoch 11/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 154.4787 - val_loss: 593.6932\n",
      "Epoch 12/20\n",
      "278/278 [==============================] - 3s 10ms/step - loss: 157.5299 - val_loss: 869.6930\n",
      "Epoch 13/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 158.2803 - val_loss: 785.9275\n",
      "Epoch 14/20\n",
      "278/278 [==============================] - 3s 10ms/step - loss: 148.9559 - val_loss: 788.0269\n",
      "Epoch 15/20\n",
      "278/278 [==============================] - 3s 10ms/step - loss: 143.9996 - val_loss: 830.4943\n",
      "Epoch 16/20\n",
      "278/278 [==============================] - 3s 10ms/step - loss: 142.4780 - val_loss: 752.7272\n",
      "Epoch 17/20\n",
      "278/278 [==============================] - 3s 10ms/step - loss: 148.0602 - val_loss: 834.2096\n",
      "Epoch 18/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 140.9585 - val_loss: 875.7225\n",
      "Epoch 19/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 142.3674 - val_loss: 793.8777\n",
      "Epoch 20/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 138.2004 - val_loss: 762.6233\n",
      "4/4 [==============================] - 0s 0s/step\n",
      "CPU times: total: 1min 1s\n",
      "Wall time: 5min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results_pca001 = pd.DataFrame()\n",
    "for SEED in range(5):  \n",
    "    set_seed(SEED)\n",
    "    mse = []\n",
    "    R2_val = []\n",
    "    RMSE = []\n",
    "    score_val = []\n",
    "    \n",
    "    # 0.20\t[64]\t0.3\ttanh\t32\t25\n",
    "    \n",
    "    # parameter's sample\n",
    "    # weights_file = \"weights_file_lstm_optimalmodel_all.h5\"\n",
    "    alpha = 0.1\n",
    "    sequence_length = 30\n",
    "    epochs = 20\n",
    "    nodes_per_layer = [128]\n",
    "    dropout = 0.2\n",
    "    activation = 'tanh'\n",
    "    batch_size = 64\n",
    "    remaining_sensors = remaining_sensors    \n",
    "    # Data prepration\n",
    "    X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "    \n",
    "    #PCA data reduction \n",
    "    X_cr_train, X_cr_test= StandardScaler().fit_transform(X_train_interim[remaining_sensors]), StandardScaler().fit_transform(X_test_interim[remaining_sensors])\n",
    "    pca = PCA()\n",
    "    component_train , component_test = pca.fit(X_cr_train).transform(X_cr_train), pca.transform(X_cr_test)\n",
    "    # print(pca.explained_variance_, np_component) # choos component which lambda >1 # kaiser\n",
    "\n",
    "    np_component = len(pca.explained_variance_[pca.explained_variance_>1])\n",
    "    print(pca.explained_variance_,'\\n', \"Nb components: \", np_component) # choos component which lambda >1 # kaiser\n",
    "    comp = ['comp' + str(i) for i in range(1,np_component+1)]\n",
    "    X_train_interim[comp],  X_test_interim[comp]= component_train[:, :np_component], component_test[:, :np_component]\n",
    "    \n",
    "\n",
    "    # create sequences train, test\n",
    "    train_array = gen_data_wrapper(X_train_interim, sequence_length, comp)\n",
    "    label_array = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'])\n",
    "\n",
    "    test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,comp, -99.))\n",
    "               for unit_nr in X_test_interim['Unit'].unique())\n",
    "    \n",
    "    test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "    test_rul = rul_piecewise_fct(y_test,rul_piecewise)\n",
    "    \n",
    "    input_shape = (sequence_length, len(comp))\n",
    "    model = model_lstm_1layer(input_shape, nodes_per_layer[0], dropout, activation)\n",
    "    print(train_array.shape, label_array.shape, test_array.shape)\n",
    "            \n",
    "    # Model fitting\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        start_time = time.time()\n",
    "        model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n",
    "        weights_file = model.get_weights()\n",
    "        model.set_weights(weights_file)  # reset optimizer and node weights before every training iteration\n",
    "        history = model.fit(train_array, label_array,\n",
    "                                validation_data=(test_array, test_rul),\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                # callbacks=[cb],\n",
    "                                verbose=1)\n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "        mse.append(history.history['val_loss'][-1])\n",
    "\n",
    "        y_hat_val_split = model.predict(test_array)\n",
    "        R2_val.append(r2_score(test_rul, y_hat_val_split))\n",
    "        RMSE.append(np.sqrt(mean_squared_error(test_rul, y_hat_val_split)))\n",
    "        score_val.append(compute_s_score(test_rul, y_hat_val_split))\n",
    "            \n",
    "        \n",
    "    #  append results\n",
    "    d = {'RMSE' :np.mean(RMSE), 'std_RMSE' :np.std(RMSE),\n",
    "         'S_score' :np.mean(score_val), 'std_S_score' :np.std(score_val),\n",
    "         'MSE':np.mean(mse), 'std_MSE':np.std(mse),\n",
    "         'nodes':str(nodes_per_layer), 'dropout':dropout, \n",
    "         'activation':activation, 'batch_size':batch_size, 'TW' : sequence_length,\n",
    "         'time': training_time}\n",
    "\n",
    "#     results = results.append(pd.DataFrame(d, index=[0]), ignore_index=True)\n",
    "    results_pca001 = pd.concat([results_pca001, pd.DataFrame(d, index=[0])], ignore_index=True)\n",
    "    results_pca001.to_csv('results/pca/fd001.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>std_RMSE</th>\n",
       "      <th>S_score</th>\n",
       "      <th>std_S_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>TW</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29.968799</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2033.496475</td>\n",
       "      <td>0.0</td>\n",
       "      <td>898.128906</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "      <td>64.759486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28.845945</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1967.829243</td>\n",
       "      <td>0.0</td>\n",
       "      <td>832.088501</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "      <td>60.036631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28.059698</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1822.027067</td>\n",
       "      <td>0.0</td>\n",
       "      <td>787.346619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "      <td>59.399234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28.940213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1941.406954</td>\n",
       "      <td>0.0</td>\n",
       "      <td>837.535889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "      <td>63.134585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27.615634</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1697.604402</td>\n",
       "      <td>0.0</td>\n",
       "      <td>762.623291</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "      <td>61.557082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RMSE  std_RMSE      S_score  std_S_score         MSE  std_MSE  nodes  \\\n",
       "0  29.968799       0.0  2033.496475          0.0  898.128906      0.0  [128]   \n",
       "1  28.845945       0.0  1967.829243          0.0  832.088501      0.0  [128]   \n",
       "2  28.059698       0.0  1822.027067          0.0  787.346619      0.0  [128]   \n",
       "3  28.940213       0.0  1941.406954          0.0  837.535889      0.0  [128]   \n",
       "4  27.615634       0.0  1697.604402          0.0  762.623291      0.0  [128]   \n",
       "\n",
       "   dropout activation  batch_size  TW       time  \n",
       "0      0.2       tanh          64  30  64.759486  \n",
       "1      0.2       tanh          64  30  60.036631  \n",
       "2      0.2       tanh          64  30  59.399234  \n",
       "3      0.2       tanh          64  30  63.134585  \n",
       "4      0.2       tanh          64  30  61.557082  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_pca001"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
