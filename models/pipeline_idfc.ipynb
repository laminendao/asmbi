{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sommaire: <a class=\"anchor\" id=\"sommaire\"></a>\n",
    "* [Sommaire](#sommaire)\n",
    "* [Package Loading](#package)\n",
    "* [Model selection](#model_selection)\n",
    "    * [FD001](#fd001)\n",
    "    * [FD002](#fd002)\n",
    "    * [FD003](#fd003)\n",
    "    * [FD004](#fd004)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package's loading <a id='package'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\ProgramData\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "from lime.lime_tabular import RecurrentTabularExplainer\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error, r2_score \n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn import preprocessing\n",
    "from keras import backend as K\n",
    "from sklearn.preprocessing import MinMaxScaler , StandardScaler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Activation, GRU\n",
    "from scipy import optimize\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "\n",
    "from sp_modif.model_function import *\n",
    "from sp_modif.methods import *\n",
    "from sp_modif.data_prep import *\n",
    "from sp_modif.evaluator import *\n",
    "from sp_modif.SHAP import *\n",
    "from sp_modif.L2X import *\n",
    "from methods import *\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEED = 0\n",
    "def set_seed(seed=SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    tf.random.set_seed(SEED)\n",
    "\n",
    "# Appeler la fonction pour fixer le seed\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FD004 <a class=\"anchor\" id=\"fd004\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61249, 27) (41214, 26) (248, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# Load data and preprocess\n",
    "with tf.device('/device:GPU:0'):\n",
    "    train, test, y_test = prepare_data('FD004.txt')\n",
    "    print(train.shape, test.shape, y_test.shape)\n",
    "    sensor_names = ['T20','T24','T30','T50','P20','P15','P30','Nf','Nc','epr','Ps30','phi',\n",
    "                    'NRf','NRc','BPR','farB','htBleed','Nf_dmd','PCNfR_dmd','W31','W32']\n",
    "\n",
    "    # remaining_sensors = [\"Ps30\", \"Nf\", \"phi\", \"BPR\", \"farB\"] # selection based on main_notebook\n",
    "    remaining_sensors = [\"Ps30\", \"Nf\", \"phi\"]\n",
    "\n",
    "    drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "\n",
    "rul_piecewise = 120\n",
    "train['RUL'].clip(upper=rul_piecewise, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "def model_lstm_1layer(input_shape, nodes_per_layer, dropout, activation):\n",
    "    \n",
    "    cb = keras.callbacks.EarlyStopping(monitor='loss', patience=4)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units = nodes_per_layer, activation=activation, \n",
    "                  input_shape=input_shape))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(256))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer=Adam(learning_rate=0.001))\n",
    "    # model.save_weights(weights_file)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\ProgramData\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "(51538, 40, 3) (51538, 1) (248, 40, 3)\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:From c:\\ProgramData\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "1611/1611 [==============================] - 10s 6ms/step - loss: 991.5342 - val_loss: 1473.9977\n",
      "Epoch 2/20\n",
      "1611/1611 [==============================] - 9s 6ms/step - loss: 448.5498 - val_loss: 1224.9937\n",
      "Epoch 3/20\n",
      "1611/1611 [==============================] - 9s 6ms/step - loss: 381.3220 - val_loss: 1110.1326\n",
      "Epoch 4/20\n",
      "1611/1611 [==============================] - 10s 6ms/step - loss: 336.9692 - val_loss: 1372.1982\n",
      "Epoch 5/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 321.1429 - val_loss: 1383.6591\n",
      "Epoch 6/20\n",
      "1611/1611 [==============================] - 10s 6ms/step - loss: 302.6805 - val_loss: 1355.4508\n",
      "Epoch 7/20\n",
      "1611/1611 [==============================] - 10s 6ms/step - loss: 287.3066 - val_loss: 1252.4161\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "(51538, 40, 3) (51538, 1) (248, 40, 3)\n",
      "Epoch 1/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 1025.2651 - val_loss: 1351.9329\n",
      "Epoch 2/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 448.1556 - val_loss: 1190.8267\n",
      "Epoch 3/20\n",
      "1611/1611 [==============================] - 10s 6ms/step - loss: 404.2617 - val_loss: 719.7515\n",
      "Epoch 4/20\n",
      "1611/1611 [==============================] - 10s 6ms/step - loss: 323.5200 - val_loss: 518.3289\n",
      "Epoch 5/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 303.1791 - val_loss: 468.9849\n",
      "Epoch 6/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 288.5741 - val_loss: 374.5723\n",
      "Epoch 7/20\n",
      "1611/1611 [==============================] - 13s 8ms/step - loss: 278.9640 - val_loss: 351.6107\n",
      "Epoch 8/20\n",
      "1611/1611 [==============================] - 12s 8ms/step - loss: 271.0198 - val_loss: 457.0888\n",
      "Epoch 9/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 264.5234 - val_loss: 485.7634\n",
      "Epoch 10/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 258.7679 - val_loss: 503.9623\n",
      "Epoch 11/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 251.4170 - val_loss: 521.2603\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "(51538, 40, 3) (51538, 1) (248, 40, 3)\n",
      "Epoch 1/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 938.7971 - val_loss: 1525.5012\n",
      "Epoch 2/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 440.1979 - val_loss: 1097.3773\n",
      "Epoch 3/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 352.2587 - val_loss: 858.9533\n",
      "Epoch 4/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 320.9924 - val_loss: 779.3619\n",
      "Epoch 5/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 305.8277 - val_loss: 817.9879\n",
      "Epoch 6/20\n",
      "1611/1611 [==============================] - 10s 7ms/step - loss: 296.1839 - val_loss: 811.8058\n",
      "Epoch 7/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 285.7628 - val_loss: 379.2087\n",
      "Epoch 8/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 277.6845 - val_loss: 329.3537\n",
      "Epoch 9/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 267.0364 - val_loss: 335.8980\n",
      "Epoch 10/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 258.5357 - val_loss: 346.0100\n",
      "Epoch 11/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 250.2529 - val_loss: 344.4039\n",
      "Epoch 12/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 243.4560 - val_loss: 312.2464\n",
      "Epoch 13/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 240.7079 - val_loss: 313.7154\n",
      "Epoch 14/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 235.8123 - val_loss: 341.1639\n",
      "Epoch 15/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 231.4961 - val_loss: 322.0762\n",
      "Epoch 16/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 230.4746 - val_loss: 339.2610\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "(51538, 40, 3) (51538, 1) (248, 40, 3)\n",
      "Epoch 1/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 906.2902 - val_loss: 1172.8663\n",
      "Epoch 2/20\n",
      "1611/1611 [==============================] - 10s 6ms/step - loss: 414.9416 - val_loss: 1469.5374\n",
      "Epoch 3/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 346.2082 - val_loss: 1524.6235\n",
      "Epoch 4/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 324.7162 - val_loss: 1550.8269\n",
      "Epoch 5/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 313.3398 - val_loss: 1401.6193\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "(51538, 40, 3) (51538, 1) (248, 40, 3)\n",
      "Epoch 1/20\n",
      "1611/1611 [==============================] - 17s 9ms/step - loss: 1057.6034 - val_loss: 1313.2345\n",
      "Epoch 2/20\n",
      "1611/1611 [==============================] - 14s 9ms/step - loss: 444.7391 - val_loss: 1326.5421\n",
      "Epoch 3/20\n",
      "1611/1611 [==============================] - 13s 8ms/step - loss: 416.0114 - val_loss: 1215.2212\n",
      "Epoch 4/20\n",
      "1611/1611 [==============================] - 13s 8ms/step - loss: 354.6866 - val_loss: 1259.2748\n",
      "Epoch 5/20\n",
      "1611/1611 [==============================] - 13s 8ms/step - loss: 307.5819 - val_loss: 1459.1547\n",
      "Epoch 6/20\n",
      "1611/1611 [==============================] - 13s 8ms/step - loss: 290.7725 - val_loss: 1656.1676\n",
      "Epoch 7/20\n",
      "1611/1611 [==============================] - 12s 8ms/step - loss: 280.2538 - val_loss: 1639.2563\n",
      "8/8 [==============================] - 1s 5ms/step\n",
      "CPU times: total: 22min 5s\n",
      "Wall time: 8min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# RMSE\tstd_RMSE\tS_score\tstd_S_score\tMSE\tstd_MSE\tnodes\tdropout\tactivation\tbatch_size\tTW\n",
    "# 1\t15.449126\t0.0\t1.242658e+03\t0.0\t238.675491\t0.0\t[64]\t0.2\ttanh\t32\t40\n",
    "# RMSE\tstd_RMSE\tS_score\tstd_S_score\tMSE\tstd_MSE\talpha\tnodes\tdropout\tactivation\tbatch_size\tTW\n",
    "# 2\t16.100150\t0.0\t1.752538e+03\t0.0\t259.214813\t0.0\t0.30\t[64]\t0.2\ttanh\t32\t40\n",
    "results = pd.DataFrame()\n",
    "\n",
    "for SEED in range(5):  \n",
    "    tf.random.set_seed(SEED)\n",
    "    mse = []\n",
    "    R2_val = []\n",
    "    RMSE = []\n",
    "    score_val = []\n",
    "    \n",
    "    # 0.20\t[64]\t0.3\ttanh\t32\t25\n",
    "    \n",
    "    # parameter's sample\n",
    "    weights_file = \"weights_file_lstm_optimalmodel_clv.h5\"\n",
    "    alpha = 0.3\n",
    "    sequence_length = 40\n",
    "    epochs = 20\n",
    "    nodes_per_layer = [64]\n",
    "    dropout = 0.2\n",
    "    activation = 'tanh'\n",
    "    batch_size = 32\n",
    "    remaining_sensors = remaining_sensors\n",
    "    # create model\n",
    "    input_shape = (sequence_length, len(remaining_sensors))\n",
    "    model = model_lstm_1layer(input_shape, nodes_per_layer[0], dropout, activation)\n",
    "    \n",
    "    # Data prepration\n",
    "    X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "\n",
    "    # create sequences train, test\n",
    "    train_array = gen_data_wrapper(X_train_interim, sequence_length,remaining_sensors)\n",
    "    label_array = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'])\n",
    "\n",
    "    test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,remaining_sensors, -99.))\n",
    "               for unit_nr in X_test_interim['Unit'].unique())\n",
    "    \n",
    "    test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "    test_rul = rul_piecewise_fct(y_test,rul_piecewise)\n",
    "    print(train_array.shape, label_array.shape, test_array.shape)\n",
    "    cb = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
    "            \n",
    "    # Model fitting\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        start_time = time.time()\n",
    "        weights_file = model.get_weights()\n",
    "        model.set_weights(weights_file)  # reset optimizer and node weights before every training iteration\n",
    "        history = model.fit(train_array, label_array,\n",
    "                                validation_data=(test_array, test_rul),\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                callbacks=[cb],\n",
    "                                verbose=1)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        training_time = end_time - start_time\n",
    "        mse.append(history.history['val_loss'][-1])\n",
    "\n",
    "        y_hat_val_split = model.predict(test_array)\n",
    "        R2_val.append(r2_score(test_rul, y_hat_val_split))\n",
    "        RMSE.append(np.sqrt(mean_squared_error(test_rul, y_hat_val_split)))\n",
    "        score_val.append(compute_s_score(test_rul, y_hat_val_split))\n",
    "            \n",
    "        \n",
    "    #  append results\n",
    "    d = {'RMSE' :np.mean(RMSE), 'std_RMSE' :np.std(RMSE),\n",
    "         'S_score' :np.mean(score_val), 'std_S_score' :np.std(score_val),\n",
    "         'MSE':np.mean(mse), 'std_MSE':np.std(mse),\n",
    "         'nodes':str(nodes_per_layer), 'dropout':dropout, \n",
    "         'activation':activation, 'batch_size':batch_size, 'TW' : sequence_length, \n",
    "         'time': training_time}\n",
    "\n",
    "#     results = results.append(pd.DataFrame(d, index=[0]), ignore_index=True)\n",
    "    results = pd.concat([results, pd.DataFrame(d, index=[0])], ignore_index=True)\n",
    "    results.to_csv('results/base/fd004.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>std_RMSE</th>\n",
       "      <th>S_score</th>\n",
       "      <th>std_S_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>TW</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33.318652</td>\n",
       "      <td>0.0</td>\n",
       "      <td>119058.063626</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1252.416138</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>69.976216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18.751287</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3837.726324</td>\n",
       "      <td>0.0</td>\n",
       "      <td>521.260254</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>122.973090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.670494</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3050.407626</td>\n",
       "      <td>0.0</td>\n",
       "      <td>339.261047</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>178.253519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34.247137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>121738.200432</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1401.619263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>55.933658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34.860023</td>\n",
       "      <td>0.0</td>\n",
       "      <td>172146.644374</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1639.256348</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>95.092187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RMSE  std_RMSE        S_score  std_S_score          MSE  std_MSE  \\\n",
       "0  33.318652       0.0  119058.063626          0.0  1252.416138      0.0   \n",
       "1  18.751287       0.0    3837.726324          0.0   521.260254      0.0   \n",
       "2  17.670494       0.0    3050.407626          0.0   339.261047      0.0   \n",
       "3  34.247137       0.0  121738.200432          0.0  1401.619263      0.0   \n",
       "4  34.860023       0.0  172146.644374          0.0  1639.256348      0.0   \n",
       "\n",
       "  nodes  dropout activation  batch_size  TW        time  \n",
       "0  [64]      0.2       tanh          32  40   69.976216  \n",
       "1  [64]      0.2       tanh          32  40  122.973090  \n",
       "2  [64]      0.2       tanh          32  40  178.253519  \n",
       "3  [64]      0.2       tanh          32  40   55.933658  \n",
       "4  [64]      0.2       tanh          32  40   95.092187  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FD002 <a class=\"anchor\" id=\"fd002\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53759, 27) (33991, 26) (259, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# Load data and preprocess\n",
    "train, test, y_test = prepare_data('FD002.txt')\n",
    "print(train.shape, test.shape, y_test.shape)\n",
    "sensor_names = ['T20','T24','T30','T50','P20','P15','P30','Nf','Nc','epr','Ps30','phi',\n",
    "                    'NRf','NRc','BPR','farB','htBleed','Nf_dmd','PCNfR_dmd','W31','W32']\n",
    "# remaining_sensors = [\"Ps30\", \"Nf\", \"phi\", \"epr\"] # selection based on main_notebook [1] \"Nf\" [1] \"Ps30\" [1] \"phi\"\n",
    "remaining_sensors = [\"Ps30\", \"Nf\", \"phi\"] # selection based on main_notebook [1] \"Nf\" [1] \"Ps30\" [1] \"phi\"\n",
    "drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "\n",
    "rul_piecewise = 125\n",
    "train['RUL'].clip(upper=rul_piecewise, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43619, 40, 3) (43619, 1) (259, 40, 3)\n",
      "Epoch 1/20\n",
      "341/341 [==============================] - 4s 9ms/step - loss: 2388.4629 - val_loss: 1790.1936\n",
      "Epoch 2/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 855.0757 - val_loss: 426.8504\n",
      "Epoch 3/20\n",
      "341/341 [==============================] - 4s 11ms/step - loss: 476.2307 - val_loss: 307.6125\n",
      "Epoch 4/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 416.4417 - val_loss: 340.2688\n",
      "Epoch 5/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 383.9802 - val_loss: 280.0737\n",
      "Epoch 6/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 371.8405 - val_loss: 288.0869\n",
      "Epoch 7/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 360.8690 - val_loss: 284.5528\n",
      "Epoch 8/20\n",
      "341/341 [==============================] - 4s 11ms/step - loss: 356.3782 - val_loss: 276.4583\n",
      "Epoch 9/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 343.8793 - val_loss: 275.4338\n",
      "Epoch 10/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 343.8907 - val_loss: 276.8447\n",
      "Epoch 11/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 338.5517 - val_loss: 273.7258\n",
      "Epoch 12/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 335.1748 - val_loss: 286.6601\n",
      "Epoch 13/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 331.6640 - val_loss: 322.2843\n",
      "Epoch 14/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 323.3736 - val_loss: 290.6533\n",
      "Epoch 15/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 326.9038 - val_loss: 344.3171\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "(43619, 40, 3) (43619, 1) (259, 40, 3)\n",
      "Epoch 1/20\n",
      "341/341 [==============================] - 4s 9ms/step - loss: 2437.6943 - val_loss: 1788.1250\n",
      "Epoch 2/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 989.3416 - val_loss: 506.9686\n",
      "Epoch 3/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 436.3492 - val_loss: 524.2650\n",
      "Epoch 4/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 394.1427 - val_loss: 520.3965\n",
      "Epoch 5/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 366.2691 - val_loss: 485.7656\n",
      "Epoch 6/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 360.7556 - val_loss: 436.8955\n",
      "Epoch 7/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 348.1662 - val_loss: 402.0252\n",
      "Epoch 8/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 346.1065 - val_loss: 424.0627\n",
      "Epoch 9/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 337.5359 - val_loss: 388.8614\n",
      "Epoch 10/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 336.4275 - val_loss: 394.0292\n",
      "Epoch 11/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 333.2657 - val_loss: 425.6618\n",
      "Epoch 12/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 327.4304 - val_loss: 370.3948\n",
      "Epoch 13/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 325.9308 - val_loss: 369.3006\n",
      "Epoch 14/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 320.1251 - val_loss: 300.5489\n",
      "Epoch 15/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 319.1971 - val_loss: 275.7902\n",
      "Epoch 16/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 319.3196 - val_loss: 272.5693\n",
      "Epoch 17/20\n",
      "341/341 [==============================] - 4s 11ms/step - loss: 315.3944 - val_loss: 277.5716\n",
      "Epoch 18/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 314.4394 - val_loss: 295.8140\n",
      "Epoch 19/20\n",
      "341/341 [==============================] - 4s 11ms/step - loss: 310.5552 - val_loss: 271.2692\n",
      "Epoch 20/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 313.9546 - val_loss: 259.6491\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "(43619, 40, 3) (43619, 1) (259, 40, 3)\n",
      "Epoch 1/20\n",
      "341/341 [==============================] - 5s 11ms/step - loss: 2368.0623 - val_loss: 1760.9148\n",
      "Epoch 2/20\n",
      "341/341 [==============================] - 4s 10ms/step - loss: 1131.0521 - val_loss: 502.7805\n",
      "Epoch 3/20\n",
      "341/341 [==============================] - 4s 10ms/step - loss: 418.3242 - val_loss: 432.9886\n",
      "Epoch 4/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 387.4585 - val_loss: 384.3557\n",
      "Epoch 5/20\n",
      "341/341 [==============================] - 4s 13ms/step - loss: 370.9112 - val_loss: 347.2989\n",
      "Epoch 6/20\n",
      "341/341 [==============================] - 4s 12ms/step - loss: 361.6508 - val_loss: 335.0884\n",
      "Epoch 7/20\n",
      "341/341 [==============================] - 4s 12ms/step - loss: 359.7403 - val_loss: 320.6023\n",
      "Epoch 8/20\n",
      "341/341 [==============================] - 4s 11ms/step - loss: 347.2095 - val_loss: 310.5867\n",
      "Epoch 9/20\n",
      "341/341 [==============================] - 4s 11ms/step - loss: 345.6387 - val_loss: 320.9255\n",
      "Epoch 10/20\n",
      "341/341 [==============================] - 4s 11ms/step - loss: 342.6803 - val_loss: 307.1434\n",
      "Epoch 11/20\n",
      "341/341 [==============================] - 5s 13ms/step - loss: 334.2591 - val_loss: 320.1285\n",
      "Epoch 12/20\n",
      "341/341 [==============================] - 4s 11ms/step - loss: 334.0517 - val_loss: 305.8424\n",
      "Epoch 13/20\n",
      "341/341 [==============================] - 4s 11ms/step - loss: 328.0684 - val_loss: 305.3603\n",
      "Epoch 14/20\n",
      "341/341 [==============================] - 4s 12ms/step - loss: 323.7570 - val_loss: 301.7728\n",
      "Epoch 15/20\n",
      "341/341 [==============================] - 4s 11ms/step - loss: 321.6016 - val_loss: 336.2405\n",
      "Epoch 16/20\n",
      "341/341 [==============================] - 4s 11ms/step - loss: 315.7021 - val_loss: 355.3106\n",
      "Epoch 17/20\n",
      "341/341 [==============================] - 4s 13ms/step - loss: 317.9129 - val_loss: 316.0560\n",
      "Epoch 18/20\n",
      "341/341 [==============================] - 4s 10ms/step - loss: 311.4698 - val_loss: 346.5418\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "(43619, 40, 3) (43619, 1) (259, 40, 3)\n",
      "Epoch 1/20\n",
      "341/341 [==============================] - 5s 12ms/step - loss: 2393.1133 - val_loss: 1777.1930\n",
      "Epoch 2/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 1143.7328 - val_loss: 421.5496\n",
      "Epoch 3/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 472.3230 - val_loss: 465.5604\n",
      "Epoch 4/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 404.4851 - val_loss: 537.1287\n",
      "Epoch 5/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 385.0496 - val_loss: 504.2571\n",
      "Epoch 6/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 369.1931 - val_loss: 458.7968\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "(43619, 40, 3) (43619, 1) (259, 40, 3)\n",
      "Epoch 1/20\n",
      "341/341 [==============================] - 5s 12ms/step - loss: 2363.1238 - val_loss: 1848.1183\n",
      "Epoch 2/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 1186.5194 - val_loss: 399.5976\n",
      "Epoch 3/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 412.0892 - val_loss: 406.6341\n",
      "Epoch 4/20\n",
      "341/341 [==============================] - 4s 11ms/step - loss: 387.2867 - val_loss: 391.7625\n",
      "Epoch 5/20\n",
      "341/341 [==============================] - 4s 12ms/step - loss: 366.9157 - val_loss: 376.9344\n",
      "Epoch 6/20\n",
      "341/341 [==============================] - 4s 11ms/step - loss: 353.4553 - val_loss: 420.5500\n",
      "Epoch 7/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 346.3037 - val_loss: 400.1771\n",
      "Epoch 8/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 343.1973 - val_loss: 385.1837\n",
      "Epoch 9/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 335.7742 - val_loss: 382.1605\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "CPU times: total: 10min 20s\n",
      "Wall time: 3min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# RMSE\tstd_RMSE\tS_score\tstd_S_score\tMSE\tstd_MSE\talpha\tnodes\tdropout\tactivation\tbatch_size\tTW\n",
    "# 16.603718\t0.0\t1.343367e+03\t0.0\t275.683502\t0.0\t0.20\t[32]\t0.1\ttanh\t128\t40\n",
    "results002 = pd.DataFrame()\n",
    "\n",
    "for SEED in range(5):  \n",
    "    tf.random.set_seed(SEED)\n",
    "    mse = []\n",
    "    R2_val = []\n",
    "    RMSE = []\n",
    "    score_val = []\n",
    "    \n",
    "    # 0.20\t[64]\t0.3\ttanh\t32\t25\n",
    "    \n",
    "    # parameter's sample\n",
    "    # weights_file = \"weights_file_lstm_optimalmodel_clv.h5\"\n",
    "    alpha = 0.2\n",
    "    sequence_length = 40\n",
    "    epochs = 20\n",
    "    nodes_per_layer = [32]\n",
    "    dropout = 0.1\n",
    "    activation = 'tanh'\n",
    "    batch_size = 128\n",
    "    remaining_sensors = remaining_sensors\n",
    "    # create model\n",
    "    input_shape = (sequence_length, len(remaining_sensors))    \n",
    "    # Data prepration\n",
    "    X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "\n",
    "    # create sequences train, test\n",
    "    train_array = gen_data_wrapper(X_train_interim, sequence_length,remaining_sensors)\n",
    "    label_array = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'])\n",
    "\n",
    "    test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,remaining_sensors, -99.))\n",
    "               for unit_nr in X_test_interim['Unit'].unique())\n",
    "    \n",
    "    test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "    test_rul = rul_piecewise_fct(y_test,rul_piecewise)\n",
    "    print(train_array.shape, label_array.shape, test_array.shape)\n",
    "    model = model_lstm_1layer(input_shape, nodes_per_layer[0], dropout, activation)\n",
    "       \n",
    "    # Model fitting\n",
    "    cb = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        start_time = time.time()\n",
    "        weights_file = model.get_weights()\n",
    "        model.set_weights(weights_file)  # reset optimizer and node weights before every training iteration\n",
    "        history = model.fit(train_array, label_array,\n",
    "                                validation_data=(test_array, test_rul),\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                callbacks=[cb],\n",
    "                                verbose=1)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        training_time = end_time - start_time\n",
    "        mse.append(history.history['val_loss'][-1])\n",
    "\n",
    "        y_hat_val_split = model.predict(test_array)\n",
    "        R2_val.append(r2_score(test_rul, y_hat_val_split))\n",
    "        RMSE.append(np.sqrt(mean_squared_error(test_rul, y_hat_val_split)))\n",
    "        score_val.append(compute_s_score(test_rul, y_hat_val_split))\n",
    "            \n",
    "        \n",
    "    #  append results\n",
    "    d = {'RMSE' :np.mean(RMSE), 'std_RMSE' :np.std(RMSE),\n",
    "         'S_score' :np.mean(score_val), 'std_S_score' :np.std(score_val),\n",
    "         'MSE':np.mean(mse), 'std_MSE':np.std(mse),\n",
    "         'nodes':str(nodes_per_layer), 'dropout':dropout, \n",
    "         'activation':activation, 'batch_size':batch_size, 'TW' : sequence_length, \n",
    "         'time': training_time}\n",
    "\n",
    "#     results = results.append(pd.DataFrame(d, index=[0]), ignore_index=True)\n",
    "    results002 = pd.concat([results002, pd.DataFrame(d, index=[0])], ignore_index=True)\n",
    "    results002.to_csv('results/base/fd002.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>std_RMSE</th>\n",
       "      <th>S_score</th>\n",
       "      <th>std_S_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>TW</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16.544660</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1329.995532</td>\n",
       "      <td>0.0</td>\n",
       "      <td>344.317139</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "      <td>40</td>\n",
       "      <td>46.287470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16.113632</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1322.375885</td>\n",
       "      <td>0.0</td>\n",
       "      <td>259.649139</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "      <td>40</td>\n",
       "      <td>59.768505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.371607</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1845.161542</td>\n",
       "      <td>0.0</td>\n",
       "      <td>346.541779</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "      <td>40</td>\n",
       "      <td>70.753855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20.531673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2653.026571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>458.796783</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "      <td>40</td>\n",
       "      <td>21.934217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19.414798</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3142.219070</td>\n",
       "      <td>0.0</td>\n",
       "      <td>382.160461</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "      <td>40</td>\n",
       "      <td>33.085212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RMSE  std_RMSE      S_score  std_S_score         MSE  std_MSE nodes  \\\n",
       "0  16.544660       0.0  1329.995532          0.0  344.317139      0.0  [32]   \n",
       "1  16.113632       0.0  1322.375885          0.0  259.649139      0.0  [32]   \n",
       "2  17.371607       0.0  1845.161542          0.0  346.541779      0.0  [32]   \n",
       "3  20.531673       0.0  2653.026571          0.0  458.796783      0.0  [32]   \n",
       "4  19.414798       0.0  3142.219070          0.0  382.160461      0.0  [32]   \n",
       "\n",
       "   dropout activation  batch_size  TW       time  \n",
       "0      0.1       tanh         128  40  46.287470  \n",
       "1      0.1       tanh         128  40  59.768505  \n",
       "2      0.1       tanh         128  40  70.753855  \n",
       "3      0.1       tanh         128  40  21.934217  \n",
       "4      0.1       tanh         128  40  33.085212  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FD003 <a class=\"anchor\" id=\"fd003\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24720, 27) (16596, 26) (100, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# Load data and preprocess\n",
    "train, test, y_test = prepare_data('FD003.txt')\n",
    "print(train.shape, test.shape, y_test.shape)\n",
    "sensor_names = ['T20','T24','T30','T50','P20','P15','P30','Nf','Nc','epr','Ps30','phi',\n",
    "                    'NRf','NRc','BPR','farB','htBleed','Nf_dmd','PCNfR_dmd','W31','W32']\n",
    "# remaining_sensors = [\"Ps30\", \"NRc\", \"W31\", \"BPR\"] # selection based on main_notebook [1] \"Nf\" [1] \"Ps30\" [1] \"phi\"\n",
    "remaining_sensors = [\"Ps30\", \"NRc\", \"W31\"] # selection based on main_notebook [1] \"Nf\" [1] \"Ps30\" [1] \"phi\"\n",
    "drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "\n",
    "rul_piecewise = 125\n",
    "train['RUL'].clip(upper=rul_piecewise, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21320, 35, 3) (21320, 1) (100, 35, 3)\n",
      "Epoch 1/20\n",
      "667/667 [==============================] - 7s 9ms/step - loss: 1924.9816 - val_loss: 1447.5288\n",
      "Epoch 2/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 504.8188 - val_loss: 690.5407\n",
      "Epoch 3/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 418.0533 - val_loss: 551.8301\n",
      "Epoch 4/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 377.1380 - val_loss: 493.5085\n",
      "Epoch 5/20\n",
      "667/667 [==============================] - 6s 8ms/step - loss: 345.7000 - val_loss: 556.4213\n",
      "Epoch 6/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 338.7039 - val_loss: 357.1455\n",
      "Epoch 7/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 327.7131 - val_loss: 350.6089\n",
      "Epoch 8/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 312.1043 - val_loss: 348.8107\n",
      "Epoch 9/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 305.1341 - val_loss: 322.0183\n",
      "Epoch 10/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 300.6976 - val_loss: 318.5475\n",
      "Epoch 11/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 289.6285 - val_loss: 327.7714\n",
      "Epoch 12/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 276.3220 - val_loss: 314.7443\n",
      "Epoch 13/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 276.3203 - val_loss: 331.1076\n",
      "Epoch 14/20\n",
      "667/667 [==============================] - 6s 9ms/step - loss: 272.0414 - val_loss: 327.1007\n",
      "Epoch 15/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 261.4550 - val_loss: 271.1956\n",
      "Epoch 16/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 257.1247 - val_loss: 286.0540\n",
      "Epoch 17/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 248.8775 - val_loss: 276.4264\n",
      "Epoch 18/20\n",
      "667/667 [==============================] - 7s 11ms/step - loss: 245.6430 - val_loss: 264.1893\n",
      "Epoch 19/20\n",
      "667/667 [==============================] - 6s 9ms/step - loss: 242.4875 - val_loss: 268.5195\n",
      "Epoch 20/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 233.1402 - val_loss: 247.9731\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "(21320, 35, 3) (21320, 1) (100, 35, 3)\n",
      "Epoch 1/20\n",
      "667/667 [==============================] - 8s 10ms/step - loss: 1984.3203 - val_loss: 1230.0511\n",
      "Epoch 2/20\n",
      "667/667 [==============================] - 7s 10ms/step - loss: 553.0701 - val_loss: 507.6968\n",
      "Epoch 3/20\n",
      "667/667 [==============================] - 8s 12ms/step - loss: 457.7014 - val_loss: 520.3049\n",
      "Epoch 4/20\n",
      "667/667 [==============================] - 6s 9ms/step - loss: 420.8738 - val_loss: 645.7241\n",
      "Epoch 5/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 371.2854 - val_loss: 436.4170\n",
      "Epoch 6/20\n",
      "667/667 [==============================] - 6s 8ms/step - loss: 345.7548 - val_loss: 378.4251\n",
      "Epoch 7/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 328.0590 - val_loss: 428.2503\n",
      "Epoch 8/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 306.0540 - val_loss: 344.6354\n",
      "Epoch 9/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 296.7693 - val_loss: 408.8329\n",
      "Epoch 10/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 288.2183 - val_loss: 297.6121\n",
      "Epoch 11/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 271.7137 - val_loss: 308.9841\n",
      "Epoch 12/20\n",
      "667/667 [==============================] - 6s 9ms/step - loss: 270.0789 - val_loss: 282.9742\n",
      "Epoch 13/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 261.1660 - val_loss: 256.3745\n",
      "Epoch 14/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 256.3114 - val_loss: 302.3030\n",
      "Epoch 15/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 252.4024 - val_loss: 329.0534\n",
      "Epoch 16/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 242.1982 - val_loss: 302.6770\n",
      "Epoch 17/20\n",
      "667/667 [==============================] - 6s 9ms/step - loss: 235.9177 - val_loss: 317.9587\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "(21320, 35, 3) (21320, 1) (100, 35, 3)\n",
      "Epoch 1/20\n",
      "667/667 [==============================] - 8s 10ms/step - loss: 1988.8673 - val_loss: 1195.9161\n",
      "Epoch 2/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 558.7130 - val_loss: 454.6783\n",
      "Epoch 3/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 452.8234 - val_loss: 489.6245\n",
      "Epoch 4/20\n",
      "667/667 [==============================] - 6s 8ms/step - loss: 386.4643 - val_loss: 450.5428\n",
      "Epoch 5/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 358.0291 - val_loss: 395.5703\n",
      "Epoch 6/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 334.3292 - val_loss: 403.6191\n",
      "Epoch 7/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 327.6500 - val_loss: 358.6826\n",
      "Epoch 8/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 315.1011 - val_loss: 394.7068\n",
      "Epoch 9/20\n",
      "667/667 [==============================] - 6s 8ms/step - loss: 302.6606 - val_loss: 330.2713\n",
      "Epoch 10/20\n",
      "667/667 [==============================] - 6s 9ms/step - loss: 294.4333 - val_loss: 317.8357\n",
      "Epoch 11/20\n",
      "667/667 [==============================] - 6s 8ms/step - loss: 290.7575 - val_loss: 313.9530\n",
      "Epoch 12/20\n",
      "667/667 [==============================] - 7s 10ms/step - loss: 285.0921 - val_loss: 318.0432\n",
      "Epoch 13/20\n",
      "667/667 [==============================] - 6s 8ms/step - loss: 280.7735 - val_loss: 285.5663\n",
      "Epoch 14/20\n",
      "667/667 [==============================] - 6s 9ms/step - loss: 273.3040 - val_loss: 284.6449\n",
      "Epoch 15/20\n",
      "667/667 [==============================] - 7s 11ms/step - loss: 265.3396 - val_loss: 300.8933\n",
      "Epoch 16/20\n",
      "667/667 [==============================] - 6s 10ms/step - loss: 261.9795 - val_loss: 308.0490\n",
      "Epoch 17/20\n",
      "667/667 [==============================] - 6s 9ms/step - loss: 261.8549 - val_loss: 309.3644\n",
      "Epoch 18/20\n",
      "667/667 [==============================] - 6s 9ms/step - loss: 247.2595 - val_loss: 264.1051\n",
      "Epoch 19/20\n",
      "667/667 [==============================] - 6s 9ms/step - loss: 242.0662 - val_loss: 249.2731\n",
      "Epoch 20/20\n",
      "667/667 [==============================] - 6s 9ms/step - loss: 234.7652 - val_loss: 253.3201\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "(21320, 35, 3) (21320, 1) (100, 35, 3)\n",
      "Epoch 1/20\n",
      "667/667 [==============================] - 9s 11ms/step - loss: 1923.7698 - val_loss: 1212.8245\n",
      "Epoch 2/20\n",
      "667/667 [==============================] - 7s 10ms/step - loss: 517.5055 - val_loss: 489.5136\n",
      "Epoch 3/20\n",
      "667/667 [==============================] - 6s 9ms/step - loss: 455.8651 - val_loss: 461.4901\n",
      "Epoch 4/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 410.5266 - val_loss: 451.4640\n",
      "Epoch 5/20\n",
      "667/667 [==============================] - 4s 6ms/step - loss: 368.3842 - val_loss: 417.4646\n",
      "Epoch 6/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 342.5925 - val_loss: 368.4566\n",
      "Epoch 7/20\n",
      "667/667 [==============================] - 4s 7ms/step - loss: 322.3210 - val_loss: 566.2628\n",
      "Epoch 8/20\n",
      "667/667 [==============================] - 6s 8ms/step - loss: 310.8549 - val_loss: 348.2068\n",
      "Epoch 9/20\n",
      "667/667 [==============================] - 4s 7ms/step - loss: 295.9763 - val_loss: 325.5897\n",
      "Epoch 10/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 290.9412 - val_loss: 322.6270\n",
      "Epoch 11/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 280.6147 - val_loss: 357.3833\n",
      "Epoch 12/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 274.1840 - val_loss: 348.4930\n",
      "Epoch 13/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 271.0341 - val_loss: 299.5467\n",
      "Epoch 14/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 264.6718 - val_loss: 275.3575\n",
      "Epoch 15/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 254.1205 - val_loss: 255.3020\n",
      "Epoch 16/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 248.8554 - val_loss: 262.8870\n",
      "Epoch 17/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 243.8037 - val_loss: 256.0658\n",
      "Epoch 18/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 235.4557 - val_loss: 293.4504\n",
      "Epoch 19/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 229.0421 - val_loss: 249.4876\n",
      "Epoch 20/20\n",
      "667/667 [==============================] - 6s 8ms/step - loss: 221.0355 - val_loss: 257.6437\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "(21320, 35, 3) (21320, 1) (100, 35, 3)\n",
      "Epoch 1/20\n",
      "667/667 [==============================] - 7s 9ms/step - loss: 1743.3201 - val_loss: 525.9451\n",
      "Epoch 2/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 485.8029 - val_loss: 622.6305\n",
      "Epoch 3/20\n",
      "667/667 [==============================] - 6s 9ms/step - loss: 431.1558 - val_loss: 389.9030\n",
      "Epoch 4/20\n",
      "667/667 [==============================] - 6s 9ms/step - loss: 384.9930 - val_loss: 469.2083\n",
      "Epoch 5/20\n",
      "667/667 [==============================] - 6s 9ms/step - loss: 350.9874 - val_loss: 409.2644\n",
      "Epoch 6/20\n",
      "667/667 [==============================] - 6s 9ms/step - loss: 324.5880 - val_loss: 440.4640\n",
      "Epoch 7/20\n",
      "667/667 [==============================] - 6s 9ms/step - loss: 320.1678 - val_loss: 314.3606\n",
      "Epoch 8/20\n",
      "667/667 [==============================] - 6s 9ms/step - loss: 302.1680 - val_loss: 361.9942\n",
      "Epoch 9/20\n",
      "667/667 [==============================] - 6s 9ms/step - loss: 297.7388 - val_loss: 295.6501\n",
      "Epoch 10/20\n",
      "667/667 [==============================] - 7s 10ms/step - loss: 283.3624 - val_loss: 300.8262\n",
      "Epoch 11/20\n",
      "667/667 [==============================] - 6s 8ms/step - loss: 275.3407 - val_loss: 263.6738\n",
      "Epoch 12/20\n",
      "667/667 [==============================] - 6s 9ms/step - loss: 267.8218 - val_loss: 256.3793\n",
      "Epoch 13/20\n",
      "667/667 [==============================] - 6s 10ms/step - loss: 259.1377 - val_loss: 240.1235\n",
      "Epoch 14/20\n",
      "667/667 [==============================] - 8s 11ms/step - loss: 253.3606 - val_loss: 253.0368\n",
      "Epoch 15/20\n",
      "667/667 [==============================] - 7s 10ms/step - loss: 250.9396 - val_loss: 256.4036\n",
      "Epoch 16/20\n",
      "667/667 [==============================] - 10s 15ms/step - loss: 245.8347 - val_loss: 248.1823\n",
      "Epoch 17/20\n",
      "667/667 [==============================] - 7s 10ms/step - loss: 238.1693 - val_loss: 279.5116\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "CPU times: total: 23min 22s\n",
      "Wall time: 9min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# \tRMSE\tstd_RMSE\tS_score\tstd_S_score\tMSE\tstd_MSE\talpha\tnodes\tdropout\tactivation\tbatch_size\tTW\n",
    "# 43\t15.404917\t0.0\t8.255290e+02\t0.0\t237.311462\t0.0\t0.10\t[64]\t0.2\ttanh\t32\t35\n",
    "results003 = pd.DataFrame()\n",
    "\n",
    "for SEED in range(5):  \n",
    "    tf.random.set_seed(SEED)\n",
    "    mse = []\n",
    "    R2_val = []\n",
    "    RMSE = []\n",
    "    score_val = []\n",
    "    \n",
    "    # 0.20\t[64]\t0.3\ttanh\t32\t25\n",
    "    \n",
    "    # parameter's sample\n",
    "    # weights_file = \"weights_file_lstm_optimalmodel_clv.h5\"\n",
    "    alpha = 0.1\n",
    "    sequence_length = 35\n",
    "    epochs = 20\n",
    "    nodes_per_layer = [64]\n",
    "    dropout = 0.2\n",
    "    activation = 'tanh'\n",
    "    batch_size = 32\n",
    "    remaining_sensors = remaining_sensors\n",
    "    # create model\n",
    "    input_shape = (sequence_length, len(remaining_sensors))    \n",
    "    # Data prepration\n",
    "    X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "\n",
    "    # create sequences train, test\n",
    "    train_array = gen_data_wrapper(X_train_interim, sequence_length,remaining_sensors)\n",
    "    label_array = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'])\n",
    "\n",
    "    test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,remaining_sensors, -99.))\n",
    "               for unit_nr in X_test_interim['Unit'].unique())\n",
    "    \n",
    "    test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "    test_rul = rul_piecewise_fct(y_test,rul_piecewise)\n",
    "    print(train_array.shape, label_array.shape, test_array.shape)\n",
    "    model = model_lstm_1layer(input_shape, nodes_per_layer[0], dropout, activation)\n",
    "       \n",
    "    # Model fitting\n",
    "    cb = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        start_time = time.time()\n",
    "        weights_file = model.get_weights()\n",
    "        model.set_weights(weights_file)  # reset optimizer and node weights before every training iteration\n",
    "        history = model.fit(train_array, label_array,\n",
    "                                validation_data=(test_array, test_rul),\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                callbacks=[cb],\n",
    "                                verbose=1)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        training_time = end_time - start_time\n",
    "        mse.append(history.history['val_loss'][-1])\n",
    "\n",
    "        y_hat_val_split = model.predict(test_array)\n",
    "        R2_val.append(r2_score(test_rul, y_hat_val_split))\n",
    "        RMSE.append(np.sqrt(mean_squared_error(test_rul, y_hat_val_split)))\n",
    "        score_val.append(compute_s_score(test_rul, y_hat_val_split))\n",
    "            \n",
    "        \n",
    "    #  append results\n",
    "    d = {'RMSE' :np.mean(RMSE), 'std_RMSE' :np.std(RMSE),\n",
    "         'S_score' :np.mean(score_val), 'std_S_score' :np.std(score_val),\n",
    "         'MSE':np.mean(mse), 'std_MSE':np.std(mse),\n",
    "         'nodes':str(nodes_per_layer), 'dropout':dropout, \n",
    "         'activation':activation, 'batch_size':batch_size, 'TW' : sequence_length, \n",
    "         'time': training_time}\n",
    "\n",
    "#     results = results.append(pd.DataFrame(d, index=[0]), ignore_index=True)\n",
    "    results003 = pd.concat([results003, pd.DataFrame(d, index=[0])], ignore_index=True)\n",
    "    results003.to_csv('results/base/fd003.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>std_RMSE</th>\n",
       "      <th>S_score</th>\n",
       "      <th>std_S_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>TW</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15.747161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>716.191246</td>\n",
       "      <td>0.0</td>\n",
       "      <td>247.973083</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "      <td>108.887381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16.011698</td>\n",
       "      <td>0.0</td>\n",
       "      <td>945.740418</td>\n",
       "      <td>0.0</td>\n",
       "      <td>317.958740</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "      <td>97.042535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.916034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1086.238069</td>\n",
       "      <td>0.0</td>\n",
       "      <td>253.320114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "      <td>119.233871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.051283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1241.762548</td>\n",
       "      <td>0.0</td>\n",
       "      <td>257.643677</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "      <td>106.356065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15.495918</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1001.098870</td>\n",
       "      <td>0.0</td>\n",
       "      <td>279.511597</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "      <td>110.428248</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RMSE  std_RMSE      S_score  std_S_score         MSE  std_MSE nodes  \\\n",
       "0  15.747161       0.0   716.191246          0.0  247.973083      0.0  [64]   \n",
       "1  16.011698       0.0   945.740418          0.0  317.958740      0.0  [64]   \n",
       "2  15.916034       0.0  1086.238069          0.0  253.320114      0.0  [64]   \n",
       "3  16.051283       0.0  1241.762548          0.0  257.643677      0.0  [64]   \n",
       "4  15.495918       0.0  1001.098870          0.0  279.511597      0.0  [64]   \n",
       "\n",
       "   dropout activation  batch_size  TW        time  \n",
       "0      0.2       tanh          32  35  108.887381  \n",
       "1      0.2       tanh          32  35   97.042535  \n",
       "2      0.2       tanh          32  35  119.233871  \n",
       "3      0.2       tanh          32  35  106.356065  \n",
       "4      0.2       tanh          32  35  110.428248  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FD001 <a class=\"anchor\" id=\"fd001\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20631, 27) (13096, 26) (100, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# Load data and preprocess\n",
    "train, test, y_test = prepare_data('FD001.txt')\n",
    "print(train.shape, test.shape, y_test.shape)\n",
    "sensor_names = ['T20','T24','T30','T50','P20','P15','P30','Nf','Nc','epr','Ps30','phi',\n",
    "                    'NRf','NRc','BPR','farB','htBleed','Nf_dmd','PCNfR_dmd','W31','W32']\n",
    "# remaining_sensors = [\"Ps30\", \"Nf\", \"P30\", \"BPR\"] # selection based on main_notebook \n",
    "remaining_sensors = [\"Ps30\", \"Nf\", \"P30\"] # selection based on main_notebook \n",
    "drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "\n",
    "rul_piecewise = 125\n",
    "train['RUL'].clip(upper=rul_piecewise, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17731, 30, 3) (17731, 1) (100, 30, 3)\n",
      "Epoch 1/20\n",
      "278/278 [==============================] - 8s 22ms/step - loss: 2104.4343 - val_loss: 1636.0094\n",
      "Epoch 2/20\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 1424.5371 - val_loss: 814.5938\n",
      "Epoch 3/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 803.5678 - val_loss: 682.2605\n",
      "Epoch 4/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 556.0916 - val_loss: 350.6610\n",
      "Epoch 5/20\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 405.3134 - val_loss: 556.1797\n",
      "Epoch 6/20\n",
      "278/278 [==============================] - 5s 19ms/step - loss: 374.6635 - val_loss: 329.7100\n",
      "Epoch 7/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 344.5364 - val_loss: 253.1703\n",
      "Epoch 8/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 321.8900 - val_loss: 274.8988\n",
      "Epoch 9/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 304.8224 - val_loss: 298.3569\n",
      "Epoch 10/20\n",
      "278/278 [==============================] - 6s 21ms/step - loss: 302.8486 - val_loss: 268.5561\n",
      "Epoch 11/20\n",
      "278/278 [==============================] - 5s 19ms/step - loss: 296.4331 - val_loss: 243.2950\n",
      "Epoch 12/20\n",
      "278/278 [==============================] - 5s 19ms/step - loss: 296.4678 - val_loss: 392.6855\n",
      "Epoch 13/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 284.0958 - val_loss: 220.9801\n",
      "Epoch 14/20\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 282.7371 - val_loss: 417.3572\n",
      "Epoch 15/20\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 305.0593 - val_loss: 250.9204\n",
      "Epoch 16/20\n",
      "278/278 [==============================] - 6s 20ms/step - loss: 489.1009 - val_loss: 322.7437\n",
      "Epoch 17/20\n",
      "278/278 [==============================] - 6s 22ms/step - loss: 344.2852 - val_loss: 570.0810\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "(17731, 30, 3) (17731, 1) (100, 30, 3)\n",
      "Epoch 1/20\n",
      "278/278 [==============================] - 8s 23ms/step - loss: 2092.9136 - val_loss: 1617.8198\n",
      "Epoch 2/20\n",
      "278/278 [==============================] - 5s 19ms/step - loss: 1417.7229 - val_loss: 819.4114\n",
      "Epoch 3/20\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 781.1060 - val_loss: 1110.3033\n",
      "Epoch 4/20\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 770.2651 - val_loss: 4828.5454\n",
      "Epoch 5/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 725.1622 - val_loss: 756.1273\n",
      "Epoch 6/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 527.3673 - val_loss: 331.2412\n",
      "Epoch 7/20\n",
      "278/278 [==============================] - 5s 19ms/step - loss: 405.3520 - val_loss: 326.6478\n",
      "Epoch 8/20\n",
      "278/278 [==============================] - 6s 20ms/step - loss: 357.8334 - val_loss: 332.7777\n",
      "Epoch 9/20\n",
      "278/278 [==============================] - 5s 20ms/step - loss: 344.0984 - val_loss: 311.3667\n",
      "Epoch 10/20\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 330.1221 - val_loss: 283.1306\n",
      "Epoch 11/20\n",
      "278/278 [==============================] - 5s 16ms/step - loss: 329.2754 - val_loss: 310.1310\n",
      "Epoch 12/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 315.3158 - val_loss: 261.3541\n",
      "Epoch 13/20\n",
      "278/278 [==============================] - 5s 19ms/step - loss: 311.4457 - val_loss: 322.9410\n",
      "Epoch 14/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 311.5634 - val_loss: 240.6054\n",
      "Epoch 15/20\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 307.2564 - val_loss: 342.6538\n",
      "Epoch 16/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 295.3783 - val_loss: 344.6459\n",
      "Epoch 17/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 293.3015 - val_loss: 337.1672\n",
      "Epoch 18/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 302.9433 - val_loss: 239.8894\n",
      "Epoch 19/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 292.9107 - val_loss: 294.8789\n",
      "Epoch 20/20\n",
      "278/278 [==============================] - 5s 16ms/step - loss: 290.6948 - val_loss: 254.5862\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "(17731, 30, 3) (17731, 1) (100, 30, 3)\n",
      "Epoch 1/20\n",
      "278/278 [==============================] - 7s 19ms/step - loss: 2102.1011 - val_loss: 1747.8093\n",
      "Epoch 2/20\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 1465.2866 - val_loss: 1364.2700\n",
      "Epoch 3/20\n",
      "278/278 [==============================] - 5s 16ms/step - loss: 832.8393 - val_loss: 734.3212\n",
      "Epoch 4/20\n",
      "278/278 [==============================] - 5s 19ms/step - loss: 706.8919 - val_loss: 892.9930\n",
      "Epoch 5/20\n",
      "278/278 [==============================] - 5s 16ms/step - loss: 530.5651 - val_loss: 702.8372\n",
      "Epoch 6/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 416.8217 - val_loss: 298.5729\n",
      "Epoch 7/20\n",
      "278/278 [==============================] - 5s 16ms/step - loss: 384.7279 - val_loss: 304.8066\n",
      "Epoch 8/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 354.8194 - val_loss: 314.3527\n",
      "Epoch 9/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 324.3485 - val_loss: 250.3754\n",
      "Epoch 10/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 318.4417 - val_loss: 272.5110\n",
      "Epoch 11/20\n",
      "278/278 [==============================] - 5s 19ms/step - loss: 314.8214 - val_loss: 232.2363\n",
      "Epoch 12/20\n",
      "278/278 [==============================] - 6s 20ms/step - loss: 312.0950 - val_loss: 225.6196\n",
      "Epoch 13/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 302.0645 - val_loss: 255.0017\n",
      "Epoch 14/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 306.1461 - val_loss: 302.1035\n",
      "Epoch 15/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 299.8694 - val_loss: 350.2993\n",
      "Epoch 16/20\n",
      "278/278 [==============================] - 4s 13ms/step - loss: 318.0790 - val_loss: 255.7037\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "(17731, 30, 3) (17731, 1) (100, 30, 3)\n",
      "Epoch 1/20\n",
      "278/278 [==============================] - 6s 16ms/step - loss: 2112.3994 - val_loss: 1582.2438\n",
      "Epoch 2/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 1319.7635 - val_loss: 900.0034\n",
      "Epoch 3/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 638.4852 - val_loss: 356.8838\n",
      "Epoch 4/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 439.2119 - val_loss: 312.0743\n",
      "Epoch 5/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 371.8061 - val_loss: 255.7014\n",
      "Epoch 6/20\n",
      "278/278 [==============================] - 7s 25ms/step - loss: 335.2336 - val_loss: 249.9306\n",
      "Epoch 7/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 332.2822 - val_loss: 307.6981\n",
      "Epoch 8/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 312.3262 - val_loss: 292.6809\n",
      "Epoch 9/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 312.3563 - val_loss: 293.8411\n",
      "Epoch 10/20\n",
      "278/278 [==============================] - 6s 22ms/step - loss: 301.8765 - val_loss: 231.3595\n",
      "Epoch 11/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 294.1272 - val_loss: 221.9328\n",
      "Epoch 12/20\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 295.3952 - val_loss: 410.0928\n",
      "Epoch 13/20\n",
      "278/278 [==============================] - 5s 19ms/step - loss: 282.8914 - val_loss: 333.1070\n",
      "Epoch 14/20\n",
      "278/278 [==============================] - 5s 19ms/step - loss: 290.6242 - val_loss: 247.9713\n",
      "Epoch 15/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 288.9475 - val_loss: 249.3601\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "(17731, 30, 3) (17731, 1) (100, 30, 3)\n",
      "Epoch 1/20\n",
      "278/278 [==============================] - 7s 19ms/step - loss: 2137.7239 - val_loss: 1598.8375\n",
      "Epoch 2/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 1389.1479 - val_loss: 812.8906\n",
      "Epoch 3/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 776.6389 - val_loss: 784.6106\n",
      "Epoch 4/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 540.3818 - val_loss: 386.0674\n",
      "Epoch 5/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 421.3092 - val_loss: 488.4331\n",
      "Epoch 6/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 385.0021 - val_loss: 286.2883\n",
      "Epoch 7/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 351.7481 - val_loss: 430.3728\n",
      "Epoch 8/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 346.9066 - val_loss: 262.3956\n",
      "Epoch 9/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 329.0202 - val_loss: 248.1878\n",
      "Epoch 10/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 325.8187 - val_loss: 233.3742\n",
      "Epoch 11/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 315.0913 - val_loss: 262.5118\n",
      "Epoch 12/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 301.1965 - val_loss: 262.5267\n",
      "Epoch 13/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 298.9255 - val_loss: 247.0846\n",
      "Epoch 14/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 302.5981 - val_loss: 223.9469\n",
      "Epoch 15/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 294.8708 - val_loss: 232.8396\n",
      "Epoch 16/20\n",
      "278/278 [==============================] - 4s 13ms/step - loss: 293.5584 - val_loss: 235.0318\n",
      "Epoch 17/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 284.4325 - val_loss: 242.4458\n",
      "Epoch 18/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 288.0204 - val_loss: 217.9851\n",
      "Epoch 19/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 280.8847 - val_loss: 222.7762\n",
      "Epoch 20/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 282.4293 - val_loss: 234.1001\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "CPU times: total: 16min 23s\n",
      "Wall time: 7min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# RMSE\tstd_RMSE\tS_score\tstd_S_score\tMSE\tstd_MSE\talpha\tnodes\tdropout\tactivation\tbatch_size\tTW\n",
    "# 21\t13.768992\t0.0\t3.830708e+02\t0.0\t189.585159\t0.0\t0.10\t[128]\t0.2\ttanh\t64\t30\n",
    "results001 = pd.DataFrame()\n",
    "\n",
    "for SEED in range(5):  \n",
    "    set_seed(SEED)\n",
    "    mse = []\n",
    "    R2_val = []\n",
    "    RMSE = []\n",
    "    score_val = []\n",
    "    \n",
    "    # 0.20\t[64]\t0.3\ttanh\t32\t25\n",
    "    \n",
    "    # parameter's sample\n",
    "    # weights_file = \"weights_file_lstm_optimalmodel_clv.h5\"\n",
    "    alpha = 0.1\n",
    "    sequence_length = 30\n",
    "    epochs = 20\n",
    "    nodes_per_layer = [128]\n",
    "    dropout = 0.2\n",
    "    activation = 'tanh'\n",
    "    batch_size = 64\n",
    "    remaining_sensors = remaining_sensors\n",
    "    # create model\n",
    "    input_shape = (sequence_length, len(remaining_sensors))    \n",
    "    # Data prepration\n",
    "    X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "\n",
    "    # create sequences train, test\n",
    "    train_array = gen_data_wrapper(X_train_interim, sequence_length,remaining_sensors)\n",
    "    label_array = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'])\n",
    "\n",
    "    test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,remaining_sensors, -99.))\n",
    "               for unit_nr in X_test_interim['Unit'].unique())\n",
    "    \n",
    "    test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "    test_rul = rul_piecewise_fct(y_test,rul_piecewise)\n",
    "    print(train_array.shape, label_array.shape, test_array.shape)\n",
    "    model = model_lstm_1layer(input_shape, nodes_per_layer[0], dropout, activation)\n",
    "       \n",
    "    # Model fitting\n",
    "    cb = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        start_time = time.time()\n",
    "        weights_file = model.get_weights()\n",
    "        model.set_weights(weights_file)  # reset optimizer and node weights before every training iteration\n",
    "        history = model.fit(train_array, label_array,\n",
    "                                validation_data=(test_array, test_rul),\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                callbacks=[cb],\n",
    "                                verbose=1)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        training_time = end_time - start_time\n",
    "        mse.append(history.history['val_loss'][-1])\n",
    "\n",
    "        y_hat_val_split = model.predict(test_array)\n",
    "        R2_val.append(r2_score(test_rul, y_hat_val_split))\n",
    "        RMSE.append(np.sqrt(mean_squared_error(test_rul, y_hat_val_split)))\n",
    "        score_val.append(compute_s_score(test_rul, y_hat_val_split))\n",
    "            \n",
    "        \n",
    "    #  append results\n",
    "    d = {'RMSE' :np.mean(RMSE), 'std_RMSE' :np.std(RMSE),\n",
    "         'S_score' :np.mean(score_val), 'std_S_score' :np.std(score_val),\n",
    "         'MSE':np.mean(mse), 'std_MSE':np.std(mse),\n",
    "         'nodes':str(nodes_per_layer), 'dropout':dropout, \n",
    "         'activation':activation, 'batch_size':batch_size, 'TW' : sequence_length, \n",
    "         'time': training_time}\n",
    "\n",
    "#     results = results.append(pd.DataFrame(d, index=[0]), ignore_index=True)\n",
    "    results001 = pd.concat([results001, pd.DataFrame(d, index=[0])], ignore_index=True)\n",
    "    results001.to_csv('results/base/fd001.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>std_RMSE</th>\n",
       "      <th>S_score</th>\n",
       "      <th>std_S_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>TW</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.865401</td>\n",
       "      <td>0.0</td>\n",
       "      <td>393.923223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>570.080994</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "      <td>89.534018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.955759</td>\n",
       "      <td>0.0</td>\n",
       "      <td>413.238111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>254.586243</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "      <td>100.249789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.020641</td>\n",
       "      <td>0.0</td>\n",
       "      <td>367.751512</td>\n",
       "      <td>0.0</td>\n",
       "      <td>255.703674</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "      <td>74.453083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.897409</td>\n",
       "      <td>0.0</td>\n",
       "      <td>356.586710</td>\n",
       "      <td>0.0</td>\n",
       "      <td>249.360062</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "      <td>73.219218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15.300330</td>\n",
       "      <td>0.0</td>\n",
       "      <td>432.614388</td>\n",
       "      <td>0.0</td>\n",
       "      <td>234.100113</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "      <td>83.403499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RMSE  std_RMSE     S_score  std_S_score         MSE  std_MSE  nodes  \\\n",
       "0  14.865401       0.0  393.923223          0.0  570.080994      0.0  [128]   \n",
       "1  15.955759       0.0  413.238111          0.0  254.586243      0.0  [128]   \n",
       "2  15.020641       0.0  367.751512          0.0  255.703674      0.0  [128]   \n",
       "3  14.897409       0.0  356.586710          0.0  249.360062      0.0  [128]   \n",
       "4  15.300330       0.0  432.614388          0.0  234.100113      0.0  [128]   \n",
       "\n",
       "   dropout activation  batch_size  TW        time  \n",
       "0      0.2       tanh          64  30   89.534018  \n",
       "1      0.2       tanh          64  30  100.249789  \n",
       "2      0.2       tanh          64  30   74.453083  \n",
       "3      0.2       tanh          64  30   73.219218  \n",
       "4      0.2       tanh          64  30   83.403499  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results001"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
