{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\ProgramData\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === Configuration des chemins ===\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "# === Bibliothèques standards ===\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# === Manipulation de données ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# === Visualisation ===\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# === Machine Learning / Deep Learning ===\n",
    "import keras\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential, optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Activation, GRU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "from lime.lime_tabular import RecurrentTabularExplainer\n",
    "from scipy import optimize\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Modules internes (utils) ===\n",
    "from utils.model_function import *\n",
    "from utils.methods import *\n",
    "from utils.data_prep import *\n",
    "from utils.evaluator import *\n",
    "from utils.SHAP import *\n",
    "from utils.L2X import *\n",
    "\n",
    "# === Affichage inline pour Jupyter Notebook (à activer uniquement si nécessaire) ===\n",
    "# %matplotlib inline  # Décommenter si dans un notebook\n",
    "\n",
    "# === Suppression des avertissements ===\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# === Initialisation du seed pour reproductibilité ===\n",
    "SEED = 0\n",
    "\n",
    "def set_seed(seed=SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61249, 27) (41214, 26) (248, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# Load data and preprocess\n",
    "train, test, y_test = prepare_data('FD004.txt')\n",
    "print(train.shape, test.shape, y_test.shape)\n",
    "sensor_names = ['T20','T24','T30','T50','P20','P15','P30','Nf','Nc','epr','Ps30','phi',\n",
    "                    'NRf','NRc','BPR','farB','htBleed','Nf_dmd','PCNfR_dmd','W31','W32']\n",
    "\n",
    "remaining_sensors = ['T24','T30','T50', 'P15', 'P30','Nf','Nc', 'epr','Ps30','phi',\n",
    "                    'NRf','NRc','BPR', 'farB','htBleed','W31','W32']\n",
    "drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "\n",
    "rul_piecewise = 120\n",
    "train['RUL'].clip(upper=rul_piecewise, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_lstm_1layer(input_shape, nodes_per_layer, dropout, activation):\n",
    "    \n",
    "    cb = keras.callbacks.EarlyStopping(monitor='loss', patience=4)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units = nodes_per_layer, activation=activation, \n",
    "                  input_shape=input_shape))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(256))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer=Adam(learning_rate=0.001))\n",
    "    # model.save_weights(weights_file)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_cr_train, X_cr_test= StandardScaler().fit_transform(X_train_interim[remaining_sensors]), StandardScaler().fit_transform(X_test_interim[remaining_sensors])\n",
    "# pca = PCA()\n",
    "\n",
    "# component_train , component_test = pca.fit(X_cr_train).transform(X_cr_train), pca.transform(X_cr_test)\n",
    "# # print(pca.explained_variance_, np_component) # choos component which lambda >1 # kaiser\n",
    "\n",
    "# np_component = len(pca.explained_variance_[pca.explained_variance_>1])\n",
    "# print(pca.explained_variance_,'\\n', \"Nb components: \", np_component) # choos component which lambda >1 # kaiser\n",
    "# comp = ['comp' + str(i) for i in range(1,np_component+1)]\n",
    "# X_train_interim[comp],  X_test_interim[comp]= component_train[:, :np_component], component_test[:, :np_component]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.95914066e+00 5.20347330e+00 1.05754225e+00 9.73443909e-01\n",
      " 6.01841369e-01 3.89076541e-01 1.95285565e-01 1.42521636e-01\n",
      " 1.18388892e-01 9.96185263e-02 9.54720058e-02 7.55025638e-02\n",
      " 3.73317439e-02 2.33308417e-02 1.38792003e-02 9.96708843e-03\n",
      " 4.46146340e-03] \n",
      " Nb components:  3\n",
      "(51538, 40, 3) (51538, 1) (248, 40, 3)\n",
      "Epoch 1/20\n",
      "1611/1611 [==============================] - 11s 6ms/step - loss: 533.9869 - val_loss: 1010.5797\n",
      "Epoch 2/20\n",
      "1611/1611 [==============================] - 9s 6ms/step - loss: 276.1711 - val_loss: 1322.0992\n",
      "Epoch 3/20\n",
      "1611/1611 [==============================] - 10s 6ms/step - loss: 257.5493 - val_loss: 1459.4508\n",
      "Epoch 4/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 244.7569 - val_loss: 1437.2783\n",
      "Epoch 5/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 235.2770 - val_loss: 1344.1259\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "[7.95914066e+00 5.20347330e+00 1.05754225e+00 9.73443909e-01\n",
      " 6.01841369e-01 3.89076541e-01 1.95285565e-01 1.42521636e-01\n",
      " 1.18388892e-01 9.96185263e-02 9.54720058e-02 7.55025638e-02\n",
      " 3.73317439e-02 2.33308417e-02 1.38792003e-02 9.96708843e-03\n",
      " 4.46146340e-03] \n",
      " Nb components:  3\n",
      "(51538, 40, 3) (51538, 1) (248, 40, 3)\n",
      "Epoch 1/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 540.0656 - val_loss: 1142.9830\n",
      "Epoch 2/20\n",
      "1611/1611 [==============================] - 10s 6ms/step - loss: 319.8537 - val_loss: 992.3078\n",
      "Epoch 3/20\n",
      "1611/1611 [==============================] - 10s 6ms/step - loss: 269.5484 - val_loss: 1074.6735\n",
      "Epoch 4/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 253.1690 - val_loss: 1017.1703\n",
      "Epoch 5/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 241.2001 - val_loss: 1342.2903\n",
      "Epoch 6/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 231.2648 - val_loss: 956.0766\n",
      "Epoch 7/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 222.8155 - val_loss: 1083.8888\n",
      "Epoch 8/20\n",
      "1611/1611 [==============================] - 12s 8ms/step - loss: 216.6421 - val_loss: 975.4564\n",
      "Epoch 9/20\n",
      "1611/1611 [==============================] - 12s 8ms/step - loss: 210.5249 - val_loss: 894.6313\n",
      "Epoch 10/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 204.0304 - val_loss: 923.1789\n",
      "Epoch 11/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 200.3452 - val_loss: 829.8500\n",
      "Epoch 12/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 194.3625 - val_loss: 815.6085\n",
      "Epoch 13/20\n",
      "1611/1611 [==============================] - 12s 8ms/step - loss: 190.2484 - val_loss: 779.3884\n",
      "Epoch 14/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 186.3154 - val_loss: 851.0284\n",
      "Epoch 15/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 178.5253 - val_loss: 850.0399\n",
      "Epoch 16/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 173.9014 - val_loss: 792.7787\n",
      "Epoch 17/20\n",
      "1611/1611 [==============================] - 12s 8ms/step - loss: 169.6152 - val_loss: 828.3042\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "[7.95914066e+00 5.20347330e+00 1.05754225e+00 9.73443909e-01\n",
      " 6.01841369e-01 3.89076541e-01 1.95285565e-01 1.42521636e-01\n",
      " 1.18388892e-01 9.96185263e-02 9.54720058e-02 7.55025638e-02\n",
      " 3.73317439e-02 2.33308417e-02 1.38792003e-02 9.96708843e-03\n",
      " 4.46146340e-03] \n",
      " Nb components:  3\n",
      "(51538, 40, 3) (51538, 1) (248, 40, 3)\n",
      "Epoch 1/20\n",
      "1611/1611 [==============================] - 16s 9ms/step - loss: 523.8400 - val_loss: 1183.8049\n",
      "Epoch 2/20\n",
      "1611/1611 [==============================] - 12s 8ms/step - loss: 280.9608 - val_loss: 1116.5779\n",
      "Epoch 3/20\n",
      "1611/1611 [==============================] - 14s 9ms/step - loss: 261.3355 - val_loss: 1161.2433\n",
      "Epoch 4/20\n",
      "1611/1611 [==============================] - 12s 8ms/step - loss: 250.8551 - val_loss: 1273.4060\n",
      "Epoch 5/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 239.3402 - val_loss: 1180.7195\n",
      "Epoch 6/20\n",
      "1611/1611 [==============================] - 12s 8ms/step - loss: 232.1572 - val_loss: 1080.8546\n",
      "Epoch 7/20\n",
      "1611/1611 [==============================] - 12s 8ms/step - loss: 224.0142 - val_loss: 982.8139\n",
      "Epoch 8/20\n",
      "1611/1611 [==============================] - 12s 8ms/step - loss: 217.0227 - val_loss: 1035.9028\n",
      "Epoch 9/20\n",
      "1611/1611 [==============================] - 13s 8ms/step - loss: 211.6302 - val_loss: 990.0309\n",
      "Epoch 10/20\n",
      "1611/1611 [==============================] - 13s 8ms/step - loss: 205.5007 - val_loss: 1096.3898\n",
      "Epoch 11/20\n",
      "1611/1611 [==============================] - 13s 8ms/step - loss: 201.1881 - val_loss: 1052.8030\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "[7.95914066e+00 5.20347330e+00 1.05754225e+00 9.73443909e-01\n",
      " 6.01841369e-01 3.89076541e-01 1.95285565e-01 1.42521636e-01\n",
      " 1.18388892e-01 9.96185263e-02 9.54720058e-02 7.55025638e-02\n",
      " 3.73317439e-02 2.33308417e-02 1.38792003e-02 9.96708843e-03\n",
      " 4.46146340e-03] \n",
      " Nb components:  3\n",
      "(51538, 40, 3) (51538, 1) (248, 40, 3)\n",
      "Epoch 1/20\n",
      "1611/1611 [==============================] - 15s 9ms/step - loss: 529.6725 - val_loss: 1168.9950\n",
      "Epoch 2/20\n",
      "1611/1611 [==============================] - 15s 9ms/step - loss: 279.1215 - val_loss: 1187.8878\n",
      "Epoch 3/20\n",
      "1611/1611 [==============================] - 14s 9ms/step - loss: 259.0399 - val_loss: 1127.0068\n",
      "Epoch 4/20\n",
      "1611/1611 [==============================] - 14s 9ms/step - loss: 244.7723 - val_loss: 1266.9390\n",
      "Epoch 5/20\n",
      "1611/1611 [==============================] - 14s 9ms/step - loss: 236.3472 - val_loss: 1336.3383\n",
      "Epoch 6/20\n",
      "1611/1611 [==============================] - 12s 8ms/step - loss: 226.8269 - val_loss: 1436.1976\n",
      "Epoch 7/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 220.8361 - val_loss: 1427.0759\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "[7.95914066e+00 5.20347330e+00 1.05754225e+00 9.73443909e-01\n",
      " 6.01841369e-01 3.89076541e-01 1.95285565e-01 1.42521636e-01\n",
      " 1.18388892e-01 9.96185263e-02 9.54720058e-02 7.55025638e-02\n",
      " 3.73317439e-02 2.33308417e-02 1.38792003e-02 9.96708843e-03\n",
      " 4.46146340e-03] \n",
      " Nb components:  3\n",
      "(51538, 40, 3) (51538, 1) (248, 40, 3)\n",
      "Epoch 1/20\n",
      "1611/1611 [==============================] - 14s 8ms/step - loss: 543.0405 - val_loss: 1164.8127\n",
      "Epoch 2/20\n",
      "1611/1611 [==============================] - 12s 8ms/step - loss: 280.1194 - val_loss: 1320.3347\n",
      "Epoch 3/20\n",
      "1611/1611 [==============================] - 13s 8ms/step - loss: 260.5828 - val_loss: 1162.2905\n",
      "Epoch 4/20\n",
      "1611/1611 [==============================] - 13s 8ms/step - loss: 246.2920 - val_loss: 1186.5045\n",
      "Epoch 5/20\n",
      "1611/1611 [==============================] - 13s 8ms/step - loss: 236.3358 - val_loss: 1159.2830\n",
      "Epoch 6/20\n",
      "1611/1611 [==============================] - 13s 8ms/step - loss: 226.9168 - val_loss: 1141.4801\n",
      "Epoch 7/20\n",
      "1611/1611 [==============================] - 14s 8ms/step - loss: 218.8549 - val_loss: 1230.1729\n",
      "Epoch 8/20\n",
      "1611/1611 [==============================] - 15s 9ms/step - loss: 212.3058 - val_loss: 1128.7368\n",
      "Epoch 9/20\n",
      "1611/1611 [==============================] - 17s 10ms/step - loss: 208.2229 - val_loss: 1019.4821\n",
      "Epoch 10/20\n",
      "1611/1611 [==============================] - 16s 10ms/step - loss: 201.7892 - val_loss: 1084.1771\n",
      "Epoch 11/20\n",
      "1611/1611 [==============================] - 15s 9ms/step - loss: 197.4538 - val_loss: 942.3898\n",
      "Epoch 12/20\n",
      "1611/1611 [==============================] - 13s 8ms/step - loss: 190.3473 - val_loss: 901.9683\n",
      "Epoch 13/20\n",
      "1611/1611 [==============================] - 13s 8ms/step - loss: 185.3679 - val_loss: 920.4504\n",
      "Epoch 14/20\n",
      "1611/1611 [==============================] - 13s 8ms/step - loss: 179.9397 - val_loss: 903.8405\n",
      "Epoch 15/20\n",
      "1611/1611 [==============================] - 13s 8ms/step - loss: 172.6891 - val_loss: 892.0800\n",
      "Epoch 16/20\n",
      "1611/1611 [==============================] - 13s 8ms/step - loss: 166.9686 - val_loss: 979.4053\n",
      "Epoch 17/20\n",
      "1611/1611 [==============================] - 13s 8ms/step - loss: 163.2495 - val_loss: 941.4232\n",
      "Epoch 18/20\n",
      "1611/1611 [==============================] - 14s 9ms/step - loss: 160.4714 - val_loss: 923.2844\n",
      "Epoch 19/20\n",
      "1611/1611 [==============================] - 15s 9ms/step - loss: 155.7882 - val_loss: 905.1851\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "CPU times: total: 33min 1s\n",
      "Wall time: 12min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results_pca = pd.DataFrame()\n",
    "for SEED in range(5):  \n",
    "    tf.random.set_seed(SEED)\n",
    "    mse = []\n",
    "    R2_val = []\n",
    "    RMSE = []\n",
    "    score_val = []\n",
    "    \n",
    "    # 0.20\t[64]\t0.3\ttanh\t32\t25\n",
    "    \n",
    "    # parameter's sample\n",
    "    # weights_file = \"weights_file_lstm_optimalmodel_all.h5\"\n",
    "    alpha = 0.3\n",
    "    sequence_length = 40\n",
    "    epochs = 20\n",
    "    nodes_per_layer = [64]\n",
    "    dropout = 0.2\n",
    "    activation = 'tanh'\n",
    "    batch_size = 32\n",
    "    remaining_sensors = remaining_sensors\n",
    "    # create model\n",
    "    input_shape = (sequence_length, len(remaining_sensors))\n",
    "    model = model_lstm_1layer(input_shape, nodes_per_layer[0], dropout, activation)\n",
    "    \n",
    "    # Data prepration\n",
    "    X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "    \n",
    "    #PCA data reduction \n",
    "    X_cr_train, X_cr_test= StandardScaler().fit_transform(X_train_interim[remaining_sensors]), StandardScaler().fit_transform(X_test_interim[remaining_sensors])\n",
    "    pca = PCA()\n",
    "    component_train , component_test = pca.fit(X_cr_train).transform(X_cr_train), pca.transform(X_cr_test)\n",
    "    # print(pca.explained_variance_, np_component) # choos component which lambda >1 # kaiser\n",
    "\n",
    "    np_component = len(pca.explained_variance_[pca.explained_variance_>1])\n",
    "    print(pca.explained_variance_,'\\n', \"Nb components: \", np_component) # choos component which lambda >1 # kaiser\n",
    "    comp = ['comp' + str(i) for i in range(1,np_component+1)]\n",
    "    X_train_interim[comp],  X_test_interim[comp]= component_train[:, :np_component], component_test[:, :np_component]\n",
    "    \n",
    "\n",
    "    # create sequences train, test\n",
    "    train_array = gen_data_wrapper(X_train_interim, sequence_length, comp)\n",
    "    label_array = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'])\n",
    "\n",
    "    test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,comp, -99.))\n",
    "               for unit_nr in X_test_interim['Unit'].unique())\n",
    "    \n",
    "    test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "    test_rul = rul_piecewise_fct(y_test,rul_piecewise)\n",
    "    \n",
    "    input_shape = (sequence_length, len(comp))\n",
    "    model = model_lstm_1layer(input_shape, nodes_per_layer[0], dropout, activation)\n",
    "    print(train_array.shape, label_array.shape, test_array.shape)\n",
    "            \n",
    "    # Model fitting\n",
    "    cb = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        start_time = time.time()\n",
    "        model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n",
    "        weights_file = model.get_weights()\n",
    "        model.set_weights(weights_file)  # reset optimizer and node weights before every training iteration\n",
    "        history = model.fit(train_array, label_array,\n",
    "                                validation_data=(test_array, test_rul),\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                callbacks=[cb],\n",
    "                                verbose=1)\n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "        mse.append(history.history['val_loss'][-1])\n",
    "\n",
    "        y_hat_val_split = model.predict(test_array)\n",
    "        R2_val.append(r2_score(test_rul, y_hat_val_split))\n",
    "        RMSE.append(np.sqrt(mean_squared_error(test_rul, y_hat_val_split)))\n",
    "        score_val.append(compute_s_score(test_rul, y_hat_val_split))\n",
    "            \n",
    "        \n",
    "    #  append results\n",
    "    d = {'RMSE' :np.mean(RMSE), 'std_RMSE' :np.std(RMSE),\n",
    "         'S_score' :np.mean(score_val), 'std_S_score' :np.std(score_val),\n",
    "         'MSE':np.mean(mse), 'std_MSE':np.std(mse),\n",
    "         'nodes':str(nodes_per_layer), 'dropout':dropout, \n",
    "         'activation':activation, 'batch_size':batch_size, 'TW' : sequence_length,\n",
    "         'time': training_time}\n",
    "\n",
    "#     results = results.append(pd.DataFrame(d, index=[0]), ignore_index=True)\n",
    "    results_pca = pd.concat([results_pca, pd.DataFrame(d, index=[0])], ignore_index=True)\n",
    "    results_pca.to_csv('../results/pca/fd004.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>std_RMSE</th>\n",
       "      <th>S_score</th>\n",
       "      <th>std_S_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>TW</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31.789616</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11464.907069</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1344.125854</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>52.219702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27.917526</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7731.710038</td>\n",
       "      <td>0.0</td>\n",
       "      <td>828.304199</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>193.037298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31.349861</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11264.924037</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1052.802979</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>142.424080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33.570922</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21575.705711</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1427.075928</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>96.986894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29.867708</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9209.641300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>905.185059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>260.879144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RMSE  std_RMSE       S_score  std_S_score          MSE  std_MSE nodes  \\\n",
       "0  31.789616       0.0  11464.907069          0.0  1344.125854      0.0  [64]   \n",
       "1  27.917526       0.0   7731.710038          0.0   828.304199      0.0  [64]   \n",
       "2  31.349861       0.0  11264.924037          0.0  1052.802979      0.0  [64]   \n",
       "3  33.570922       0.0  21575.705711          0.0  1427.075928      0.0  [64]   \n",
       "4  29.867708       0.0   9209.641300          0.0   905.185059      0.0  [64]   \n",
       "\n",
       "   dropout activation  batch_size  TW        time  \n",
       "0      0.2       tanh          32  40   52.219702  \n",
       "1      0.2       tanh          32  40  193.037298  \n",
       "2      0.2       tanh          32  40  142.424080  \n",
       "3      0.2       tanh          32  40   96.986894  \n",
       "4      0.2       tanh          32  40  260.879144  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FD002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53759, 27) (33991, 26) (259, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# Load data and preprocess\n",
    "train, test, y_test = prepare_data('FD002.txt')\n",
    "print(train.shape, test.shape, y_test.shape)\n",
    "sensor_names = ['T20','T24','T30','T50','P20','P15','P30','Nf','Nc','epr','Ps30','phi',\n",
    "                    'NRf','NRc','BPR','farB','htBleed','Nf_dmd','PCNfR_dmd','W31','W32']\n",
    "\n",
    "remaining_sensors = ['T24','T30','T50', 'P15', 'P30','Nf','Nc', 'epr','Ps30','phi',\n",
    "                    'NRf','NRc','BPR','htBleed','W31','W32']\n",
    "drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "\n",
    "rul_piecewise = 125\n",
    "train['RUL'].clip(upper=rul_piecewise, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.04271508e+01 2.66149542e+00 1.05555422e+00 4.67459514e-01\n",
      " 2.81050559e-01 2.11386654e-01 1.92544371e-01 1.64287263e-01\n",
      " 1.32272442e-01 1.16578816e-01 1.04679947e-01 7.46965656e-02\n",
      " 4.85539287e-02 3.45854321e-02 1.97697647e-02 8.23190831e-03] \n",
      " Nb components:  3\n",
      "(43619, 40, 3) (43619, 1) (259, 40, 3)\n",
      "Epoch 1/20\n",
      "341/341 [==============================] - 5s 12ms/step - loss: 1384.3842 - val_loss: 876.1518\n",
      "Epoch 2/20\n",
      "341/341 [==============================] - 4s 11ms/step - loss: 433.4120 - val_loss: 823.0498\n",
      "Epoch 3/20\n",
      "341/341 [==============================] - 4s 11ms/step - loss: 424.6640 - val_loss: 789.3731\n",
      "Epoch 4/20\n",
      "341/341 [==============================] - 4s 12ms/step - loss: 387.0952 - val_loss: 873.8283\n",
      "Epoch 5/20\n",
      "341/341 [==============================] - 4s 11ms/step - loss: 300.4011 - val_loss: 745.8162\n",
      "Epoch 6/20\n",
      "341/341 [==============================] - 4s 12ms/step - loss: 258.9048 - val_loss: 634.3585\n",
      "Epoch 7/20\n",
      "341/341 [==============================] - 4s 12ms/step - loss: 247.3683 - val_loss: 561.0745\n",
      "Epoch 8/20\n",
      "341/341 [==============================] - 4s 11ms/step - loss: 238.7275 - val_loss: 556.8059\n",
      "Epoch 9/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 238.9057 - val_loss: 585.7537\n",
      "Epoch 10/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 237.1591 - val_loss: 577.5851\n",
      "Epoch 11/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 232.6417 - val_loss: 597.3887\n",
      "Epoch 12/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 230.3390 - val_loss: 548.5800\n",
      "Epoch 13/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 229.6414 - val_loss: 573.8098\n",
      "Epoch 14/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 225.6916 - val_loss: 580.0823\n",
      "Epoch 15/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 225.4310 - val_loss: 547.9427\n",
      "Epoch 16/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 222.3907 - val_loss: 588.7170\n",
      "Epoch 17/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 220.9351 - val_loss: 561.1433\n",
      "Epoch 18/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 219.8834 - val_loss: 586.1384\n",
      "Epoch 19/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 215.9854 - val_loss: 574.3770\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "[1.04271508e+01 2.66149542e+00 1.05555422e+00 4.67459514e-01\n",
      " 2.81050559e-01 2.11386654e-01 1.92544371e-01 1.64287263e-01\n",
      " 1.32272442e-01 1.16578816e-01 1.04679947e-01 7.46965656e-02\n",
      " 4.85539287e-02 3.45854321e-02 1.97697647e-02 8.23190831e-03] \n",
      " Nb components:  3\n",
      "(43619, 40, 3) (43619, 1) (259, 40, 3)\n",
      "Epoch 1/20\n",
      "341/341 [==============================] - 5s 12ms/step - loss: 1460.1169 - val_loss: 798.9950\n",
      "Epoch 2/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 440.6393 - val_loss: 794.3800\n",
      "Epoch 3/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 431.9056 - val_loss: 762.0864\n",
      "Epoch 4/20\n",
      "341/341 [==============================] - 4s 10ms/step - loss: 428.8854 - val_loss: 862.8931\n",
      "Epoch 5/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 426.1221 - val_loss: 808.8055\n",
      "Epoch 6/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 397.8127 - val_loss: 760.9892\n",
      "Epoch 7/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 280.5617 - val_loss: 836.6212\n",
      "Epoch 8/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 251.4516 - val_loss: 800.7253\n",
      "Epoch 9/20\n",
      "341/341 [==============================] - 4s 10ms/step - loss: 245.5134 - val_loss: 725.5422\n",
      "Epoch 10/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 241.9852 - val_loss: 654.1192\n",
      "Epoch 11/20\n",
      "341/341 [==============================] - 4s 11ms/step - loss: 238.1407 - val_loss: 703.3031\n",
      "Epoch 12/20\n",
      "341/341 [==============================] - 4s 11ms/step - loss: 236.0569 - val_loss: 669.9686\n",
      "Epoch 13/20\n",
      "341/341 [==============================] - 4s 10ms/step - loss: 233.8594 - val_loss: 694.7496\n",
      "Epoch 14/20\n",
      "341/341 [==============================] - 4s 11ms/step - loss: 229.5765 - val_loss: 654.9034\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "[1.04271508e+01 2.66149542e+00 1.05555422e+00 4.67459514e-01\n",
      " 2.81050559e-01 2.11386654e-01 1.92544371e-01 1.64287263e-01\n",
      " 1.32272442e-01 1.16578816e-01 1.04679947e-01 7.46965656e-02\n",
      " 4.85539287e-02 3.45854321e-02 1.97697647e-02 8.23190831e-03] \n",
      " Nb components:  3\n",
      "(43619, 40, 3) (43619, 1) (259, 40, 3)\n",
      "Epoch 1/20\n",
      "341/341 [==============================] - 6s 13ms/step - loss: 1375.9553 - val_loss: 866.4975\n",
      "Epoch 2/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 389.0464 - val_loss: 758.0341\n",
      "Epoch 3/20\n",
      "341/341 [==============================] - 4s 12ms/step - loss: 351.4838 - val_loss: 766.9044\n",
      "Epoch 4/20\n",
      "341/341 [==============================] - 4s 11ms/step - loss: 300.6168 - val_loss: 759.7846\n",
      "Epoch 5/20\n",
      "341/341 [==============================] - 4s 12ms/step - loss: 272.3353 - val_loss: 766.3838\n",
      "Epoch 6/20\n",
      "341/341 [==============================] - 4s 11ms/step - loss: 252.0682 - val_loss: 736.9410\n",
      "Epoch 7/20\n",
      "341/341 [==============================] - 4s 12ms/step - loss: 248.1113 - val_loss: 678.9899\n",
      "Epoch 8/20\n",
      "341/341 [==============================] - 4s 10ms/step - loss: 241.6031 - val_loss: 674.0952\n",
      "Epoch 9/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 240.2036 - val_loss: 720.0824\n",
      "Epoch 10/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 238.8313 - val_loss: 712.1783\n",
      "Epoch 11/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 235.3637 - val_loss: 785.8952\n",
      "Epoch 12/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 234.1628 - val_loss: 726.4702\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "[1.04271508e+01 2.66149542e+00 1.05555422e+00 4.67459514e-01\n",
      " 2.81050559e-01 2.11386654e-01 1.92544371e-01 1.64287263e-01\n",
      " 1.32272442e-01 1.16578816e-01 1.04679947e-01 7.46965656e-02\n",
      " 4.85539287e-02 3.45854321e-02 1.97697647e-02 8.23190831e-03] \n",
      " Nb components:  3\n",
      "(43619, 40, 3) (43619, 1) (259, 40, 3)\n",
      "Epoch 1/20\n",
      "341/341 [==============================] - 5s 11ms/step - loss: 1556.3500 - val_loss: 936.6337\n",
      "Epoch 2/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 411.2877 - val_loss: 842.2570\n",
      "Epoch 3/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 402.1641 - val_loss: 836.5876\n",
      "Epoch 4/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 411.2144 - val_loss: 843.1787\n",
      "Epoch 5/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 387.9792 - val_loss: 796.7607\n",
      "Epoch 6/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 387.8377 - val_loss: 891.3377\n",
      "Epoch 7/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 375.1231 - val_loss: 700.4009\n",
      "Epoch 8/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 336.0469 - val_loss: 773.0343\n",
      "Epoch 9/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 313.1384 - val_loss: 878.8029\n",
      "Epoch 10/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 308.4041 - val_loss: 935.5980\n",
      "Epoch 11/20\n",
      "341/341 [==============================] - 4s 10ms/step - loss: 291.6913 - val_loss: 840.7689\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "[1.04271508e+01 2.66149542e+00 1.05555422e+00 4.67459514e-01\n",
      " 2.81050559e-01 2.11386654e-01 1.92544371e-01 1.64287263e-01\n",
      " 1.32272442e-01 1.16578816e-01 1.04679947e-01 7.46965656e-02\n",
      " 4.85539287e-02 3.45854321e-02 1.97697647e-02 8.23190831e-03] \n",
      " Nb components:  3\n",
      "(43619, 40, 3) (43619, 1) (259, 40, 3)\n",
      "Epoch 1/20\n",
      "341/341 [==============================] - 5s 11ms/step - loss: 1417.3318 - val_loss: 816.7598\n",
      "Epoch 2/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 415.0840 - val_loss: 855.0056\n",
      "Epoch 3/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 425.3702 - val_loss: 747.7908\n",
      "Epoch 4/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 353.0941 - val_loss: 836.5168\n",
      "Epoch 5/20\n",
      "341/341 [==============================] - 4s 11ms/step - loss: 282.3171 - val_loss: 831.9901\n",
      "Epoch 6/20\n",
      "341/341 [==============================] - 4s 10ms/step - loss: 259.3670 - val_loss: 836.2079\n",
      "Epoch 7/20\n",
      "341/341 [==============================] - 4s 11ms/step - loss: 250.4256 - val_loss: 872.1711\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "CPU times: total: 10min 20s\n",
      "Wall time: 3min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results_pca002 = pd.DataFrame()\n",
    "for SEED in range(5):  \n",
    "    tf.random.set_seed(SEED)\n",
    "    mse = []\n",
    "    R2_val = []\n",
    "    RMSE = []\n",
    "    score_val = []\n",
    "    \n",
    "    # 0.20\t[64]\t0.3\ttanh\t32\t25\n",
    "    \n",
    "    # parameter's sample\n",
    "    # weights_file = \"weights_file_lstm_optimalmodel_all.h5\"\n",
    "    alpha = 0.2\n",
    "    sequence_length = 40\n",
    "    epochs = 20\n",
    "    nodes_per_layer = [32]\n",
    "    dropout = 0.1\n",
    "    activation = 'tanh'\n",
    "    batch_size = 128\n",
    "    remaining_sensors = remaining_sensors\n",
    "    # create model\n",
    "    input_shape = (sequence_length, len(remaining_sensors))\n",
    "    model = model_lstm_1layer(input_shape, nodes_per_layer[0], dropout, activation)\n",
    "    \n",
    "    # Data prepration\n",
    "    X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "    \n",
    "    #PCA data reduction \n",
    "    X_cr_train, X_cr_test= StandardScaler().fit_transform(X_train_interim[remaining_sensors]), StandardScaler().fit_transform(X_test_interim[remaining_sensors])\n",
    "    pca = PCA()\n",
    "    component_train , component_test = pca.fit(X_cr_train).transform(X_cr_train), pca.transform(X_cr_test)\n",
    "    # print(pca.explained_variance_, np_component) # choos component which lambda >1 # kaiser\n",
    "\n",
    "    np_component = len(pca.explained_variance_[pca.explained_variance_>1])\n",
    "    print(pca.explained_variance_,'\\n', \"Nb components: \", np_component) # choos component which lambda >1 # kaiser\n",
    "    comp = ['comp' + str(i) for i in range(1,np_component+1)]\n",
    "    X_train_interim[comp],  X_test_interim[comp]= component_train[:, :np_component], component_test[:, :np_component]\n",
    "    \n",
    "\n",
    "    # create sequences train, test\n",
    "    train_array = gen_data_wrapper(X_train_interim, sequence_length, comp)\n",
    "    label_array = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'])\n",
    "\n",
    "    test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,comp, -99.))\n",
    "               for unit_nr in X_test_interim['Unit'].unique())\n",
    "    \n",
    "    test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "    test_rul = rul_piecewise_fct(y_test,rul_piecewise)\n",
    "    \n",
    "    input_shape = (sequence_length, len(comp))\n",
    "    model = model_lstm_1layer(input_shape, nodes_per_layer[0], dropout, activation)\n",
    "    print(train_array.shape, label_array.shape, test_array.shape)\n",
    "            \n",
    "    # Model fitting\n",
    "    cb = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        start_time = time.time()\n",
    "        model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n",
    "        weights_file = model.get_weights()\n",
    "        model.set_weights(weights_file)  # reset optimizer and node weights before every training iteration\n",
    "        history = model.fit(train_array, label_array,\n",
    "                                validation_data=(test_array, test_rul),\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                callbacks=[cb],\n",
    "                                verbose=1)\n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "        mse.append(history.history['val_loss'][-1])\n",
    "\n",
    "        y_hat_val_split = model.predict(test_array)\n",
    "        R2_val.append(r2_score(test_rul, y_hat_val_split))\n",
    "        RMSE.append(np.sqrt(mean_squared_error(test_rul, y_hat_val_split)))\n",
    "        score_val.append(compute_s_score(test_rul, y_hat_val_split))\n",
    "            \n",
    "        \n",
    "    #  append results\n",
    "    d = {'RMSE' :np.mean(RMSE), 'std_RMSE' :np.std(RMSE),\n",
    "         'S_score' :np.mean(score_val), 'std_S_score' :np.std(score_val),\n",
    "         'MSE':np.mean(mse), 'std_MSE':np.std(mse),\n",
    "         'nodes':str(nodes_per_layer), 'dropout':dropout, \n",
    "         'activation':activation, 'batch_size':batch_size, 'TW' : sequence_length,\n",
    "         'time': training_time}\n",
    "\n",
    "#     results = results.append(pd.DataFrame(d, index=[0]), ignore_index=True)\n",
    "    results_pca002 = pd.concat([results_pca002, pd.DataFrame(d, index=[0])], ignore_index=True)\n",
    "    results_pca002.to_csv('../results/pca/fd002.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>std_RMSE</th>\n",
       "      <th>S_score</th>\n",
       "      <th>std_S_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>TW</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23.408177</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2811.583041</td>\n",
       "      <td>0.0</td>\n",
       "      <td>574.377014</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "      <td>40</td>\n",
       "      <td>67.750551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25.575755</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5889.030208</td>\n",
       "      <td>0.0</td>\n",
       "      <td>654.903381</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "      <td>40</td>\n",
       "      <td>50.043765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25.963342</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8704.382250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>726.470154</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "      <td>40</td>\n",
       "      <td>44.908500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26.465088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5526.936039</td>\n",
       "      <td>0.0</td>\n",
       "      <td>840.768921</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "      <td>40</td>\n",
       "      <td>36.857980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27.345766</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3920.622468</td>\n",
       "      <td>0.0</td>\n",
       "      <td>872.171082</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "      <td>40</td>\n",
       "      <td>25.823909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RMSE  std_RMSE      S_score  std_S_score         MSE  std_MSE nodes  \\\n",
       "0  23.408177       0.0  2811.583041          0.0  574.377014      0.0  [32]   \n",
       "1  25.575755       0.0  5889.030208          0.0  654.903381      0.0  [32]   \n",
       "2  25.963342       0.0  8704.382250          0.0  726.470154      0.0  [32]   \n",
       "3  26.465088       0.0  5526.936039          0.0  840.768921      0.0  [32]   \n",
       "4  27.345766       0.0  3920.622468          0.0  872.171082      0.0  [32]   \n",
       "\n",
       "   dropout activation  batch_size  TW       time  \n",
       "0      0.1       tanh         128  40  67.750551  \n",
       "1      0.1       tanh         128  40  50.043765  \n",
       "2      0.1       tanh         128  40  44.908500  \n",
       "3      0.1       tanh         128  40  36.857980  \n",
       "4      0.1       tanh         128  40  25.823909  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_pca002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FD003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24720, 27) (16596, 26) (100, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# Load data and preprocess\n",
    "train, test, y_test = prepare_data('FD003.txt')\n",
    "print(train.shape, test.shape, y_test.shape)\n",
    "sensor_names = ['T20','T24','T30','T50','P20','P15','P30','Nf','Nc','epr','Ps30','phi',\n",
    "                    'NRf','NRc','BPR','farB','htBleed','Nf_dmd','PCNfR_dmd','W31','W32']\n",
    "\n",
    "remaining_sensors = ['T24','T30','T50', 'P15', 'P30','Nf','Nc', 'epr','Ps30','phi',\n",
    "                    'NRf','NRc','BPR','htBleed','W31','W32']\n",
    "drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "\n",
    "rul_piecewise = 125\n",
    "train['RUL'].clip(upper=rul_piecewise, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.19095667e+00 5.49399656e+00 8.81347307e-01 7.81564822e-01\n",
      " 3.51238057e-01 1.43235420e-01 4.09913921e-02 3.53324081e-02\n",
      " 2.44339253e-02 1.75337353e-02 1.33853528e-02 1.23424994e-02\n",
      " 6.63828566e-03 3.52672765e-03 3.10180225e-03 1.02230366e-03] \n",
      " Nb components:  2\n",
      "(21320, 35, 2) (21320, 1) (100, 35, 2)\n",
      "Epoch 1/20\n",
      "667/667 [==============================] - 7s 9ms/step - loss: 836.8309 - val_loss: 944.6252\n",
      "Epoch 2/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 289.8248 - val_loss: 1114.4414\n",
      "Epoch 3/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 212.7500 - val_loss: 959.8607\n",
      "Epoch 4/20\n",
      "667/667 [==============================] - 6s 9ms/step - loss: 192.4150 - val_loss: 878.5651\n",
      "Epoch 5/20\n",
      "667/667 [==============================] - 6s 9ms/step - loss: 183.3982 - val_loss: 832.0654\n",
      "Epoch 6/20\n",
      "667/667 [==============================] - 6s 9ms/step - loss: 180.6049 - val_loss: 874.3455\n",
      "Epoch 7/20\n",
      "667/667 [==============================] - 6s 9ms/step - loss: 173.5380 - val_loss: 951.0679\n",
      "Epoch 8/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 170.6400 - val_loss: 718.4102\n",
      "Epoch 9/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 165.5586 - val_loss: 835.9971\n",
      "Epoch 10/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 159.6880 - val_loss: 764.1189\n",
      "Epoch 11/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 156.3004 - val_loss: 875.2067\n",
      "Epoch 12/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 155.2384 - val_loss: 806.8122\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "[8.19095667e+00 5.49399656e+00 8.81347307e-01 7.81564822e-01\n",
      " 3.51238057e-01 1.43235420e-01 4.09913921e-02 3.53324081e-02\n",
      " 2.44339253e-02 1.75337353e-02 1.33853528e-02 1.23424994e-02\n",
      " 6.63828566e-03 3.52672765e-03 3.10180225e-03 1.02230366e-03] \n",
      " Nb components:  2\n",
      "(21320, 35, 2) (21320, 1) (100, 35, 2)\n",
      "Epoch 1/20\n",
      "667/667 [==============================] - 6s 8ms/step - loss: 871.3484 - val_loss: 1264.0946\n",
      "Epoch 2/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 325.4044 - val_loss: 1069.9731\n",
      "Epoch 3/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 248.1033 - val_loss: 1062.0975\n",
      "Epoch 4/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 204.5336 - val_loss: 1080.6292\n",
      "Epoch 5/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 190.4118 - val_loss: 1024.2748\n",
      "Epoch 6/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 184.2433 - val_loss: 859.3016\n",
      "Epoch 7/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 177.8422 - val_loss: 736.0818\n",
      "Epoch 8/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 167.4729 - val_loss: 931.7216\n",
      "Epoch 9/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 178.0766 - val_loss: 824.2667\n",
      "Epoch 10/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 160.6780 - val_loss: 892.9830\n",
      "Epoch 11/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 154.9885 - val_loss: 927.1850\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "[8.19095667e+00 5.49399656e+00 8.81347307e-01 7.81564822e-01\n",
      " 3.51238057e-01 1.43235420e-01 4.09913921e-02 3.53324081e-02\n",
      " 2.44339253e-02 1.75337353e-02 1.33853528e-02 1.23424994e-02\n",
      " 6.63828566e-03 3.52672765e-03 3.10180225e-03 1.02230366e-03] \n",
      " Nb components:  2\n",
      "(21320, 35, 2) (21320, 1) (100, 35, 2)\n",
      "Epoch 1/20\n",
      "667/667 [==============================] - 7s 9ms/step - loss: 870.2594 - val_loss: 990.8940\n",
      "Epoch 2/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 266.4536 - val_loss: 916.6393\n",
      "Epoch 3/20\n",
      "667/667 [==============================] - 6s 8ms/step - loss: 203.4665 - val_loss: 784.1457\n",
      "Epoch 4/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 193.0265 - val_loss: 890.9320\n",
      "Epoch 5/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 183.6679 - val_loss: 957.4260\n",
      "Epoch 6/20\n",
      "667/667 [==============================] - 6s 9ms/step - loss: 179.0565 - val_loss: 941.8049\n",
      "Epoch 7/20\n",
      "667/667 [==============================] - 6s 8ms/step - loss: 173.8165 - val_loss: 827.7164\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "[8.19095667e+00 5.49399656e+00 8.81347307e-01 7.81564822e-01\n",
      " 3.51238057e-01 1.43235420e-01 4.09913921e-02 3.53324081e-02\n",
      " 2.44339253e-02 1.75337353e-02 1.33853528e-02 1.23424994e-02\n",
      " 6.63828566e-03 3.52672765e-03 3.10180225e-03 1.02230366e-03] \n",
      " Nb components:  2\n",
      "(21320, 35, 2) (21320, 1) (100, 35, 2)\n",
      "Epoch 1/20\n",
      "667/667 [==============================] - 7s 9ms/step - loss: 878.6287 - val_loss: 1457.4213\n",
      "Epoch 2/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 315.2050 - val_loss: 1235.2123\n",
      "Epoch 3/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 234.3548 - val_loss: 1101.5878\n",
      "Epoch 4/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 197.6350 - val_loss: 1019.9833\n",
      "Epoch 5/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 186.0987 - val_loss: 978.9929\n",
      "Epoch 6/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 182.2670 - val_loss: 981.0732\n",
      "Epoch 7/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 177.7139 - val_loss: 919.9444\n",
      "Epoch 8/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 169.7271 - val_loss: 748.3656\n",
      "Epoch 9/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 167.1950 - val_loss: 883.6426\n",
      "Epoch 10/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 165.4212 - val_loss: 851.3831\n",
      "Epoch 11/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 161.6480 - val_loss: 923.8270\n",
      "Epoch 12/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 155.8247 - val_loss: 717.2908\n",
      "Epoch 13/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 152.5312 - val_loss: 832.1443\n",
      "Epoch 14/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 150.7034 - val_loss: 823.7430\n",
      "Epoch 15/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 145.2525 - val_loss: 797.0776\n",
      "Epoch 16/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 140.4224 - val_loss: 793.7237\n",
      "4/4 [==============================] - 1s 4ms/step\n",
      "[8.19095667e+00 5.49399656e+00 8.81347307e-01 7.81564822e-01\n",
      " 3.51238057e-01 1.43235420e-01 4.09913921e-02 3.53324081e-02\n",
      " 2.44339253e-02 1.75337353e-02 1.33853528e-02 1.23424994e-02\n",
      " 6.63828566e-03 3.52672765e-03 3.10180225e-03 1.02230366e-03] \n",
      " Nb components:  2\n",
      "(21320, 35, 2) (21320, 1) (100, 35, 2)\n",
      "Epoch 1/20\n",
      "667/667 [==============================] - 7s 9ms/step - loss: 866.8098 - val_loss: 1344.9794\n",
      "Epoch 2/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 256.4071 - val_loss: 879.0283\n",
      "Epoch 3/20\n",
      "667/667 [==============================] - 6s 9ms/step - loss: 208.5192 - val_loss: 885.9451\n",
      "Epoch 4/20\n",
      "667/667 [==============================] - 6s 9ms/step - loss: 192.7369 - val_loss: 911.5170\n",
      "Epoch 5/20\n",
      "667/667 [==============================] - 6s 8ms/step - loss: 178.5781 - val_loss: 810.0723\n",
      "Epoch 6/20\n",
      "667/667 [==============================] - 6s 9ms/step - loss: 178.3516 - val_loss: 965.7986\n",
      "Epoch 7/20\n",
      "667/667 [==============================] - 6s 8ms/step - loss: 172.6986 - val_loss: 651.5446\n",
      "Epoch 8/20\n",
      "667/667 [==============================] - 5s 8ms/step - loss: 167.7273 - val_loss: 807.7855\n",
      "Epoch 9/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 163.8087 - val_loss: 823.3336\n",
      "Epoch 10/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 156.1326 - val_loss: 867.9001\n",
      "Epoch 11/20\n",
      "667/667 [==============================] - 5s 7ms/step - loss: 154.0166 - val_loss: 796.0851\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "CPU times: total: 13min 26s\n",
      "Wall time: 5min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results_pca003 = pd.DataFrame()\n",
    "for SEED in range(5):  \n",
    "    tf.random.set_seed(SEED)\n",
    "    mse = []\n",
    "    R2_val = []\n",
    "    RMSE = []\n",
    "    score_val = []\n",
    "    \n",
    "    # 0.20\t[64]\t0.3\ttanh\t32\t25\n",
    "    \n",
    "    # parameter's sample\n",
    "    # weights_file = \"weights_file_lstm_optimalmodel_all.h5\"\n",
    "    alpha = 0.1\n",
    "    sequence_length = 35\n",
    "    epochs = 20\n",
    "    nodes_per_layer = [64]\n",
    "    dropout = 0.2\n",
    "    activation = 'tanh'\n",
    "    batch_size = 32\n",
    "    remaining_sensors = remaining_sensors\n",
    "    \n",
    "    # Data prepration\n",
    "    X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "    \n",
    "    #PCA data reduction \n",
    "    X_cr_train, X_cr_test= StandardScaler().fit_transform(X_train_interim[remaining_sensors]), StandardScaler().fit_transform(X_test_interim[remaining_sensors])\n",
    "    pca = PCA()\n",
    "    component_train , component_test = pca.fit(X_cr_train).transform(X_cr_train), pca.transform(X_cr_test)\n",
    "    # print(pca.explained_variance_, np_component) # choos component which lambda >1 # kaiser\n",
    "\n",
    "    np_component = len(pca.explained_variance_[pca.explained_variance_>1])\n",
    "    print(pca.explained_variance_,'\\n', \"Nb components: \", np_component) # choos component which lambda >1 # kaiser\n",
    "    comp = ['comp' + str(i) for i in range(1,np_component+1)]\n",
    "    X_train_interim[comp],  X_test_interim[comp]= component_train[:, :np_component], component_test[:, :np_component]\n",
    "    \n",
    "\n",
    "    # create sequences train, test\n",
    "    train_array = gen_data_wrapper(X_train_interim, sequence_length, comp)\n",
    "    label_array = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'])\n",
    "\n",
    "    test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,comp, -99.))\n",
    "               for unit_nr in X_test_interim['Unit'].unique())\n",
    "    \n",
    "    test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "    test_rul = rul_piecewise_fct(y_test,rul_piecewise)\n",
    "    \n",
    "    input_shape = (sequence_length, len(comp))\n",
    "    model = model_lstm_1layer(input_shape, nodes_per_layer[0], dropout, activation)\n",
    "    print(train_array.shape, label_array.shape, test_array.shape)\n",
    "            \n",
    "    # Model fitting\n",
    "    cb = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        start_time = time.time()\n",
    "        model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n",
    "        weights_file = model.get_weights()\n",
    "        model.set_weights(weights_file)  # reset optimizer and node weights before every training iteration\n",
    "        history = model.fit(train_array, label_array,\n",
    "                                validation_data=(test_array, test_rul),\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                callbacks=[cb],\n",
    "                                verbose=1)\n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "        mse.append(history.history['val_loss'][-1])\n",
    "\n",
    "        y_hat_val_split = model.predict(test_array)\n",
    "        R2_val.append(r2_score(test_rul, y_hat_val_split))\n",
    "        RMSE.append(np.sqrt(mean_squared_error(test_rul, y_hat_val_split)))\n",
    "        score_val.append(compute_s_score(test_rul, y_hat_val_split))\n",
    "            \n",
    "        \n",
    "    #  append results\n",
    "    d = {'RMSE' :np.mean(RMSE), 'std_RMSE' :np.std(RMSE),\n",
    "         'S_score' :np.mean(score_val), 'std_S_score' :np.std(score_val),\n",
    "         'MSE':np.mean(mse), 'std_MSE':np.std(mse),\n",
    "         'nodes':str(nodes_per_layer), 'dropout':dropout, \n",
    "         'activation':activation, 'batch_size':batch_size, 'TW' : sequence_length,\n",
    "         'time': training_time}\n",
    "\n",
    "#     results = results.append(pd.DataFrame(d, index=[0]), ignore_index=True)\n",
    "    results_pca003 = pd.concat([results_pca003, pd.DataFrame(d, index=[0])], ignore_index=True)\n",
    "    results_pca003.to_csv('../results/pca/fd003.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>std_RMSE</th>\n",
       "      <th>S_score</th>\n",
       "      <th>std_S_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>TW</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26.803176</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1810.801647</td>\n",
       "      <td>0.0</td>\n",
       "      <td>806.812195</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "      <td>67.104996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27.130829</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1630.070580</td>\n",
       "      <td>0.0</td>\n",
       "      <td>927.184998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "      <td>58.705701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28.002601</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2009.826680</td>\n",
       "      <td>0.0</td>\n",
       "      <td>827.716431</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "      <td>40.354693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26.782287</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1225.253310</td>\n",
       "      <td>0.0</td>\n",
       "      <td>793.723694</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "      <td>83.087591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25.525371</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1314.506074</td>\n",
       "      <td>0.0</td>\n",
       "      <td>796.085144</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "      <td>60.990287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RMSE  std_RMSE      S_score  std_S_score         MSE  std_MSE nodes  \\\n",
       "0  26.803176       0.0  1810.801647          0.0  806.812195      0.0  [64]   \n",
       "1  27.130829       0.0  1630.070580          0.0  927.184998      0.0  [64]   \n",
       "2  28.002601       0.0  2009.826680          0.0  827.716431      0.0  [64]   \n",
       "3  26.782287       0.0  1225.253310          0.0  793.723694      0.0  [64]   \n",
       "4  25.525371       0.0  1314.506074          0.0  796.085144      0.0  [64]   \n",
       "\n",
       "   dropout activation  batch_size  TW       time  \n",
       "0      0.2       tanh          32  35  67.104996  \n",
       "1      0.2       tanh          32  35  58.705701  \n",
       "2      0.2       tanh          32  35  40.354693  \n",
       "3      0.2       tanh          32  35  83.087591  \n",
       "4      0.2       tanh          32  35  60.990287  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_pca003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FD001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20631, 27) (13096, 26) (100, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# Load data and preprocess\n",
    "train, test, y_test = prepare_data('FD001.txt')\n",
    "print(train.shape, test.shape, y_test.shape)\n",
    "sensor_names = ['T20','T24','T30','T50','P20','P15','P30','Nf','Nc','epr','Ps30','phi',\n",
    "                    'NRf','NRc','BPR','farB','htBleed','Nf_dmd','PCNfR_dmd','W31','W32']\n",
    "remaining_sensors = ['T24','T30','T50', 'P15', 'P30','Nf','Nc','Ps30','phi',\n",
    "                    'NRf','NRc','BPR','htBleed','W31','W32']\n",
    "drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "\n",
    "rul_piecewise = 125\n",
    "train['RUL'].clip(upper=rul_piecewise, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.16218316e+01 2.22745151e+00 7.53919107e-01 7.60689125e-02\n",
      " 5.80733626e-02 4.69453854e-02 4.15078334e-02 3.47383080e-02\n",
      " 3.32981565e-02 2.89969632e-02 2.53785317e-02 1.84836056e-02\n",
      " 1.69423744e-02 1.35008525e-02 3.59058504e-03] \n",
      " Nb components:  2\n",
      "(17731, 30, 2) (17731, 1) (100, 30, 2)\n",
      "Epoch 1/20\n",
      "278/278 [==============================] - 6s 18ms/step - loss: 1091.9460 - val_loss: 993.8204\n",
      "Epoch 2/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 224.6014 - val_loss: 880.8183\n",
      "Epoch 3/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 192.0079 - val_loss: 687.6577\n",
      "Epoch 4/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 187.4294 - val_loss: 767.1495\n",
      "Epoch 5/20\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 174.4967 - val_loss: 792.8438\n",
      "Epoch 6/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 170.6644 - val_loss: 663.6057\n",
      "Epoch 7/20\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 172.4901 - val_loss: 724.1043\n",
      "Epoch 8/20\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 162.9078 - val_loss: 804.3661\n",
      "Epoch 9/20\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 162.1567 - val_loss: 830.8133\n",
      "Epoch 10/20\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 160.6317 - val_loss: 853.1472\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "[1.16218316e+01 2.22745151e+00 7.53919107e-01 7.60689125e-02\n",
      " 5.80733626e-02 4.69453854e-02 4.15078334e-02 3.47383080e-02\n",
      " 3.32981565e-02 2.89969632e-02 2.53785317e-02 1.84836056e-02\n",
      " 1.69423744e-02 1.35008525e-02 3.59058504e-03] \n",
      " Nb components:  2\n",
      "(17731, 30, 2) (17731, 1) (100, 30, 2)\n",
      "Epoch 1/20\n",
      "278/278 [==============================] - 6s 18ms/step - loss: 1100.1071 - val_loss: 1153.1545\n",
      "Epoch 2/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 217.4684 - val_loss: 785.4532\n",
      "Epoch 3/20\n",
      "278/278 [==============================] - 5s 19ms/step - loss: 189.2479 - val_loss: 555.6754\n",
      "Epoch 4/20\n",
      "278/278 [==============================] - 5s 19ms/step - loss: 181.4359 - val_loss: 773.5963\n",
      "Epoch 5/20\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 174.4355 - val_loss: 630.0936\n",
      "Epoch 6/20\n",
      "278/278 [==============================] - 5s 19ms/step - loss: 168.9748 - val_loss: 728.0293\n",
      "Epoch 7/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 163.8810 - val_loss: 735.5632\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "[1.16218316e+01 2.22745151e+00 7.53919107e-01 7.60689125e-02\n",
      " 5.80733626e-02 4.69453854e-02 4.15078334e-02 3.47383080e-02\n",
      " 3.32981565e-02 2.89969632e-02 2.53785317e-02 1.84836056e-02\n",
      " 1.69423744e-02 1.35008525e-02 3.59058504e-03] \n",
      " Nb components:  2\n",
      "(17731, 30, 2) (17731, 1) (100, 30, 2)\n",
      "Epoch 1/20\n",
      "278/278 [==============================] - 6s 19ms/step - loss: 1170.8821 - val_loss: 1167.8439\n",
      "Epoch 2/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 444.6333 - val_loss: 1145.7039\n",
      "Epoch 3/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 472.4609 - val_loss: 955.2648\n",
      "Epoch 4/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 241.0593 - val_loss: 984.5386\n",
      "Epoch 5/20\n",
      "278/278 [==============================] - 5s 19ms/step - loss: 213.8667 - val_loss: 932.2952\n",
      "Epoch 6/20\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 203.0894 - val_loss: 852.5136\n",
      "Epoch 7/20\n",
      "278/278 [==============================] - 5s 16ms/step - loss: 189.8178 - val_loss: 599.8365\n",
      "Epoch 8/20\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 191.1189 - val_loss: 956.4289\n",
      "Epoch 9/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 180.8203 - val_loss: 798.9120\n",
      "Epoch 10/20\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 178.8482 - val_loss: 783.3610\n",
      "Epoch 11/20\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 178.8869 - val_loss: 642.9565\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "[1.16218316e+01 2.22745151e+00 7.53919107e-01 7.60689125e-02\n",
      " 5.80733626e-02 4.69453854e-02 4.15078334e-02 3.47383080e-02\n",
      " 3.32981565e-02 2.89969632e-02 2.53785317e-02 1.84836056e-02\n",
      " 1.69423744e-02 1.35008525e-02 3.59058504e-03] \n",
      " Nb components:  2\n",
      "(17731, 30, 2) (17731, 1) (100, 30, 2)\n",
      "Epoch 1/20\n",
      "278/278 [==============================] - 7s 20ms/step - loss: 1106.7518 - val_loss: 1104.9806\n",
      "Epoch 2/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 221.7606 - val_loss: 886.9553\n",
      "Epoch 3/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 194.1648 - val_loss: 805.6320\n",
      "Epoch 4/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 189.6561 - val_loss: 961.6819\n",
      "Epoch 5/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 178.6871 - val_loss: 793.7977\n",
      "Epoch 6/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 177.9328 - val_loss: 725.9621\n",
      "Epoch 7/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 170.4878 - val_loss: 747.6244\n",
      "Epoch 8/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 168.6939 - val_loss: 1006.4252\n",
      "Epoch 9/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 165.2650 - val_loss: 858.8205\n",
      "Epoch 10/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 161.2479 - val_loss: 746.6833\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "[1.16218316e+01 2.22745151e+00 7.53919107e-01 7.60689125e-02\n",
      " 5.80733626e-02 4.69453854e-02 4.15078334e-02 3.47383080e-02\n",
      " 3.32981565e-02 2.89969632e-02 2.53785317e-02 1.84836056e-02\n",
      " 1.69423744e-02 1.35008525e-02 3.59058504e-03] \n",
      " Nb components:  2\n",
      "(17731, 30, 2) (17731, 1) (100, 30, 2)\n",
      "Epoch 1/20\n",
      "278/278 [==============================] - 6s 16ms/step - loss: 1126.6289 - val_loss: 900.7766\n",
      "Epoch 2/20\n",
      "278/278 [==============================] - 4s 13ms/step - loss: 220.2926 - val_loss: 1120.5793\n",
      "Epoch 3/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 195.7609 - val_loss: 667.2241\n",
      "Epoch 4/20\n",
      "278/278 [==============================] - 4s 13ms/step - loss: 181.1488 - val_loss: 807.1113\n",
      "Epoch 5/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 177.6113 - val_loss: 883.7792\n",
      "Epoch 6/20\n",
      "278/278 [==============================] - 4s 14ms/step - loss: 174.7022 - val_loss: 888.8145\n",
      "Epoch 7/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 165.7505 - val_loss: 803.2195\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "CPU times: total: 8min 25s\n",
      "Wall time: 3min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results_pca001 = pd.DataFrame()\n",
    "for SEED in range(5):  \n",
    "    set_seed(SEED)\n",
    "    mse = []\n",
    "    R2_val = []\n",
    "    RMSE = []\n",
    "    score_val = []\n",
    "    \n",
    "    # 0.20\t[64]\t0.3\ttanh\t32\t25\n",
    "    \n",
    "    # parameter's sample\n",
    "    # weights_file = \"weights_file_lstm_optimalmodel_all.h5\"\n",
    "    alpha = 0.1\n",
    "    sequence_length = 30\n",
    "    epochs = 20\n",
    "    nodes_per_layer = [128]\n",
    "    dropout = 0.2\n",
    "    activation = 'tanh'\n",
    "    batch_size = 64\n",
    "    remaining_sensors = remaining_sensors    \n",
    "    # Data prepration\n",
    "    X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "    \n",
    "    #PCA data reduction \n",
    "    X_cr_train, X_cr_test= StandardScaler().fit_transform(X_train_interim[remaining_sensors]), StandardScaler().fit_transform(X_test_interim[remaining_sensors])\n",
    "    pca = PCA()\n",
    "    component_train , component_test = pca.fit(X_cr_train).transform(X_cr_train), pca.transform(X_cr_test)\n",
    "    # print(pca.explained_variance_, np_component) # choos component which lambda >1 # kaiser\n",
    "\n",
    "    np_component = len(pca.explained_variance_[pca.explained_variance_>1])\n",
    "    print(pca.explained_variance_,'\\n', \"Nb components: \", np_component) # choos component which lambda >1 # kaiser\n",
    "    comp = ['comp' + str(i) for i in range(1,np_component+1)]\n",
    "    X_train_interim[comp],  X_test_interim[comp]= component_train[:, :np_component], component_test[:, :np_component]\n",
    "    \n",
    "\n",
    "    # create sequences train, test\n",
    "    train_array = gen_data_wrapper(X_train_interim, sequence_length, comp)\n",
    "    label_array = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'])\n",
    "\n",
    "    test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,comp, -99.))\n",
    "               for unit_nr in X_test_interim['Unit'].unique())\n",
    "    \n",
    "    test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "    test_rul = rul_piecewise_fct(y_test,rul_piecewise)\n",
    "    \n",
    "    input_shape = (sequence_length, len(comp))\n",
    "    model = model_lstm_1layer(input_shape, nodes_per_layer[0], dropout, activation)\n",
    "    print(train_array.shape, label_array.shape, test_array.shape)\n",
    "            \n",
    "    # Model fitting\n",
    "    cb = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        start_time = time.time()\n",
    "        model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n",
    "        weights_file = model.get_weights()\n",
    "        model.set_weights(weights_file)  # reset optimizer and node weights before every training iteration\n",
    "        history = model.fit(train_array, label_array,\n",
    "                                validation_data=(test_array, test_rul),\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                callbacks=[cb],\n",
    "                                verbose=1)\n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "        mse.append(history.history['val_loss'][-1])\n",
    "\n",
    "        y_hat_val_split = model.predict(test_array)\n",
    "        R2_val.append(r2_score(test_rul, y_hat_val_split))\n",
    "        RMSE.append(np.sqrt(mean_squared_error(test_rul, y_hat_val_split)))\n",
    "        score_val.append(compute_s_score(test_rul, y_hat_val_split))\n",
    "            \n",
    "        \n",
    "    #  append results\n",
    "    d = {'RMSE' :np.mean(RMSE), 'std_RMSE' :np.std(RMSE),\n",
    "         'S_score' :np.mean(score_val), 'std_S_score' :np.std(score_val),\n",
    "         'MSE':np.mean(mse), 'std_MSE':np.std(mse),\n",
    "         'nodes':str(nodes_per_layer), 'dropout':dropout, \n",
    "         'activation':activation, 'batch_size':batch_size, 'TW' : sequence_length,\n",
    "         'time': training_time}\n",
    "\n",
    "#     results = results.append(pd.DataFrame(d, index=[0]), ignore_index=True)\n",
    "    results_pca001 = pd.concat([results_pca001, pd.DataFrame(d, index=[0])], ignore_index=True)\n",
    "    results_pca001.to_csv('../results/pca/fd001.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>std_RMSE</th>\n",
       "      <th>S_score</th>\n",
       "      <th>std_S_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>TW</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25.760545</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1524.348391</td>\n",
       "      <td>0.0</td>\n",
       "      <td>853.147217</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "      <td>49.599175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23.572769</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1357.352482</td>\n",
       "      <td>0.0</td>\n",
       "      <td>735.563232</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "      <td>36.226404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24.491559</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1475.492664</td>\n",
       "      <td>0.0</td>\n",
       "      <td>642.956482</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "      <td>55.152636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26.943685</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1641.376650</td>\n",
       "      <td>0.0</td>\n",
       "      <td>746.683289</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "      <td>44.794605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25.830680</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1599.015014</td>\n",
       "      <td>0.0</td>\n",
       "      <td>803.219543</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "      <td>28.847952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RMSE  std_RMSE      S_score  std_S_score         MSE  std_MSE  nodes  \\\n",
       "0  25.760545       0.0  1524.348391          0.0  853.147217      0.0  [128]   \n",
       "1  23.572769       0.0  1357.352482          0.0  735.563232      0.0  [128]   \n",
       "2  24.491559       0.0  1475.492664          0.0  642.956482      0.0  [128]   \n",
       "3  26.943685       0.0  1641.376650          0.0  746.683289      0.0  [128]   \n",
       "4  25.830680       0.0  1599.015014          0.0  803.219543      0.0  [128]   \n",
       "\n",
       "   dropout activation  batch_size  TW       time  \n",
       "0      0.2       tanh          64  30  49.599175  \n",
       "1      0.2       tanh          64  30  36.226404  \n",
       "2      0.2       tanh          64  30  55.152636  \n",
       "3      0.2       tanh          64  30  44.794605  \n",
       "4      0.2       tanh          64  30  28.847952  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_pca001"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
