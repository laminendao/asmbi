rcommander
don<-data.frame(niveau=factor(c("CP", "CE1", "CE2", "CM1", "CM2")), temps =
c(3139,
1564, 1135, 748, 663, 2972, 1499, 1029, 884, 743, 3055, 1518,
1032, 769, 775, 3098, 1429, 1021, 794, 683))
don
don$niveau<-ordered(don$niveau, levels=c("CP", "CE1", "CE2", "CM1", "CM2"))
stripchart(temps~niveau, vertical = TRUE, data=don,pch=20)
tapply(don$temps, INDEX = don$niveau, FUN = mean)
varinter <- function(x, gpe) {
moyennes <- tapply(x, gpe, mean)
effectifs <- tapply(x, gpe, length)
res <- (sum(effectifs * (moyennes - mean(x))^2))
return(res)
}
varInter = varinter(don$temps, don$niveau)
varInter
vartot <- function(x) {
res <- sum((x - mean(x))^2)
return(res)
}
vartot(don$temps)
don <- matrix(c(68,15,5,20,
119,54,29,84,
26,14,14,17
,7,10,16,94),4,4)
rownames(don) <- c("marron","noisette","vert","bleu")
colnames(don) <- c("brune","chatain","roux","blond")
don <- t(don)
tablecont <- t(addmargins(don,FUN=sum,quiet=TRUE))
tablecont
# Exo1
# eta2 = Vinter/Vintra
don <- matrix(c(68,15,5,20,119,54,29,84, 26,14,14,17 ,7,10,16,94),4,4)
rownames(don) <- c("marron","noisette","vert","bleu")
colnames(don) <- c("brune","chatain","roux","blond")
don <- t(don)
tablecont <- t(addmargins(don,FUN=sum,quiet=TRUE))
tablecont
# Exo1
# eta2 = Vinter/Vintra
don <- matrix(c(68,15,5,20,119,54,29,84, 26,14,14,17 ,7,10,16,94),4,4)
rownames(don) <- c("marron","noisette","vert","bleu")
colnames(don) <- c("brune","chatain","roux","blond")
don <- t(don)
tablecont <- t(addmargins(don,FUN=sum,quiet=TRUE))
tablecont
don<-data.frame(niveau=factor(c("CP", "CE1", "CE2", "CM1", "CM2")), temps =
+ c(3139,
+ 1564, 1135, 748, 663, 2972, 1499, 1029, 884, 743, 3055, 1518,
+ 1032, 769, 775, 3098, 1429, 1021, 794, 683))
don
don$niveau<-ordered(don$niveau, levels=c("CP", "CE1", "CE2", "CM1", "CM2"))
stripchart(temps~niveau, vertical = TRUE, data=don,pch=20)
tapply(don$temps, INDEX = don$niveau, FUN = mean)
> varinter <- function(x, gpe) {
+  moyennes <- tapply(x, gpe, mean)
+  effectifs <- tapply(x, gpe, length)
+  res <- (sum(effectifs * (moyennes - mean(x))^2))
+ return(res)
+ }
vartot(don$temps)
don$niveau<-ordered(don$niveau, levels=c("CP", "CE1", "CE2", "CM1", "CM2"))
stripchart(temps~niveau, vertical = TRUE, data=don,pch=20)
tapply(don$temps, INDEX = don$niveau, FUN = mean)
varinter <- function(x, gpe) {
+  moyennes <- tapply(x, gpe, mean)
+  effectifs <- tapply(x, gpe, length)
+  res <- (sum(effectifs * (moyennes - mean(x))^2))
+ return(res)}
vartot(don$temps)
eta2(don$temps,don$niveau)
eta2 <- function(x, gpe) {
res <- varinter(x, gpe)/vartot(x)
return(res)
}
eta2(don$temps,don$niveau)
varinter <- function(x, gpe) {
moyennes <- tapply(x, gpe, mean)
effectifs <- tapply(x, gpe, length)
res <- (sum(effectifs * (moyennes - mean(x))^2))
}
eta2(don$temps,don$niveau)
t(addmargins(prop.table(don,2), FUN=sum, quiet=TRUE, margin=1))
don <- matrix(c(68,15,5,20,119,54,29,84, 26,14,14,17 ,7,10,16,94),4,4)
rownames(don) <- c("marron","noisette","vert","bleu")
colnames(don) <- c("brune","chatain","roux","blond")
don <- t(don)
tablecont <- t(addmargins(don,FUN=sum,quiet=TRUE))
tablecont
t(addmargins(prop.table(don,2), FUN=sum, quiet=TRUE, margin=1))
mosaic(t(don))
library(mosaic)
t(addmargins(prop.table(don,1), FUN=sum, quiet=TRUE, margin=2))
rowSums(don)/sum(don)
#marge colonne
rowSums(t(don))/sum(don)
# Valeurs_attendues
res.ch <- chisq.test(don)
res.ch$expected
sqrt((res.ch$statistic/592)/3)
CramerV(don)
library(CramerV)
library(DescTools)
local({pkg <- select.list(sort(.packages(all.available = TRUE)),graphics=TRUE)
if(nchar(pkg)) library(pkg, character.only=TRUE)})
utils:::menuInstallLocal()
utils:::menuInstallPkgs()
local({pkg <- select.list(sort(.packages(all.available = TRUE)),graphics=TRUE)
if(nchar(pkg)) library(pkg, character.only=TRUE)})
library(DescTools)
CramerV(don)
x<-seq(0, 30, 1/100)
plot(x, dchisq(x,df=9), type="l", ylab="")
x<-seq(0, 1000, 1/100)
plot(x, dchisq(x,df=9), type="l", ylab="")
don<-data.frame(niveau=factor(c("CP", "CE1", "CE2", "CM1", "CM2")), temps =
+ c(3139,
+ 1564, 1135, 748, 663, 2972, 1499, 1029, 884, 743, 3055, 1518,
+ 1032, 769, 775, 3098, 1429, 1021, 794, 683))
don
don$niveau<-ordered(don$niveau, levels=c("CP", "CE1", "CE2", "CM1", "CM2"))
stripchart(temps~niveau, vertical = TRUE, data=don,pch=20)
tapply(don$temps, INDEX = don$niveau, FUN = mean)
varinter <- function(x, gpe) {
moyennes <- tapply(x, gpe, mean)
effectifs <- tapply(x, gpe, length)
res <- (sum(effectifs * (moyennes - mean(x))^2))
vartot(don$temps)
# eta2 = Vinter/Vintra
eta2 <- function(x, gpe) {
res <- varinter(x, gpe)/vartot(x)
return(res)
}
eta2(don$temps,don$niveau)
eta = eta2(don$temps,don$niveau)
eta
q()
q()
q()
V1 = c(1,5,8)
V2 = c(8,2,0)
cor(V1,V2)
V1 = c(0,1,1,1,1,0)
sqrt(6)*V1
V1 = sqrt(6)*V1
V2 = c(1,-1,3,-3,1,-1)
cor(V1,V2)
V2 = c(4,2,6,0,4,2)
cor(V1,V2)
sqrt(6)
sqrt(6)*sqrt(6)
V1 = c(0,1,-1,1,-1,0)
V1=sqrt(6)*V1
V1
cor(V1,V2)
q()
q()
library(readr)
formated_result <- read_csv("C:/Users/mlndao/OneDrive - LECNAM/ThÃ¨se XAI/XAI_comparaison/rul_lstm_metric/sfds_result/formated_result.csv")
View(formated_result)
View(formated_result)
df_pca = formated_result[, 2:9]
df_pca
# Faire une acp
library(dplyr)
library(FactoMineR)
View(df_pca)
res.pca=PCA(df_pca, scale.unit=TRUE, ncp=2, graph=T)
res.pca
df_pca = formated_result[, 4:9]
View(df_pca)
res.pca=PCA(df_pca, scale.unit=TRUE, ncp=2, graph=T)
res.pca$eig
res.pca$var
View(formated_result)
df_pca = formated_result[, c(4:9, 13)]
res.pca=PCA(df_pca, scale.unit=TRUE, ncp=2, graph=T)
View(formated_result)
df_pca = formated_result[, c(4:5, 7:9, 13)]
res.pca=PCA(df_pca, scale.unit=TRUE, ncp=2, graph=T)
knitr::opts_chunk$set(echo = TRUE)
library(ClustVarLV)
library(MASS)
library(FactoMineR)
library(writexl)
# utilisation de la fonction SPC
library(PMA)
library(clusterCrit)
library(aricode)
library(corrplot)
library(diceR)
library(readxl)
library(readr)
library(reticulate)
reticulate::repl_python()
library(reticulate)
reticulate::repl_python()
library(reticulate)
reticulate::repl_python()
library(reticulate)
reticulate::repl_python()
library(reticulate)
# myenvs=conda_list()
#
# envname=myenvs$name[2]
# use_condaenv(envname, required = TRUE)
# or
use_condaenv("r-miniconda", required = TRUE)
library(reticulate)
myenvs=conda_list()
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
py_install("varclushi")
reticulate::repl_python()
reticulate::repl_python()
# myenvs=conda_list()
#
# envname=myenvs$name[2]
# use_condaenv(envname, required = TRUE)
# or
use_condaenv("C:\ProgramData\anaconda3\envs\tensorflow_env")
# myenvs=conda_list()
#
# envname=myenvs$name[2]
# use_condaenv(envname, required = TRUE)
# or
use_condaenv("C:\ProgramData\anaconda3\envs\tensorflow_env")
knitr::opts_chunk$set(echo = TRUE)
library(ClustVarLV)
library(MASS)
library(FactoMineR)
library(writexl)
# utilisation de la fonction SPC
library(PMA)
library(clusterCrit)
library(aricode)
library(corrplot)
library(diceR)
library(readxl)
library(readr)
library(reticulate)
# myenvs=conda_list()
#
# envname=myenvs$name[2]
# use_condaenv(envname, required = TRUE)
# or
use_condaenv("C:\ProgramData\anaconda3\envs\tensorflow_env")
library(reticulate)
# myenvs=conda_list()
#
# envname=myenvs$name[2]
# use_condaenv(envname, required = TRUE)
# or
use_condaenv("C:\\ProgramData\\anaconda3\\envs\\tensorflow_env")
reticulate::repl_python()
reticulate::repl_python()
import numpy as np
import pandas as pd
from varclushi import VarClusHi
from sklearn.metrics import silhouette_score
demo1_df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv', sep=';')
demo1_df.drop('quality',axis=1,inplace=True)
X = demo1_df
demo1_vc = VarClusHi(demo1_df,maxeigval2=1,maxclus=None)
demo1_vc.varclus()
demo1_df.columns
demo1_vc.info
import numpy as np
import pandas as pd
from varclushi import VarClusHi
from sklearn.metrics import silhouette_score
demo1_df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv', sep=';')
demo1_df.drop('quality',axis=1,inplace=True)
X = demo1_df
demo1_vc = VarClusHi(X,maxeigval2=1,maxclus=None)
X = demo1_df.drop('quality',axis=1,inplace=True)
demo1_df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv', sep=';')
X = demo1_df.drop('quality',axis=1,inplace=True)
demo1_vc = VarClusHi(X,maxeigval2=1,maxclus=None)
from varclushi import VarClusHi
import numpy as np
import pandas as pd
from varclushi import VarClusHi
quit
py_install("varclushi")
# myenvs=conda_list()
#
# envname=myenvs$name[2]
# use_condaenv(envname, required = TRUE)
# or
use_condaenv("C:\\ProgramData\\anaconda3")
library(reticulate)
# myenvs=conda_list()
#
# envname=myenvs$name[2]
# use_condaenv(envname, required = TRUE)
# or
use_condaenv("C:\\ProgramData\\anaconda3")
# myenvs=conda_list()
#
# envname=myenvs$name[2]
# use_condaenv(envname, required = TRUE)
# or
use_condaenv("C:\\ProgramData\\anaconda3\\envs\\tensorflow_env")
py_install("varclushi")
# myenvs=conda_list()
#
# envname=myenvs$name[2]
# use_condaenv(envname, required = TRUE)
# or
use_condaenv("C:\\ProgramData\\anaconda3\\envs\\weibull")
library(reticulate)
# myenvs=conda_list()
#
# envname=myenvs$name[2]
# use_condaenv(envname, required = TRUE)
# or
use_condaenv("C:\\ProgramData\\anaconda3\\envs\\weibull")
# myenvs=conda_list()
#
# envname=myenvs$name[2]
# use_condaenv(envname, required = TRUE)
# or
use_condaenv("C:\\ProgramData\\anaconda3\\envs\\tensorflow_env")
py_install("varclushi")
conda_install("varclushi")
py_install("varclushi")
library(reticulate)
# myenvs=conda_list()
#
# envname=myenvs$name[2]
# use_condaenv(envname, required = TRUE)
# or
# use_condaenv("C:\\ProgramData\\anaconda3\\envs\\tensorflow_env")
use_condaenv("r-reticulate")
knitr::opts_chunk$set(echo = TRUE)
library(ClustVarLV)
library(MASS)
library(FactoMineR)
# library(writexl)
# utilisation de la fonction SPC
library(PMA)
library(clusterCrit)
library(aricode)
library(corrplot)
library(diceR)
library(readxl)
library(readr)
library(dplyr)
# Fit varclus approach to get k cluster with a potential large number os small clusters
# Packages disponible en python
init_partition <- read_csv("init_partition/init_partition001.csv")
init_partition$cluster
X <- read_csv("data_cleaned/X_cleaned001.csv")
X <- X[, -1]
# Fit a clv initialization with K'+1 partition or CLV with K'=K
# package ClustVarLV package disponible dans R
res.clvkm.hc <- CLV_kmeans(X = X, method = "local", clust=init_partition$cluster, strategy="kplusone", rho = 0.5)
plot_var(res.clvkm.hc, beside = T, cex.lab = 1, label = T)
summary(res.clvkm.hc)
# First PC or first sparse componenent
# Prototypes :K most correlated variables with each latent variables
# Extract the group latent variables
# Extract the group membership of each variable
partition = get_partition(res.clvkm.hc,type="vector")
# or
get_partition(res.clvkm.hc,type="matrix")
# get_comp(res.clvkm.hc)
table(partition)
init_partition$cluster_final = partition
init_partition$idx = colnames(X)
mat_cor = list()
centers <- res.clvkm.hc$comp
for (i in 1:ncol(centers)) {
g1 = init_partition%>%filter(cluster_final==i)
mat_cor[[i]] = cor(centers[,i], X[,g1$idx])
}
mat_cor
summary(res.clvkm.hc)
knitr::opts_chunk$set(echo = TRUE)
library(ClustVarLV)
library(MASS)
library(FactoMineR)
library(writexl)
# utilisation de la fonction SPC
library(PMA)
library(clusterCrit)
library(aricode)
library(corrplot)
library(diceR)
library(readxl)
library(readr)
library(dplyr)
# Fit varclus approach to get k cluster with a potential large number os small clusters
# Packages disponible en python
init_partition <- read_csv("init_partition/init_partition_1_1_2")
# Fit varclus approach to get k cluster with a potential large number os small clusters
# Packages disponible en python
init_partition <- read_csv("init_partition/init_partition_1_1_2.csv")
setwd("C:/Users/mlndao/OneDrive/asmbi")
# Fit varclus approach to get k cluster with a potential large number os small clusters
# Packages disponible en python
init_partition <- read_csv("init_partition/init_partition_1_1_2.csv")
init_partition$cluster
X <- read_csv("data_cleaned/X_1_1_2.csv")
X <- X[, -1]
# Fit a clv initialization with K'+1 partition or CLV with K'=K
# package ClustVarLV package disponible dans R
res.clvkm.hc <- CLV_kmeans(X = X, method = "local", clust=init_partition$cluster, strategy="kplusone", rho = 0.5)
init_partition$cluster
init_partition <- read_csv("init_partition/init_partition_1_1_2.csv")
init_partition$cluster
X <- read_csv("data_cleaned/X_1_1_2.csv")
X <- X[, -1]
res.clvkm.hc <- CLV_kmeans(X = X, method = "local", clust=init_partition$cluster, strategy="kplusone", rho = 0.5)
plot_var(res.clvkm.hc, beside = T, cex.lab = 1, label = T)
summary(res.clvkm.hc)
# Extract the group latent variables
# Extract the group membership of each variable
partition = get_partition(res.clvkm.hc,type="vector")
partition
# or
get_partition(res.clvkm.hc,type="matrix")
table
table(partition)
?CLV_kmeans
res.clvkm.hc <- CLV_kmeans(X = X, method = "local", clust=init_partition$cluster, strategy="kplusone", rho = 0.5)
plot_var(res.clvkm.hc, beside = T, cex.lab = 1, label = T)
res.clvkm.hc <- CLV_kmeans(X = X, method = "local", clust=init_partition$cluster, strategy="kplusone")
plot_var(res.clvkm.hc, beside = T, cex.lab = 1, label = T)
summary(res.clvkm.hc)
# Extract the group latent variables
# Extract the group membership of each variable
partition = get_partition(res.clvkm.hc,type="vector")
# or
get_partition(res.clvkm.hc,type="matrix")
tab_part = table(partition)
list_cluster = as.numeric(names(tab_part))
init_partition$cluster_final = partition
init_partition$idx = colnames(X)
mat_cor = list()
centers <- res.clvkm.hc$comp
for (i in list_cluster) {
if(i!=0){
g1 = init_partition%>%filter(cluster_final==i)
mat_cor[[i]] = cor(centers[,i], X[,g1$idx])
}
}
mat_cor
plot_var(res.clvkm.hc, beside = T, cex.lab = 1, label = T, v_symbol=T)
for (elm in mat_cor) {
if(!is.null(elm)){
rownames(elm) = "Comp"
corrplot(elm, method = 'number')
}
}
for (elm in mat_cor){
if(!is.null(elm)){
indice_max <- which.max(elm)
print(colnames(elm)[indice_max])
}
}
