keep_fraction <- .05
min_ntree <- 5000
ntree_factor <- 5
final_ntree <- 5000
screen_params <- screen_control(drop_fraction = drop_fraction,
keep_fraction = keep_fraction, min_ntree = min_ntree,
mtry_factor = mtry_factor, ntree_factor = ntree_factor)
select_params <- select_control(drop_fraction = drop_fraction,
number_selected = number_selected, min_ntree = min_ntree,
mtry_factor = mtry_factor, ntree_factor = ntree_factor)
# Finally, we use wff to fit fuzzy forests to the data set.
wff_fit <- wff(expression_levels, weight, WGCNA_params = WGCNA_params,
screen_params = screen_params, select_params = select_params,
final_ntree = final_ntree, num_processors = 1)
wff_fit$feature_list
modplot(wff_fit)
# CAlcul des partitions des variables de X avec differentes méthodes de partitionnement
# EnsCl=clusteringSims(X,y, 19, E=E)
# EnsCl_liver = EnsCl
# save(EnsCl, file = "corrected/EnsCl_liver.RData")
load("corrected/EnsCl_liver.RData")
# EnsCl=cbind(EnsCl,ClWGCNA)
clusters = EnsCl ########################## à enlever
Mlist=list()
for(i in 1:dim(clusters)[2]){
Mlist[[i]]=clusters[,i]
}
k = 19
cspaCons = FctCSPA(Mlist,k,noms)
cspaCons = FctCSPA(Mlist,k,noms)
CSPA_NMF = cspaCons$ResNMFCspa
ResTotPart=cbind(EnsCl, CSPA_NMF)
ResTotPart
ResRandAjusted=matrix(0,dim(ResTotPart)[2],dim(ResTotPart)[2])
for(i in 1:dim(ResTotPart)[2]){
for( j in i:dim(ResTotPart)[2]){
ResRandAjusted[i,j]=ARI(ResTotPart[,i],ResTotPart[,j])
ResRandAjusted[j,i]=ResRandAjusted[i,j]
}
}
nom_partitions = colnames(ResTotPart)
ResRandAjusted
corrplot(ResRandAjusted, method = "color")
colnames(ResTotPart)
colnames(ResRandAjusted) = colnames(ResTotPart)
rownames(ResRandAjusted) = colnames(ResTotPart)
corrplot(ResRandAjusted, method = "color")
ResRandAjusted_correct = ResRandAjusted[-c(11), -c(11)]
corrplot(ResRandAjusted_correct, method = "color")
colnames(ResRandAjusted_correct) = c("NbClusWrd", "NbClusAvrg", "NbClusSgl", "NbClusComp", "k+1", "CLV",
"PAM", "hclus", "ScClus", "WGCNA", "CSPA_NMF")
rownames(ResRandAjusted_correct) = c("NbClusWrd", "NbClusAvrg", "NbClusSgl", "NbClusComp", "k+1", "CLV",
"PAM", "hclus", "ScClus", "WGCNA", "CSPA_NMF")
png("images/rand_secom.png", width = 300, height = 300)
corrplot(ResRandAjusted_correct, method = "color")
dev.off()
corrplot(ResRandAjusted_correct, method = "color")
png("images/rand_liver.png", width = 300, height = 300)
corrplot(ResRandAjusted_correct, method = "color")
dev.off()
corrplot(ResRandAjusted_correct, method = "color")
knitr::opts_chunk$set(echo = TRUE)
library(ClustVarLV)
library(MASS)
library(FactoMineR)
library(writexl)
# utilisation de la fonction SPC
library(PMA)
library(clusterCrit)
library(aricode)
library(corrplot)
library(diceR)
library(readxl)
library(readr)
library(dplyr)
library(ClustVarLV)
library(MASS)
library(FactoMineR)
library(writexl)
# utilisation de la fonction SPC
library(PMA)
library(clusterCrit)
library(aricode)
library(corrplot)
library(diceR)
library(readxl)
library(readr)
library(dplyr)
# Fit varclus approach to get k cluster with a potential large number os small clusters
# Packages disponible en python
init_partition <- read_csv("data/Data_cleaned/init_partition/bearing_1.csv")
cluster <- init_partition$cluster
# Identifier les classes uniques et créer un mapping
cluster <- init_partition$cluster
unique_classes <- sort(unique(cluster))
mapping <- setNames(seq_along(unique_classes), unique_classes)
# Recoder la partition
recoded_partition <- sapply(cluster, function(x) mapping[as.character(x)])
X <- read_csv("data/Data_cleaned/df_1.csv")
X <- subset(X, select = -c(223, 224))
# Fit a clv initialization with K'+1 partition or CLV with K'=K
# package ClustVarLV package disponible dans R
res.clvkm.hc <- CLV_kmeans(X = X, method = "directional", clust=recoded_partition, strategy="kplusone", rho=0.5,
iter.max = 500)
# plot_var(res.clvkm.hc, beside = T, cex.lab = 0.5, label = F)
# First PC or first sparse componenent
# Prototypes :K most correlated variables with each latent variables
# Extract the group latent variables
# Extract the group membership of each variable
partition = get_partition(res.clvkm.hc,type="vector")
# or
# get_partition(res.clvkm.hc,type="matrix")
# get_comp(res.clvkm.hc)
tab_part = table(partition)
list_cluster = as.numeric(names(tab_part))
init_partition$cluster_final = partition
init_partition$idx = colnames(X)
mat_cor = list()
centers <- res.clvkm.hc$comp
for (i in list_cluster) {
if(i!=0){
g1 = init_partition%>%filter(cluster_final==i)
mat_cor[[i]] = cor(centers[,i], X[,g1$idx])
}
}
mat_cor
for (elm in mat_cor){
if(!is.null(elm)){
rownames(elm) = "Comp"
corrplot(elm, method = 'number')
}
}
# Initialiser un vecteur vide
feature_selected <- character()  # Vecteur de type caractère
feat <- 1
# Boucle pour parcourir les éléments de mat_cor
for (elm in mat_cor){
if (!is.null(elm)) {
indice_max <- which.max(elm)  # Trouver l'indice de la valeur maximale
feature_selected[feat] <- colnames(elm)[indice_max]  # Ajouter au vecteur
feat <- feat + 1  # Incrémenter l'indice
}
}
# Vérification
print(feature_selected)
write.csv(feature_selected, "data/Data_cleaned/features/bearing_1.csv", row.names = FALSE)
knitr::opts_chunk$set(echo = TRUE)
library(ClustVarLV)
library(MASS)
library(FactoMineR)
library(writexl)
# utilisation de la fonction SPC
library(PMA)
library(clusterCrit)
library(aricode)
library(corrplot)
library(diceR)
library(readxl)
library(readr)
library(dplyr)
# Fit varclus approach to get k cluster with a potential large number os small clusters
# Packages disponible en python
init_partition <- read_csv("data/Data_cleaned/init_partition/bearing_2.csv")
cluster <- init_partition$cluster
# Identifier les classes uniques et créer un mapping
cluster <- init_partition$cluster
unique_classes <- sort(unique(cluster))
mapping <- setNames(seq_along(unique_classes), unique_classes)
# Recoder la partition
recoded_partition <- sapply(cluster, function(x) mapping[as.character(x)])
X <- read_csv("data/Data_cleaned/df_2.csv")
X <- subset(X, select = -c(223, 224))
# Fit a clv initialization with K'+1 partition or CLV with K'=K
# package ClustVarLV package disponible dans R
res.clvkm.hc <- CLV_kmeans(X = X, method = "directional", clust=recoded_partition, strategy="kplusone", rho=0.5, iter.max = 500)
# plot_var(res.clvkm.hc, beside = T, cex.lab = 0.5, label = F)
# First PC or first sparse componenent
# Prototypes :K most correlated variables with each latent variables
# Extract the group latent variables
# Extract the group membership of each variable
partition = get_partition(res.clvkm.hc,type="vector")
# or
# get_partition(res.clvkm.hc,type="matrix")
# get_comp(res.clvkm.hc)
tab_part = table(partition)
list_cluster = as.numeric(names(tab_part))
init_partition$cluster_final = partition
init_partition$idx = colnames(X)
mat_cor = list()
centers <- res.clvkm.hc$comp
for (i in list_cluster) {
if(i!=0){
g1 = init_partition%>%filter(cluster_final==i)
mat_cor[[i]] = cor(centers[,i], X[,g1$idx])
}
}
mat_cor
for (elm in mat_cor) {
if(!is.null(elm)){
rownames(elm) = "Comp"
corrplot(elm, method = 'number')
}
}
# Initialiser un vecteur vide
feature_selected <- character()  # Vecteur de type caractère
feat <- 1
# Boucle pour parcourir les éléments de mat_cor
for (elm in mat_cor){
if (!is.null(elm)) {
indice_max <- which.max(elm)  # Trouver l'indice de la valeur maximale
feature_selected[feat] <- colnames(elm)[indice_max]  # Ajouter au vecteur
feat <- feat + 1  # Incrémenter l'indice
}
}
# Vérification
print(feature_selected)
write.csv(feature_selected, "data/Data_cleaned/features/bearing_2.csv", row.names = FALSE)
knitr::opts_chunk$set(echo = TRUE)
library(ClustVarLV)
library(MASS)
library(FactoMineR)
library(writexl)
# utilisation de la fonction SPC
library(PMA)
library(clusterCrit)
library(aricode)
library(corrplot)
library(diceR)
library(readxl)
library(readr)
library(dplyr)
# Fit varclus approach to get k cluster with a potential large number os small clusters
# Packages disponible en python
init_partition <- read_csv("data/Data_cleaned/init_partition/bearing_3.csv")
cluster <- init_partition$cluster
# Identifier les classes uniques et créer un mapping
cluster <- init_partition$cluster
unique_classes <- sort(unique(cluster))
mapping <- setNames(seq_along(unique_classes), unique_classes)
# Recoder la partition
recoded_partition <- sapply(cluster, function(x) mapping[as.character(x)])
X <- read_csv("data/Data_cleaned/df_3.csv")
X <- subset(X, select = -c(223, 224))
# Fit a clv initialization with K'+1 partition or CLV with K'=K
# package ClustVarLV package disponible dans R
res.clvkm.hc <- CLV_kmeans(X = X, method = "directional", clust=recoded_partition, strategy="kplusone",rho=0.5, iter.max = 500)
# plot_var(res.clvkm.hc, beside = T, cex.lab = 0.5, label = F)
# First PC or first sparse componenent
# Prototypes :K most correlated variables with each latent variables
# Extract the group latent variables
# Extract the group membership of each variable
partition = get_partition(res.clvkm.hc,type="vector")
# or
# get_partition(res.clvkm.hc,type="matrix")
# get_comp(res.clvkm.hc)
tab_part = table(partition)
list_cluster = as.numeric(names(tab_part))
init_partition$cluster_final = partition
init_partition$idx = colnames(X)
mat_cor = list()
centers <- res.clvkm.hc$comp
for (i in list_cluster) {
if(i!=0){
g1 = init_partition%>%filter(cluster_final==i)
mat_cor[[i]] = cor(centers[,i], X[,g1$idx])
}
}
mat_cor
for (elm in mat_cor) {
if(!is.null(elm)){
rownames(elm) = "Comp"
corrplot(elm, method = 'number')
}
}
# Initialiser un vecteur vide
feature_selected <- character()  # Vecteur de type caractère
feat <- 1
# Boucle pour parcourir les éléments de mat_cor
for (elm in mat_cor){
if (!is.null(elm)) {
indice_max <- which.max(elm)  # Trouver l'indice de la valeur maximale
feature_selected[feat] <- colnames(elm)[indice_max]  # Ajouter au vecteur
feat <- feat + 1  # Incrémenter l'indice
}
}
# Vérification
print(feature_selected)
write.csv(feature_selected, "data/Data_cleaned/features/bearing_3.csv", row.names = FALSE)
knitr::opts_chunk$set(echo = TRUE)
library(ClustVarLV)
library(MASS)
library(FactoMineR)
library(writexl)
# utilisation de la fonction SPC
library(PMA)
library(clusterCrit)
library(aricode)
library(corrplot)
library(diceR)
library(readxl)
library(readr)
library(dplyr)
# Fit varclus approach to get k cluster with a potential large number os small clusters
# Packages disponible en python
init_partition <- read_csv("data/Data_cleaned/init_partition/bearing_2.csv")
cluster <- init_partition$cluster
# Identifier les classes uniques et créer un mapping
cluster <- init_partition$cluster
unique_classes <- sort(unique(cluster))
mapping <- setNames(seq_along(unique_classes), unique_classes)
# Recoder la partition
recoded_partition <- sapply(cluster, function(x) mapping[as.character(x)])
X <- read_csv("data/Data_cleaned/df_2.csv")
X <- subset(X, select = -c(223, 224))
# Fit a clv initialization with K'+1 partition or CLV with K'=K
# package ClustVarLV package disponible dans R
res.clvkm.hc <- CLV_kmeans(X = X, method = "directional", clust=recoded_partition, strategy="kplusone", rho=0.5, iter.max = 500)
# plot_var(res.clvkm.hc, beside = T, cex.lab = 0.5, label = F)
# First PC or first sparse componenent
# Prototypes :K most correlated variables with each latent variables
# Extract the group latent variables
# Extract the group membership of each variable
partition = get_partition(res.clvkm.hc,type="vector")
# or
# get_partition(res.clvkm.hc,type="matrix")
# get_comp(res.clvkm.hc)
tab_part = table(partition)
list_cluster = as.numeric(names(tab_part))
init_partition$cluster_final = partition
init_partition$idx = colnames(X)
mat_cor = list()
centers <- res.clvkm.hc$comp
for (i in list_cluster) {
if(i!=0){
g1 = init_partition%>%filter(cluster_final==i)
mat_cor[[i]] = cor(centers[,i], X[,g1$idx])
}
}
mat_cor
for (elm in mat_cor) {
if(!is.null(elm)){
rownames(elm) = "Comp"
corrplot(elm, method = 'number')
}
}
# Initialiser un vecteur vide
feature_selected <- character()  # Vecteur de type caractère
feat <- 1
# Boucle pour parcourir les éléments de mat_cor
for (elm in mat_cor){
if (!is.null(elm)) {
indice_max <- which.max(elm)  # Trouver l'indice de la valeur maximale
feature_selected[feat] <- colnames(elm)[indice_max]  # Ajouter au vecteur
feat <- feat + 1  # Incrémenter l'indice
}
}
# Vérification
print(feature_selected)
write.csv(feature_selected, "data/Data_cleaned/features/bearing_2.csv", row.names = FALSE)
knitr::opts_chunk$set(echo = TRUE)
# Fit varclus approach to get k cluster with a potential large number os small clusters
# Packages disponible en python
init_partition <- read_csv("data/Data_cleaned/init_partition/bearing_1.csv")
knitr::opts_chunk$set(echo = TRUE)
library(ClustVarLV)
library(MASS)
library(FactoMineR)
library(writexl)
# utilisation de la fonction SPC
library(PMA)
library(clusterCrit)
library(aricode)
library(corrplot)
library(diceR)
library(readxl)
library(readr)
library(dplyr)
library(ClustVarLV)
library(MASS)
library(FactoMineR)
library(writexl)
# utilisation de la fonction SPC
library(PMA)
library(clusterCrit)
library(aricode)
library(corrplot)
library(diceR)
library(readxl)
library(readr)
library(dplyr)
init_partition <- read_csv("data/Data_cleaned/init_partition/bearing_1.csv")
cluster <- init_partition$cluster
# Identifier les classes uniques et créer un mapping
cluster <- init_partition$cluster
unique_classes <- sort(unique(cluster))
mapping <- setNames(seq_along(unique_classes), unique_classes)
# Recoder la partition
recoded_partition <- sapply(cluster, function(x) mapping[as.character(x)])
X <- read_csv("data/Data_cleaned/df_1.csv")
X <- subset(X, select = -c(223, 224))
res.clvkm.hc <- CLV_kmeans(X = X, method = "directional", clust=recoded_partition, strategy="kplusone", rho=0.5,
iter.max = 500)
# Extract the group latent variables
# Extract the group membership of each variable
partition = get_partition(res.clvkm.hc,type="vector")
tab_part = table(partition)
tab_part
knitr::opts_chunk$set(echo = TRUE)
library(ClustVarLV)
library(MASS)
library(FactoMineR)
library(writexl)
# utilisation de la fonction SPC
library(PMA)
library(clusterCrit)
library(aricode)
library(corrplot)
library(diceR)
library(readxl)
library(readr)
library(dplyr)
# Fit varclus approach to get k cluster with a potential large number os small clusters
# Packages disponible en python
init_partition <- read_csv("data/Data_cleaned/init_partition/bearing_2.csv")
cluster <- init_partition$cluster
# Identifier les classes uniques et créer un mapping
cluster <- init_partition$cluster
unique_classes <- sort(unique(cluster))
mapping <- setNames(seq_along(unique_classes), unique_classes)
# Recoder la partition
recoded_partition <- sapply(cluster, function(x) mapping[as.character(x)])
X <- read_csv("data/Data_cleaned/df_2.csv")
X <- subset(X, select = -c(223, 224))
# Fit a clv initialization with K'+1 partition or CLV with K'=K
# package ClustVarLV package disponible dans R
res.clvkm.hc <- CLV_kmeans(X = X, method = "directional", clust=recoded_partition, strategy="kplusone", rho=0.5, iter.max = 500)
# plot_var(res.clvkm.hc, beside = T, cex.lab = 0.5, label = F)
# First PC or first sparse componenent
# Prototypes :K most correlated variables with each latent variables
# Extract the group latent variables
# Extract the group membership of each variable
partition = get_partition(res.clvkm.hc,type="vector")
# or
# get_partition(res.clvkm.hc,type="matrix")
# get_comp(res.clvkm.hc)
tab_part = table(partition)
list_cluster = as.numeric(names(tab_part))
init_partition$cluster_final = partition
init_partition$idx = colnames(X)
mat_cor = list()
centers <- res.clvkm.hc$comp
for (i in list_cluster) {
if(i!=0){
g1 = init_partition%>%filter(cluster_final==i)
mat_cor[[i]] = cor(centers[,i], X[,g1$idx])
}
}
mat_cor
tab_part
knitr::opts_chunk$set(echo = TRUE)
library(ClustVarLV)
library(MASS)
library(FactoMineR)
library(writexl)
# utilisation de la fonction SPC
library(PMA)
library(clusterCrit)
library(aricode)
library(corrplot)
library(diceR)
library(readxl)
library(readr)
library(dplyr)
# Fit varclus approach to get k cluster with a potential large number os small clusters
# Packages disponible en python
init_partition <- read_csv("data/Data_cleaned/init_partition/bearing_3.csv")
cluster <- init_partition$cluster
# Identifier les classes uniques et créer un mapping
cluster <- init_partition$cluster
unique_classes <- sort(unique(cluster))
mapping <- setNames(seq_along(unique_classes), unique_classes)
# Recoder la partition
recoded_partition <- sapply(cluster, function(x) mapping[as.character(x)])
X <- read_csv("data/Data_cleaned/df_3.csv")
X <- subset(X, select = -c(223, 224))
# Fit a clv initialization with K'+1 partition or CLV with K'=K
# package ClustVarLV package disponible dans R
res.clvkm.hc <- CLV_kmeans(X = X, method = "directional", clust=recoded_partition, strategy="kplusone",rho=0.5, iter.max = 500)
# plot_var(res.clvkm.hc, beside = T, cex.lab = 0.5, label = F)
# First PC or first sparse componenent
# Prototypes :K most correlated variables with each latent variables
# Extract the group latent variables
# Extract the group membership of each variable
partition = get_partition(res.clvkm.hc,type="vector")
# or
# get_partition(res.clvkm.hc,type="matrix")
# get_comp(res.clvkm.hc)
tab_part = table(partition)
list_cluster = as.numeric(names(tab_part))
init_partition$cluster_final = partition
init_partition$idx = colnames(X)
mat_cor = list()
centers <- res.clvkm.hc$comp
for (i in list_cluster) {
if(i!=0){
g1 = init_partition%>%filter(cluster_final==i)
mat_cor[[i]] = cor(centers[,i], X[,g1$idx])
}
}
mat_cor
tab_part
setwd("C:/Users/mlndao/OneDrive/asmbi")
