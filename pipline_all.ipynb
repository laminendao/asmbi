{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\ProgramData\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "from lime.lime_tabular import RecurrentTabularExplainer\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error, r2_score \n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn import preprocessing\n",
    "from keras import backend as K\n",
    "from sklearn.preprocessing import MinMaxScaler , StandardScaler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Activation, GRU\n",
    "from scipy import optimize\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "\n",
    "from sp_modif.model_function import *\n",
    "from sp_modif.methods import *\n",
    "from sp_modif.data_prep import *\n",
    "from sp_modif.evaluator import *\n",
    "from sp_modif.SHAP import *\n",
    "from sp_modif.L2X import *\n",
    "from methods import *\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEED = 0\n",
    "def set_seed(seed=SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    tf.random.set_seed(SEED)\n",
    "\n",
    "# Appeler la fonction pour fixer le seed\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61249, 27) (41214, 26) (248, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# Load data and preprocess\n",
    "train, test, y_test = prepare_data('FD004.txt')\n",
    "print(train.shape, test.shape, y_test.shape)\n",
    "sensor_names = ['T20','T24','T30','T50','P20','P15','P30','Nf','Nc','epr','Ps30','phi',\n",
    "                    'NRf','NRc','BPR','farB','htBleed','Nf_dmd','PCNfR_dmd','W31','W32']\n",
    "\n",
    "remaining_sensors = ['T24','T30','T50', 'P15', 'P30','Nf','Nc', 'epr','Ps30','phi',\n",
    "                    'NRf','NRc','BPR', 'farB','htBleed','W31','W32']\n",
    "drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "\n",
    "rul_piecewise = 120\n",
    "train['RUL'].clip(upper=rul_piecewise, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_lstm_1layer(input_shape, nodes_per_layer, dropout, activation):\n",
    "    \n",
    "    cb = keras.callbacks.EarlyStopping(monitor='loss', patience=4)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units = nodes_per_layer, activation=activation, \n",
    "                  input_shape=input_shape))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(256))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer=Adam(learning_rate=0.001))\n",
    "    # model.save_weights(weights_file)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51538, 40, 17) (51538, 1) (248, 40, 17)\n",
      "Epoch 1/20\n",
      "1611/1611 [==============================] - 13s 7ms/step - loss: 762.7661 - val_loss: 328.0673\n",
      "Epoch 2/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 294.9990 - val_loss: 232.8337\n",
      "Epoch 3/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 249.8761 - val_loss: 213.0574\n",
      "Epoch 4/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 231.6885 - val_loss: 207.1930\n",
      "Epoch 5/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 215.2335 - val_loss: 189.2900\n",
      "Epoch 6/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 200.3619 - val_loss: 187.0126\n",
      "Epoch 7/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 189.2251 - val_loss: 180.8434\n",
      "Epoch 8/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 182.3082 - val_loss: 169.7781\n",
      "Epoch 9/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 175.0479 - val_loss: 163.9495\n",
      "Epoch 10/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 172.0753 - val_loss: 171.1262\n",
      "Epoch 11/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 166.3886 - val_loss: 154.3001\n",
      "Epoch 12/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 165.4960 - val_loss: 174.5352\n",
      "Epoch 13/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 161.2135 - val_loss: 161.3920\n",
      "Epoch 14/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 159.8887 - val_loss: 153.0198\n",
      "Epoch 15/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 157.9835 - val_loss: 159.0417\n",
      "Epoch 16/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 156.1850 - val_loss: 168.4477\n",
      "Epoch 17/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 156.3740 - val_loss: 155.2228\n",
      "Epoch 18/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 153.7750 - val_loss: 153.6516\n",
      "Epoch 19/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 152.8708 - val_loss: 157.1522\n",
      "Epoch 20/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 151.8726 - val_loss: 157.5444\n",
      "8/8 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51538, 40, 17) (51538, 1) (248, 40, 17)\n",
      "Epoch 1/20\n",
      "1611/1611 [==============================] - 14s 8ms/step - loss: 768.5524 - val_loss: 357.2986\n",
      "Epoch 2/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 307.8346 - val_loss: 258.7034\n",
      "Epoch 3/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 260.2329 - val_loss: 259.2318\n",
      "Epoch 4/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 236.9283 - val_loss: 207.3871\n",
      "Epoch 5/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 221.7244 - val_loss: 203.9132\n",
      "Epoch 6/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 206.4735 - val_loss: 193.6375\n",
      "Epoch 7/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 194.2180 - val_loss: 170.4289\n",
      "Epoch 8/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 182.5528 - val_loss: 178.5842\n",
      "Epoch 9/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 176.8829 - val_loss: 168.9284\n",
      "Epoch 10/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 171.3504 - val_loss: 154.0263\n",
      "Epoch 11/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 168.8634 - val_loss: 162.2958\n",
      "Epoch 12/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 165.7318 - val_loss: 153.7889\n",
      "Epoch 13/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 163.7947 - val_loss: 168.0947\n",
      "Epoch 14/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 162.2239 - val_loss: 160.6669\n",
      "Epoch 15/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 160.3041 - val_loss: 153.3077\n",
      "Epoch 16/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 157.8728 - val_loss: 151.7543\n",
      "Epoch 17/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 156.0360 - val_loss: 159.9081\n",
      "Epoch 18/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 155.1613 - val_loss: 173.3584\n",
      "Epoch 19/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 155.0004 - val_loss: 167.2662\n",
      "Epoch 20/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 152.6180 - val_loss: 152.9929\n",
      "8/8 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51538, 40, 17) (51538, 1) (248, 40, 17)\n",
      "Epoch 1/20\n",
      "1611/1611 [==============================] - 13s 8ms/step - loss: 753.3668 - val_loss: 332.4726\n",
      "Epoch 2/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 291.6102 - val_loss: 275.1886\n",
      "Epoch 3/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 256.3381 - val_loss: 245.3007\n",
      "Epoch 4/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 244.8977 - val_loss: 252.5877\n",
      "Epoch 5/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 226.0944 - val_loss: 216.1996\n",
      "Epoch 6/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 215.0120 - val_loss: 199.5721\n",
      "Epoch 7/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 203.7311 - val_loss: 202.4122\n",
      "Epoch 8/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 193.6084 - val_loss: 189.6073\n",
      "Epoch 9/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 184.7121 - val_loss: 191.5496\n",
      "Epoch 10/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 179.3772 - val_loss: 173.7553\n",
      "Epoch 11/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 174.2852 - val_loss: 166.1596\n",
      "Epoch 12/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 171.0979 - val_loss: 173.3935\n",
      "Epoch 13/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 167.4876 - val_loss: 163.3571\n",
      "Epoch 14/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 165.2920 - val_loss: 173.9165\n",
      "Epoch 15/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 162.4385 - val_loss: 167.0608\n",
      "Epoch 16/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 162.1559 - val_loss: 163.3437\n",
      "Epoch 17/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 159.0485 - val_loss: 170.2850\n",
      "Epoch 18/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 157.3618 - val_loss: 163.1757\n",
      "Epoch 19/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 155.2088 - val_loss: 162.9583\n",
      "Epoch 20/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 155.0840 - val_loss: 162.2798\n",
      "8/8 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51538, 40, 17) (51538, 1) (248, 40, 17)\n",
      "Epoch 1/20\n",
      "1611/1611 [==============================] - 13s 8ms/step - loss: 817.0692 - val_loss: 340.1449\n",
      "Epoch 2/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 295.0835 - val_loss: 244.6979\n",
      "Epoch 3/20\n",
      "1611/1611 [==============================] - 13s 8ms/step - loss: 255.6130 - val_loss: 248.2810\n",
      "Epoch 4/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 235.3248 - val_loss: 237.1968\n",
      "Epoch 5/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 223.5804 - val_loss: 238.4308\n",
      "Epoch 6/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 206.3964 - val_loss: 202.0321\n",
      "Epoch 7/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 196.3786 - val_loss: 190.3189\n",
      "Epoch 8/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 187.0319 - val_loss: 225.8476\n",
      "Epoch 9/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 179.6189 - val_loss: 172.6664\n",
      "Epoch 10/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 174.9776 - val_loss: 193.4172\n",
      "Epoch 11/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 171.4982 - val_loss: 228.1268\n",
      "Epoch 12/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 166.4991 - val_loss: 189.9253\n",
      "Epoch 13/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 163.2391 - val_loss: 176.7972\n",
      "Epoch 14/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 161.9377 - val_loss: 168.8548\n",
      "Epoch 15/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 161.1185 - val_loss: 156.9911\n",
      "Epoch 16/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 157.6080 - val_loss: 162.5801\n",
      "Epoch 17/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 156.2803 - val_loss: 160.0764\n",
      "Epoch 18/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 155.1763 - val_loss: 163.5851\n",
      "Epoch 19/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 153.7146 - val_loss: 157.7239\n",
      "Epoch 20/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 152.8282 - val_loss: 177.6713\n",
      "8/8 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51538, 40, 17) (51538, 1) (248, 40, 17)\n",
      "Epoch 1/20\n",
      "1611/1611 [==============================] - 13s 8ms/step - loss: 866.2994 - val_loss: 383.3265\n",
      "Epoch 2/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 305.2556 - val_loss: 270.8068\n",
      "Epoch 3/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 249.9944 - val_loss: 224.3305\n",
      "Epoch 4/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 229.3898 - val_loss: 264.2542\n",
      "Epoch 5/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 210.1374 - val_loss: 216.4985\n",
      "Epoch 6/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 198.5177 - val_loss: 236.7857\n",
      "Epoch 7/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 189.3521 - val_loss: 207.5854\n",
      "Epoch 8/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 182.1673 - val_loss: 218.2080\n",
      "Epoch 9/20\n",
      "1611/1611 [==============================] - 13s 8ms/step - loss: 176.9418 - val_loss: 232.2634\n",
      "Epoch 10/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 171.4207 - val_loss: 194.1695\n",
      "Epoch 11/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 167.9801 - val_loss: 187.8001\n",
      "Epoch 12/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 163.5310 - val_loss: 188.2122\n",
      "Epoch 13/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 161.2068 - val_loss: 197.6093\n",
      "Epoch 14/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 160.3929 - val_loss: 178.6706\n",
      "Epoch 15/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 157.4545 - val_loss: 191.3115\n",
      "Epoch 16/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 157.3991 - val_loss: 167.2928\n",
      "Epoch 17/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 155.4136 - val_loss: 177.8455\n",
      "Epoch 18/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 152.7415 - val_loss: 203.7287\n",
      "Epoch 19/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 151.4303 - val_loss: 199.5851\n",
      "Epoch 20/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 151.7600 - val_loss: 190.8820\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "CPU times: user 19min 18s, sys: 55.5 s, total: 20min 13s\n",
      "Wall time: 19min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results_all = pd.DataFrame()\n",
    "for SEED in range(5):  \n",
    "    tf.random.set_seed(SEED)\n",
    "    mse = []\n",
    "    R2_val = []\n",
    "    RMSE = []\n",
    "    score_val = []\n",
    "    \n",
    "    # 0.20\t[64]\t0.3\ttanh\t32\t25\n",
    "    \n",
    "    # parameter's sample\n",
    "    # weights_file = \"weights_file_lstm_optimalmodel_all.h5\"\n",
    "    alpha = 0.3\n",
    "    sequence_length = 40\n",
    "    epochs = 20\n",
    "    nodes_per_layer = [64]\n",
    "    dropout = 0.2\n",
    "    activation = 'tanh'\n",
    "    batch_size = 32\n",
    "    remaining_sensors = remaining_sensors\n",
    "    # create model\n",
    "    input_shape = (sequence_length, len(remaining_sensors))\n",
    "    model = model_lstm_1layer(input_shape, nodes_per_layer[0], dropout, activation)\n",
    "    \n",
    "    # Data prepration\n",
    "    X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "\n",
    "    # create sequences train, test\n",
    "    train_array = gen_data_wrapper(X_train_interim, sequence_length,remaining_sensors)\n",
    "    label_array = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'])\n",
    "\n",
    "    test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,remaining_sensors, -99.))\n",
    "               for unit_nr in X_test_interim['Unit'].unique())\n",
    "    \n",
    "    test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "    test_rul = rul_piecewise_fct(y_test,rul_piecewise)\n",
    "    print(train_array.shape, label_array.shape, test_array.shape)\n",
    "            \n",
    "    # Model fitting\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        start_time = time.time()\n",
    "        model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n",
    "        weights_file = model.get_weights()\n",
    "        model.set_weights(weights_file)  # reset optimizer and node weights before every training iteration\n",
    "        history = model.fit(train_array, label_array,\n",
    "                                validation_data=(test_array, test_rul),\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                # callbacks=[cb],\n",
    "                                verbose=1)\n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "        mse.append(history.history['val_loss'][-1])\n",
    "\n",
    "        y_hat_val_split = model.predict(test_array)\n",
    "        R2_val.append(r2_score(test_rul, y_hat_val_split))\n",
    "        RMSE.append(np.sqrt(mean_squared_error(test_rul, y_hat_val_split)))\n",
    "        score_val.append(compute_s_score(test_rul, y_hat_val_split))\n",
    "            \n",
    "        \n",
    "    #  append results\n",
    "    d = {'RMSE' :np.mean(RMSE), 'std_RMSE' :np.std(RMSE),\n",
    "         'S_score' :np.mean(score_val), 'std_S_score' :np.std(score_val),\n",
    "         'MSE':np.mean(mse), 'std_MSE':np.std(mse),\n",
    "         'nodes':str(nodes_per_layer), 'dropout':dropout, \n",
    "         'activation':activation, 'batch_size':batch_size, 'TW' : sequence_length,\n",
    "         'time':training_time}\n",
    "\n",
    "#     results = results.append(pd.DataFrame(d, index=[0]), ignore_index=True)\n",
    "    results_all = pd.concat([results_all, pd.DataFrame(d, index=[0])], ignore_index=True)\n",
    "    results_all.to_csv('results/all/fd004.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>std_RMSE</th>\n",
       "      <th>S_score</th>\n",
       "      <th>std_S_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>TW</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.551669</td>\n",
       "      <td>0.0</td>\n",
       "      <td>741.853794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>157.544388</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>220.229309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.369032</td>\n",
       "      <td>0.0</td>\n",
       "      <td>720.105677</td>\n",
       "      <td>0.0</td>\n",
       "      <td>152.992905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>227.559174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.738909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>804.189686</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.279831</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>228.795339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.329341</td>\n",
       "      <td>0.0</td>\n",
       "      <td>641.524037</td>\n",
       "      <td>0.0</td>\n",
       "      <td>177.671310</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>230.277804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.816006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>775.655890</td>\n",
       "      <td>0.0</td>\n",
       "      <td>190.882004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>230.477888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RMSE  std_RMSE     S_score  std_S_score         MSE  std_MSE nodes  \\\n",
       "0  12.551669       0.0  741.853794          0.0  157.544388      0.0  [64]   \n",
       "1  12.369032       0.0  720.105677          0.0  152.992905      0.0  [64]   \n",
       "2  12.738909       0.0  804.189686          0.0  162.279831      0.0  [64]   \n",
       "3  13.329341       0.0  641.524037          0.0  177.671310      0.0  [64]   \n",
       "4  13.816006       0.0  775.655890          0.0  190.882004      0.0  [64]   \n",
       "\n",
       "   dropout activation  batch_size  TW        time  \n",
       "0      0.2       tanh          32  40  220.229309  \n",
       "1      0.2       tanh          32  40  227.559174  \n",
       "2      0.2       tanh          32  40  228.795339  \n",
       "3      0.2       tanh          32  40  230.277804  \n",
       "4      0.2       tanh          32  40  230.477888  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FD002 <a class=\"anchor\" id=\"fd002\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53759, 27) (33991, 26) (259, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# Load data and preprocess\n",
    "train, test, y_test = prepare_data('FD002.txt')\n",
    "print(train.shape, test.shape, y_test.shape)\n",
    "sensor_names = ['T20','T24','T30','T50','P20','P15','P30','Nf','Nc','epr','Ps30','phi',\n",
    "                    'NRf','NRc','BPR','farB','htBleed','Nf_dmd','PCNfR_dmd','W31','W32']\n",
    "\n",
    "remaining_sensors = ['T24','T30','T50', 'P15', 'P30','Nf','Nc', 'epr','Ps30','phi',\n",
    "                    'NRf','NRc','BPR','htBleed','W31','W32']\n",
    "drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "\n",
    "rul_piecewise = 125\n",
    "train['RUL'].clip(upper=rul_piecewise, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43619, 40, 16) (43619, 1) (259, 40, 16)\n",
      "Epoch 1/20\n",
      "341/341 [==============================] - 4s 9ms/step - loss: 2284.7837 - val_loss: 1184.4233\n",
      "Epoch 2/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 488.9340 - val_loss: 232.4040\n",
      "Epoch 3/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 277.7340 - val_loss: 208.3209\n",
      "Epoch 4/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 252.8589 - val_loss: 210.1246\n",
      "Epoch 5/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 239.3640 - val_loss: 197.7523\n",
      "Epoch 6/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 235.1574 - val_loss: 202.5185\n",
      "Epoch 7/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 233.2388 - val_loss: 186.3883\n",
      "Epoch 8/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 230.1901 - val_loss: 191.8869\n",
      "Epoch 9/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 224.1405 - val_loss: 196.1385\n",
      "Epoch 10/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 224.0703 - val_loss: 188.5056\n",
      "Epoch 11/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 219.6916 - val_loss: 192.3419\n",
      "Epoch 12/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 221.3274 - val_loss: 185.2276\n",
      "Epoch 13/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 219.9296 - val_loss: 188.5966\n",
      "Epoch 14/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 215.5274 - val_loss: 177.7591\n",
      "Epoch 15/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 214.2000 - val_loss: 195.0212\n",
      "Epoch 16/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 211.2679 - val_loss: 185.1345\n",
      "Epoch 17/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 211.2674 - val_loss: 195.9393\n",
      "Epoch 18/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 208.7072 - val_loss: 167.5057\n",
      "Epoch 19/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 206.1053 - val_loss: 171.5527\n",
      "Epoch 20/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 206.4769 - val_loss: 179.1922\n",
      "9/9 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43619, 40, 16) (43619, 1) (259, 40, 16)\n",
      "Epoch 1/20\n",
      "341/341 [==============================] - 4s 9ms/step - loss: 2342.7776 - val_loss: 1485.9114\n",
      "Epoch 2/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 501.3373 - val_loss: 232.1009\n",
      "Epoch 3/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 284.6033 - val_loss: 210.2034\n",
      "Epoch 4/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 268.0725 - val_loss: 190.1271\n",
      "Epoch 5/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 245.6246 - val_loss: 201.6791\n",
      "Epoch 6/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 242.2276 - val_loss: 184.8576\n",
      "Epoch 7/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 235.3008 - val_loss: 181.2748\n",
      "Epoch 8/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 233.1525 - val_loss: 184.5342\n",
      "Epoch 9/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 228.3228 - val_loss: 183.7434\n",
      "Epoch 10/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 224.9263 - val_loss: 179.9525\n",
      "Epoch 11/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 224.0370 - val_loss: 189.5378\n",
      "Epoch 12/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 219.2511 - val_loss: 171.0850\n",
      "Epoch 13/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 217.3723 - val_loss: 177.9568\n",
      "Epoch 14/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 214.9292 - val_loss: 171.8691\n",
      "Epoch 15/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 212.3354 - val_loss: 170.9448\n",
      "Epoch 16/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 211.1767 - val_loss: 172.4133\n",
      "Epoch 17/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 209.3891 - val_loss: 174.2648\n",
      "Epoch 18/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 207.7528 - val_loss: 181.6481\n",
      "Epoch 19/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 203.8762 - val_loss: 171.4424\n",
      "Epoch 20/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 204.2929 - val_loss: 178.2119\n",
      "9/9 [==============================] - 1s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43619, 40, 16) (43619, 1) (259, 40, 16)\n",
      "Epoch 1/20\n",
      "341/341 [==============================] - 4s 9ms/step - loss: 2312.6143 - val_loss: 1375.4172\n",
      "Epoch 2/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 517.6144 - val_loss: 241.2267\n",
      "Epoch 3/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 293.7717 - val_loss: 206.6121\n",
      "Epoch 4/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 264.7542 - val_loss: 202.7057\n",
      "Epoch 5/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 256.7372 - val_loss: 196.3154\n",
      "Epoch 6/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 245.3320 - val_loss: 191.3672\n",
      "Epoch 7/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 241.0967 - val_loss: 199.1064\n",
      "Epoch 8/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 235.3632 - val_loss: 200.3324\n",
      "Epoch 9/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 230.2278 - val_loss: 190.0517\n",
      "Epoch 10/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 231.6207 - val_loss: 206.1999\n",
      "Epoch 11/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 227.3792 - val_loss: 199.3103\n",
      "Epoch 12/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 228.5390 - val_loss: 198.6482\n",
      "Epoch 13/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 219.6096 - val_loss: 202.6657\n",
      "Epoch 14/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 217.9097 - val_loss: 191.7222\n",
      "Epoch 15/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 215.3685 - val_loss: 189.4078\n",
      "Epoch 16/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 214.2215 - val_loss: 189.2334\n",
      "Epoch 17/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 213.8795 - val_loss: 198.0862\n",
      "Epoch 18/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 207.9833 - val_loss: 182.0893\n",
      "Epoch 19/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 207.0907 - val_loss: 191.5364\n",
      "Epoch 20/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 209.6435 - val_loss: 178.1689\n",
      "9/9 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43619, 40, 16) (43619, 1) (259, 40, 16)\n",
      "Epoch 1/20\n",
      "341/341 [==============================] - 4s 10ms/step - loss: 2347.4243 - val_loss: 1551.2659\n",
      "Epoch 2/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 551.0797 - val_loss: 245.1803\n",
      "Epoch 3/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 275.3705 - val_loss: 195.8227\n",
      "Epoch 4/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 253.0560 - val_loss: 224.1236\n",
      "Epoch 5/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 243.1407 - val_loss: 183.4476\n",
      "Epoch 6/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 232.0502 - val_loss: 181.7664\n",
      "Epoch 7/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 228.7188 - val_loss: 182.9298\n",
      "Epoch 8/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 228.1400 - val_loss: 176.8402\n",
      "Epoch 9/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 224.3821 - val_loss: 185.5224\n",
      "Epoch 10/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 221.7222 - val_loss: 177.4453\n",
      "Epoch 11/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 219.7609 - val_loss: 198.0157\n",
      "Epoch 12/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 218.9003 - val_loss: 183.5460\n",
      "Epoch 13/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 217.9325 - val_loss: 179.5179\n",
      "Epoch 14/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 215.8728 - val_loss: 173.4287\n",
      "Epoch 15/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 214.7942 - val_loss: 179.2447\n",
      "Epoch 16/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 210.1801 - val_loss: 171.4576\n",
      "Epoch 17/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 208.7482 - val_loss: 181.3923\n",
      "Epoch 18/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 205.9745 - val_loss: 172.5556\n",
      "Epoch 19/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 206.5656 - val_loss: 175.2587\n",
      "Epoch 20/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 202.7684 - val_loss: 183.5944\n",
      "9/9 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43619, 40, 16) (43619, 1) (259, 40, 16)\n",
      "Epoch 1/20\n",
      "341/341 [==============================] - 4s 9ms/step - loss: 2275.6174 - val_loss: 1325.1198\n",
      "Epoch 2/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 536.1032 - val_loss: 243.3846\n",
      "Epoch 3/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 279.8450 - val_loss: 202.9051\n",
      "Epoch 4/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 252.0998 - val_loss: 207.0529\n",
      "Epoch 5/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 242.0358 - val_loss: 183.7728\n",
      "Epoch 6/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 231.5358 - val_loss: 178.5688\n",
      "Epoch 7/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 228.2318 - val_loss: 203.5125\n",
      "Epoch 8/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 226.3349 - val_loss: 173.9748\n",
      "Epoch 9/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 221.1027 - val_loss: 197.2489\n",
      "Epoch 10/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 218.1178 - val_loss: 178.2442\n",
      "Epoch 11/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 217.5971 - val_loss: 173.3676\n",
      "Epoch 12/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 215.6693 - val_loss: 176.1133\n",
      "Epoch 13/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 211.1023 - val_loss: 203.3021\n",
      "Epoch 14/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 211.5169 - val_loss: 203.3211\n",
      "Epoch 15/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 208.9190 - val_loss: 206.4723\n",
      "Epoch 16/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 208.2958 - val_loss: 213.2473\n",
      "Epoch 17/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 203.3949 - val_loss: 205.7394\n",
      "Epoch 18/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 203.2192 - val_loss: 208.9626\n",
      "Epoch 19/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 200.0624 - val_loss: 210.3093\n",
      "Epoch 20/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 200.9444 - val_loss: 186.1168\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "CPU times: user 4min 56s, sys: 32.5 s, total: 5min 29s\n",
      "Wall time: 5min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results_all002 = pd.DataFrame()\n",
    "for SEED in range(5):  \n",
    "    tf.random.set_seed(SEED)\n",
    "    mse = []\n",
    "    R2_val = []\n",
    "    RMSE = []\n",
    "    score_val = []\n",
    "    \n",
    "    # parameter's sample\n",
    "    # weights_file = \"weights_file_lstm_optimalmodel_all.h5\"\n",
    "    alpha = 0.2\n",
    "    sequence_length = 40\n",
    "    epochs = 20\n",
    "    nodes_per_layer = [32]\n",
    "    dropout = 0.1\n",
    "    activation = 'tanh'\n",
    "    batch_size = 128\n",
    "    remaining_sensors = remaining_sensors\n",
    "    # create model\n",
    "    input_shape = (sequence_length, len(remaining_sensors))\n",
    "    model = model_lstm_1layer(input_shape, nodes_per_layer[0], dropout, activation)\n",
    "    \n",
    "    # Data prepration\n",
    "    X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "\n",
    "    # create sequences train, test\n",
    "    train_array = gen_data_wrapper(X_train_interim, sequence_length,remaining_sensors)\n",
    "    label_array = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'])\n",
    "\n",
    "    test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,remaining_sensors, -99.))\n",
    "               for unit_nr in X_test_interim['Unit'].unique())\n",
    "    \n",
    "    test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "    test_rul = rul_piecewise_fct(y_test,rul_piecewise)\n",
    "    print(train_array.shape, label_array.shape, test_array.shape)\n",
    "            \n",
    "    # Model fitting\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        start_time = time.time()\n",
    "        model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n",
    "        weights_file = model.get_weights()\n",
    "        model.set_weights(weights_file)  # reset optimizer and node weights before every training iteration\n",
    "        history = model.fit(train_array, label_array,\n",
    "                                validation_data=(test_array, test_rul),\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                # callbacks=[cb],\n",
    "                                verbose=1)\n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "        mse.append(history.history['val_loss'][-1])\n",
    "\n",
    "        y_hat_val_split = model.predict(test_array)\n",
    "        R2_val.append(r2_score(test_rul, y_hat_val_split))\n",
    "        RMSE.append(np.sqrt(mean_squared_error(test_rul, y_hat_val_split)))\n",
    "        score_val.append(compute_s_score(test_rul, y_hat_val_split))\n",
    "            \n",
    "        \n",
    "    #  append results\n",
    "    d = {'RMSE' :np.mean(RMSE), 'std_RMSE' :np.std(RMSE),\n",
    "         'S_score' :np.mean(score_val), 'std_S_score' :np.std(score_val),\n",
    "         'MSE':np.mean(mse), 'std_MSE':np.std(mse),\n",
    "         'nodes':str(nodes_per_layer), 'dropout':dropout, \n",
    "         'activation':activation, 'batch_size':batch_size, 'TW' : sequence_length,\n",
    "         'time':training_time}\n",
    "\n",
    "#     results = results.append(pd.DataFrame(d, index=[0]), ignore_index=True)\n",
    "    results_all002 = pd.concat([results_all002, pd.DataFrame(d, index=[0])], ignore_index=True)\n",
    "    results_all002.to_csv('results/all/fd002.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>std_RMSE</th>\n",
       "      <th>S_score</th>\n",
       "      <th>std_S_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>TW</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.386268</td>\n",
       "      <td>0.0</td>\n",
       "      <td>933.507017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>179.192200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "      <td>40</td>\n",
       "      <td>60.836705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.349602</td>\n",
       "      <td>0.0</td>\n",
       "      <td>974.155848</td>\n",
       "      <td>0.0</td>\n",
       "      <td>178.211899</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "      <td>40</td>\n",
       "      <td>59.740708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.347993</td>\n",
       "      <td>0.0</td>\n",
       "      <td>934.270798</td>\n",
       "      <td>0.0</td>\n",
       "      <td>178.168915</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "      <td>40</td>\n",
       "      <td>58.691479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.549702</td>\n",
       "      <td>0.0</td>\n",
       "      <td>900.602838</td>\n",
       "      <td>0.0</td>\n",
       "      <td>183.594406</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "      <td>40</td>\n",
       "      <td>64.237170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.642462</td>\n",
       "      <td>0.0</td>\n",
       "      <td>867.849042</td>\n",
       "      <td>0.0</td>\n",
       "      <td>186.116791</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "      <td>40</td>\n",
       "      <td>58.900007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RMSE  std_RMSE     S_score  std_S_score         MSE  std_MSE nodes  \\\n",
       "0  13.386268       0.0  933.507017          0.0  179.192200      0.0  [32]   \n",
       "1  13.349602       0.0  974.155848          0.0  178.211899      0.0  [32]   \n",
       "2  13.347993       0.0  934.270798          0.0  178.168915      0.0  [32]   \n",
       "3  13.549702       0.0  900.602838          0.0  183.594406      0.0  [32]   \n",
       "4  13.642462       0.0  867.849042          0.0  186.116791      0.0  [32]   \n",
       "\n",
       "   dropout activation  batch_size  TW       time  \n",
       "0      0.1       tanh         128  40  60.836705  \n",
       "1      0.1       tanh         128  40  59.740708  \n",
       "2      0.1       tanh         128  40  58.691479  \n",
       "3      0.1       tanh         128  40  64.237170  \n",
       "4      0.1       tanh         128  40  58.900007  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_all002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FD003 <a class=\"anchor\" id=\"fd003\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53759, 27) (33991, 26) (259, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# Load data and preprocess\n",
    "train, test, y_test = prepare_data('FD002.txt')\n",
    "print(train.shape, test.shape, y_test.shape)\n",
    "sensor_names = ['T20','T24','T30','T50','P20','P15','P30','Nf','Nc','epr','Ps30','phi',\n",
    "                    'NRf','NRc','BPR','farB','htBleed','Nf_dmd','PCNfR_dmd','W31','W32']\n",
    "\n",
    "remaining_sensors = ['T24','T30','T50', 'P15', 'P30','Nf','Nc', 'epr','Ps30','phi',\n",
    "                    'NRf','NRc','BPR','htBleed','W31','W32']\n",
    "drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "\n",
    "rul_piecewise = 125\n",
    "train['RUL'].clip(upper=rul_piecewise, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44919, 35, 16) (44919, 1) (259, 35, 16)\n",
      "Epoch 1/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 886.5042 - val_loss: 319.7583\n",
      "Epoch 2/20\n",
      "1404/1404 [==============================] - 8s 6ms/step - loss: 283.5926 - val_loss: 230.9650\n",
      "Epoch 3/20\n",
      "1404/1404 [==============================] - 8s 6ms/step - loss: 252.2755 - val_loss: 201.6743\n",
      "Epoch 4/20\n",
      "1404/1404 [==============================] - 8s 6ms/step - loss: 237.7799 - val_loss: 189.3466\n",
      "Epoch 5/20\n",
      "1404/1404 [==============================] - 8s 6ms/step - loss: 227.3036 - val_loss: 171.3220\n",
      "Epoch 6/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 218.2708 - val_loss: 168.0999\n",
      "Epoch 7/20\n",
      "1404/1404 [==============================] - 9s 7ms/step - loss: 214.3134 - val_loss: 181.3601\n",
      "Epoch 8/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 206.2580 - val_loss: 177.7361\n",
      "Epoch 9/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 199.8463 - val_loss: 170.3373\n",
      "Epoch 10/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 195.3867 - val_loss: 185.7114\n",
      "Epoch 11/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 193.0003 - val_loss: 193.7086\n",
      "Epoch 12/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 190.8898 - val_loss: 172.7049\n",
      "Epoch 13/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 186.5093 - val_loss: 173.7872\n",
      "Epoch 14/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 185.3799 - val_loss: 169.7116\n",
      "Epoch 15/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 184.5261 - val_loss: 163.8571\n",
      "Epoch 16/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 181.1580 - val_loss: 175.8474\n",
      "Epoch 17/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 179.3920 - val_loss: 169.5831\n",
      "Epoch 18/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 178.3683 - val_loss: 156.7474\n",
      "Epoch 19/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 176.6124 - val_loss: 169.7509\n",
      "Epoch 20/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 176.0355 - val_loss: 152.9959\n",
      "9/9 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44919, 35, 16) (44919, 1) (259, 35, 16)\n",
      "Epoch 1/20\n",
      "1404/1404 [==============================] - 10s 6ms/step - loss: 772.4301 - val_loss: 205.7240\n",
      "Epoch 2/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 272.9133 - val_loss: 185.1960\n",
      "Epoch 3/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 243.5848 - val_loss: 180.9420\n",
      "Epoch 4/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 233.5709 - val_loss: 199.6880\n",
      "Epoch 5/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 222.2003 - val_loss: 177.5709\n",
      "Epoch 6/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 214.3226 - val_loss: 180.2964\n",
      "Epoch 7/20\n",
      "1404/1404 [==============================] - 9s 7ms/step - loss: 207.2195 - val_loss: 185.6786\n",
      "Epoch 8/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 201.4224 - val_loss: 175.3193\n",
      "Epoch 9/20\n",
      "1404/1404 [==============================] - 9s 7ms/step - loss: 199.0803 - val_loss: 171.6209\n",
      "Epoch 10/20\n",
      "1404/1404 [==============================] - 9s 7ms/step - loss: 192.6386 - val_loss: 181.4002\n",
      "Epoch 11/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 192.0719 - val_loss: 172.9645\n",
      "Epoch 12/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 188.8302 - val_loss: 193.8364\n",
      "Epoch 13/20\n",
      "1404/1404 [==============================] - 11s 8ms/step - loss: 187.5087 - val_loss: 185.4088\n",
      "Epoch 14/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 185.4720 - val_loss: 187.7965\n",
      "Epoch 15/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 184.6570 - val_loss: 168.0076\n",
      "Epoch 16/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 183.3858 - val_loss: 168.2340\n",
      "Epoch 17/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 181.4144 - val_loss: 178.0901\n",
      "Epoch 18/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 179.8394 - val_loss: 163.6173\n",
      "Epoch 19/20\n",
      "1404/1404 [==============================] - 11s 8ms/step - loss: 177.9520 - val_loss: 181.1564\n",
      "Epoch 20/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 176.2646 - val_loss: 160.2890\n",
      "9/9 [==============================] - 1s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44919, 35, 16) (44919, 1) (259, 35, 16)\n",
      "Epoch 1/20\n",
      "1404/1404 [==============================] - 12s 7ms/step - loss: 888.2208 - val_loss: 256.3362\n",
      "Epoch 2/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 272.3698 - val_loss: 206.1081\n",
      "Epoch 3/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 244.7237 - val_loss: 182.8092\n",
      "Epoch 4/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 229.8972 - val_loss: 220.7837\n",
      "Epoch 5/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 221.9341 - val_loss: 183.2493\n",
      "Epoch 6/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 213.7457 - val_loss: 189.4396\n",
      "Epoch 7/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 209.1642 - val_loss: 180.9539\n",
      "Epoch 8/20\n",
      "1404/1404 [==============================] - 9s 7ms/step - loss: 202.3134 - val_loss: 191.4040\n",
      "Epoch 9/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 197.8971 - val_loss: 170.5309\n",
      "Epoch 10/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 193.3029 - val_loss: 174.0141\n",
      "Epoch 11/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 188.2686 - val_loss: 170.5560\n",
      "Epoch 12/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 186.8900 - val_loss: 180.1564\n",
      "Epoch 13/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 183.9980 - val_loss: 165.8591\n",
      "Epoch 14/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 182.1568 - val_loss: 172.6038\n",
      "Epoch 15/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 179.9523 - val_loss: 174.6693\n",
      "Epoch 16/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 177.2245 - val_loss: 193.3188\n",
      "Epoch 17/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 176.6154 - val_loss: 179.6847\n",
      "Epoch 18/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 173.2787 - val_loss: 180.7564\n",
      "Epoch 19/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 173.2975 - val_loss: 177.1434\n",
      "Epoch 20/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 171.9246 - val_loss: 175.5342\n",
      "9/9 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44919, 35, 16) (44919, 1) (259, 35, 16)\n",
      "Epoch 1/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 861.8621 - val_loss: 234.3589\n",
      "Epoch 2/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 292.9422 - val_loss: 252.8376\n",
      "Epoch 3/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 255.3584 - val_loss: 187.7728\n",
      "Epoch 4/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 245.7108 - val_loss: 186.2925\n",
      "Epoch 5/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 233.0231 - val_loss: 192.4771\n",
      "Epoch 6/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 225.1458 - val_loss: 178.6168\n",
      "Epoch 7/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 216.9049 - val_loss: 178.9526\n",
      "Epoch 8/20\n",
      "1404/1404 [==============================] - 9s 7ms/step - loss: 210.5146 - val_loss: 178.4032\n",
      "Epoch 9/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 205.4955 - val_loss: 170.8001\n",
      "Epoch 10/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 201.1410 - val_loss: 192.5244\n",
      "Epoch 11/20\n",
      "1404/1404 [==============================] - 9s 7ms/step - loss: 196.4263 - val_loss: 171.8377\n",
      "Epoch 12/20\n",
      "1404/1404 [==============================] - 9s 7ms/step - loss: 192.9115 - val_loss: 168.3338\n",
      "Epoch 13/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 188.3173 - val_loss: 160.8899\n",
      "Epoch 14/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 186.7159 - val_loss: 185.3685\n",
      "Epoch 15/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 183.7425 - val_loss: 160.2543\n",
      "Epoch 16/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 181.6547 - val_loss: 165.5318\n",
      "Epoch 17/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 180.3977 - val_loss: 163.7735\n",
      "Epoch 18/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 177.9451 - val_loss: 163.1462\n",
      "Epoch 19/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 176.8180 - val_loss: 161.0755\n",
      "Epoch 20/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 174.9338 - val_loss: 163.6193\n",
      "9/9 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44919, 35, 16) (44919, 1) (259, 35, 16)\n",
      "Epoch 1/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 793.4174 - val_loss: 299.7874\n",
      "Epoch 2/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 270.9649 - val_loss: 195.3567\n",
      "Epoch 3/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 246.2209 - val_loss: 182.0868\n",
      "Epoch 4/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 233.2557 - val_loss: 223.7067\n",
      "Epoch 5/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 221.8022 - val_loss: 193.7404\n",
      "Epoch 6/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 213.4007 - val_loss: 174.4439\n",
      "Epoch 7/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 207.2265 - val_loss: 175.5321\n",
      "Epoch 8/20\n",
      "1404/1404 [==============================] - 9s 7ms/step - loss: 201.6379 - val_loss: 163.5542\n",
      "Epoch 9/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 195.2943 - val_loss: 177.1051\n",
      "Epoch 10/20\n",
      "1404/1404 [==============================] - 9s 7ms/step - loss: 194.4614 - val_loss: 164.3921\n",
      "Epoch 11/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 192.4545 - val_loss: 159.5734\n",
      "Epoch 12/20\n",
      "1404/1404 [==============================] - 9s 7ms/step - loss: 185.9835 - val_loss: 158.7585\n",
      "Epoch 13/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 184.8547 - val_loss: 176.9700\n",
      "Epoch 14/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 184.0186 - val_loss: 156.5438\n",
      "Epoch 15/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 181.9649 - val_loss: 162.6846\n",
      "Epoch 16/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 179.7185 - val_loss: 173.7984\n",
      "Epoch 17/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 177.7988 - val_loss: 176.2175\n",
      "Epoch 18/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 176.7011 - val_loss: 158.5614\n",
      "Epoch 19/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 174.6946 - val_loss: 162.7726\n",
      "Epoch 20/20\n",
      "1404/1404 [==============================] - 9s 7ms/step - loss: 173.1694 - val_loss: 166.1268\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "CPU times: user 15min 30s, sys: 48.3 s, total: 16min 18s\n",
      "Wall time: 15min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results_all003 = pd.DataFrame()\n",
    "for SEED in range(5):  \n",
    "    tf.random.set_seed(SEED)\n",
    "    mse = []\n",
    "    R2_val = []\n",
    "    RMSE = []\n",
    "    score_val = []\n",
    "    \n",
    "    # parameter's sample\n",
    "    # weights_file = \"weights_file_lstm_optimalmodel_all.h5\"\n",
    "    alpha = 0.1\n",
    "    sequence_length = 35\n",
    "    epochs = 20\n",
    "    nodes_per_layer = [64]\n",
    "    dropout = 0.2\n",
    "    activation = 'tanh'\n",
    "    batch_size = 32\n",
    "    remaining_sensors = remaining_sensors\n",
    "    # create model\n",
    "    input_shape = (sequence_length, len(remaining_sensors))\n",
    "    model = model_lstm_1layer(input_shape, nodes_per_layer[0], dropout,\n",
    "                             activation)\n",
    "    \n",
    "    # Data prepration\n",
    "    X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "\n",
    "    # create sequences train, test\n",
    "    train_array = gen_data_wrapper(X_train_interim, sequence_length,remaining_sensors)\n",
    "    label_array = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'])\n",
    "\n",
    "    test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,remaining_sensors, -99.))\n",
    "               for unit_nr in X_test_interim['Unit'].unique())\n",
    "    \n",
    "    test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "    test_rul = rul_piecewise_fct(y_test,rul_piecewise)\n",
    "    print(train_array.shape, label_array.shape, test_array.shape)\n",
    "            \n",
    "    # Model fitting\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        start_time = time.time()\n",
    "        model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n",
    "        weights_file = model.get_weights()\n",
    "        model.set_weights(weights_file)  # reset optimizer and node weights before every training iteration\n",
    "        history = model.fit(train_array, label_array,\n",
    "                                validation_data=(test_array, test_rul),\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                # callbacks=[cb],\n",
    "                                verbose=1)\n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "        mse.append(history.history['val_loss'][-1])\n",
    "\n",
    "        y_hat_val_split = model.predict(test_array)\n",
    "        R2_val.append(r2_score(test_rul, y_hat_val_split))\n",
    "        RMSE.append(np.sqrt(mean_squared_error(test_rul, y_hat_val_split)))\n",
    "        score_val.append(compute_s_score(test_rul, y_hat_val_split))\n",
    "            \n",
    "        \n",
    "    #  append results\n",
    "    d = {'RMSE' :np.mean(RMSE), 'std_RMSE' :np.std(RMSE),\n",
    "         'S_score' :np.mean(score_val), 'std_S_score' :np.std(score_val),\n",
    "         'MSE':np.mean(mse), 'std_MSE':np.std(mse),\n",
    "         'nodes':str(nodes_per_layer), 'dropout':dropout, \n",
    "         'activation':activation, 'batch_size':batch_size, 'TW' : sequence_length,\n",
    "         'time':training_time}\n",
    "\n",
    "#     results = results.append(pd.DataFrame(d, index=[0]), ignore_index=True)\n",
    "    results_all003 = pd.concat([results_all003, pd.DataFrame(d, index=[0])], ignore_index=True)\n",
    "    results_all003.to_csv('results/all/fd003.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>std_RMSE</th>\n",
       "      <th>S_score</th>\n",
       "      <th>std_S_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>TW</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.369151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>653.681430</td>\n",
       "      <td>0.0</td>\n",
       "      <td>152.995911</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "      <td>176.937613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.660530</td>\n",
       "      <td>0.0</td>\n",
       "      <td>708.243193</td>\n",
       "      <td>0.0</td>\n",
       "      <td>160.289001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "      <td>188.546215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.248932</td>\n",
       "      <td>0.0</td>\n",
       "      <td>828.569532</td>\n",
       "      <td>0.0</td>\n",
       "      <td>175.534210</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "      <td>183.073372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.791377</td>\n",
       "      <td>0.0</td>\n",
       "      <td>791.082270</td>\n",
       "      <td>0.0</td>\n",
       "      <td>163.619324</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "      <td>186.517013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.889018</td>\n",
       "      <td>0.0</td>\n",
       "      <td>637.156003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>166.126801</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "      <td>181.034672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RMSE  std_RMSE     S_score  std_S_score         MSE  std_MSE nodes  \\\n",
       "0  12.369151       0.0  653.681430          0.0  152.995911      0.0  [64]   \n",
       "1  12.660530       0.0  708.243193          0.0  160.289001      0.0  [64]   \n",
       "2  13.248932       0.0  828.569532          0.0  175.534210      0.0  [64]   \n",
       "3  12.791377       0.0  791.082270          0.0  163.619324      0.0  [64]   \n",
       "4  12.889018       0.0  637.156003          0.0  166.126801      0.0  [64]   \n",
       "\n",
       "   dropout activation  batch_size  TW        time  \n",
       "0      0.2       tanh          32  35  176.937613  \n",
       "1      0.2       tanh          32  35  188.546215  \n",
       "2      0.2       tanh          32  35  183.073372  \n",
       "3      0.2       tanh          32  35  186.517013  \n",
       "4      0.2       tanh          32  35  181.034672  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_all003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FD001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20631, 27) (13096, 26) (100, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# Load data and preprocess\n",
    "train, test, y_test = prepare_data('FD001.txt')\n",
    "print(train.shape, test.shape, y_test.shape)\n",
    "sensor_names = ['T20','T24','T30','T50','P20','P15','P30','Nf','Nc','epr','Ps30','phi',\n",
    "                    'NRf','NRc','BPR','farB','htBleed','Nf_dmd','PCNfR_dmd','W31','W32']\n",
    "remaining_sensors = ['T24','T30','T50', 'P15', 'P30','Nf','Nc','Ps30','phi',\n",
    "                    'NRf','NRc','BPR','htBleed','W31','W32']\n",
    "drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "\n",
    "rul_piecewise = 125\n",
    "train['RUL'].clip(upper=rul_piecewise, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17731, 30, 15) (17731, 1) (100, 30, 15)\n",
      "Epoch 1/20\n",
      "278/278 [==============================] - 6s 17ms/step - loss: 1945.8270 - val_loss: 1011.8328\n",
      "Epoch 2/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 557.1711 - val_loss: 283.6362\n",
      "Epoch 3/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 336.0125 - val_loss: 402.8049\n",
      "Epoch 4/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 256.9118 - val_loss: 234.6717\n",
      "Epoch 5/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 240.9247 - val_loss: 188.8959\n",
      "Epoch 6/20\n",
      "278/278 [==============================] - 5s 16ms/step - loss: 215.9529 - val_loss: 246.9015\n",
      "Epoch 7/20\n",
      "278/278 [==============================] - 5s 16ms/step - loss: 212.4774 - val_loss: 200.7240\n",
      "Epoch 8/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 196.8468 - val_loss: 287.7190\n",
      "Epoch 9/20\n",
      "278/278 [==============================] - 5s 16ms/step - loss: 198.1949 - val_loss: 179.0840\n",
      "Epoch 10/20\n",
      "278/278 [==============================] - 5s 16ms/step - loss: 189.8759 - val_loss: 183.7105\n",
      "Epoch 11/20\n",
      "278/278 [==============================] - 5s 16ms/step - loss: 178.0664 - val_loss: 206.1296\n",
      "Epoch 12/20\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 180.6395 - val_loss: 204.1548\n",
      "Epoch 13/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 176.2058 - val_loss: 185.8178\n",
      "Epoch 14/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 175.0867 - val_loss: 266.6446\n",
      "Epoch 15/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 181.0719 - val_loss: 195.0177\n",
      "Epoch 16/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 174.0650 - val_loss: 225.4183\n",
      "Epoch 17/20\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 167.9206 - val_loss: 170.4974\n",
      "Epoch 18/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 169.1513 - val_loss: 168.4302\n",
      "Epoch 19/20\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 168.1328 - val_loss: 181.4885\n",
      "Epoch 20/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 163.0819 - val_loss: 179.4211\n",
      "4/4 [==============================] - 0s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17731, 30, 15) (17731, 1) (100, 30, 15)\n",
      "Epoch 1/20\n",
      "278/278 [==============================] - 6s 17ms/step - loss: 1952.8248 - val_loss: 989.5231\n",
      "Epoch 2/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 608.6642 - val_loss: 457.7419\n",
      "Epoch 3/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 358.4405 - val_loss: 682.9811\n",
      "Epoch 4/20\n",
      "278/278 [==============================] - 5s 16ms/step - loss: 251.7239 - val_loss: 341.1137\n",
      "Epoch 5/20\n",
      "278/278 [==============================] - 5s 16ms/step - loss: 211.5727 - val_loss: 217.0099\n",
      "Epoch 6/20\n",
      "278/278 [==============================] - 5s 16ms/step - loss: 201.9030 - val_loss: 207.4494\n",
      "Epoch 7/20\n",
      "278/278 [==============================] - 7s 24ms/step - loss: 192.0596 - val_loss: 207.3164\n",
      "Epoch 8/20\n",
      "278/278 [==============================] - 5s 16ms/step - loss: 189.4069 - val_loss: 176.4110\n",
      "Epoch 9/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 183.2050 - val_loss: 192.1585\n",
      "Epoch 10/20\n",
      "278/278 [==============================] - 5s 16ms/step - loss: 170.9815 - val_loss: 273.9358\n",
      "Epoch 11/20\n",
      "278/278 [==============================] - 5s 16ms/step - loss: 172.2622 - val_loss: 175.6589\n",
      "Epoch 12/20\n",
      "278/278 [==============================] - 5s 16ms/step - loss: 176.8715 - val_loss: 169.4872\n",
      "Epoch 13/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 172.5498 - val_loss: 159.8728\n",
      "Epoch 14/20\n",
      "278/278 [==============================] - 5s 16ms/step - loss: 168.3288 - val_loss: 175.7836\n",
      "Epoch 15/20\n",
      "278/278 [==============================] - 5s 16ms/step - loss: 165.6964 - val_loss: 230.1221\n",
      "Epoch 16/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 168.5158 - val_loss: 279.9387\n",
      "Epoch 17/20\n",
      "278/278 [==============================] - 5s 16ms/step - loss: 166.2818 - val_loss: 208.7392\n",
      "Epoch 18/20\n",
      "278/278 [==============================] - 5s 16ms/step - loss: 165.4128 - val_loss: 167.0048\n",
      "Epoch 19/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 161.3411 - val_loss: 167.7612\n",
      "Epoch 20/20\n",
      "278/278 [==============================] - 5s 16ms/step - loss: 160.7052 - val_loss: 173.7776\n",
      "4/4 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17731, 30, 15) (17731, 1) (100, 30, 15)\n",
      "Epoch 1/20\n",
      "278/278 [==============================] - 5s 16ms/step - loss: 1969.4290 - val_loss: 1184.3573\n",
      "Epoch 2/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 569.7576 - val_loss: 422.7188\n",
      "Epoch 3/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 306.3215 - val_loss: 222.5002\n",
      "Epoch 4/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 235.5519 - val_loss: 214.6935\n",
      "Epoch 5/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 214.0097 - val_loss: 179.1317\n",
      "Epoch 6/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 201.6206 - val_loss: 170.3593\n",
      "Epoch 7/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 195.8453 - val_loss: 250.8557\n",
      "Epoch 8/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 191.1101 - val_loss: 202.9509\n",
      "Epoch 9/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 186.9187 - val_loss: 181.4738\n",
      "Epoch 10/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 186.7070 - val_loss: 211.4318\n",
      "Epoch 11/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 185.1064 - val_loss: 229.2648\n",
      "Epoch 12/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 184.7687 - val_loss: 178.2850\n",
      "Epoch 13/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 177.5807 - val_loss: 234.1747\n",
      "Epoch 14/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 181.5688 - val_loss: 163.8609\n",
      "Epoch 15/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 178.6478 - val_loss: 187.1144\n",
      "Epoch 16/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 170.8074 - val_loss: 174.3833\n",
      "Epoch 17/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 169.8305 - val_loss: 236.5552\n",
      "Epoch 18/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 174.3710 - val_loss: 168.9100\n",
      "Epoch 19/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 166.4170 - val_loss: 193.3184\n",
      "Epoch 20/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 163.4410 - val_loss: 180.6208\n",
      "4/4 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17731, 30, 15) (17731, 1) (100, 30, 15)\n",
      "Epoch 1/20\n",
      "278/278 [==============================] - 5s 16ms/step - loss: 1981.1896 - val_loss: 917.8284\n",
      "Epoch 2/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 589.6582 - val_loss: 387.9435\n",
      "Epoch 3/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 354.6008 - val_loss: 215.8253\n",
      "Epoch 4/20\n",
      "278/278 [==============================] - 5s 16ms/step - loss: 249.8626 - val_loss: 274.0669\n",
      "Epoch 5/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 218.3911 - val_loss: 303.3423\n",
      "Epoch 6/20\n",
      "278/278 [==============================] - 5s 16ms/step - loss: 212.5883 - val_loss: 251.4697\n",
      "Epoch 7/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 202.4792 - val_loss: 616.0234\n",
      "Epoch 8/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 196.4964 - val_loss: 172.3805\n",
      "Epoch 9/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 184.9444 - val_loss: 233.0498\n",
      "Epoch 10/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 184.0321 - val_loss: 215.6457\n",
      "Epoch 11/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 176.5638 - val_loss: 175.0033\n",
      "Epoch 12/20\n",
      "278/278 [==============================] - 5s 16ms/step - loss: 173.6998 - val_loss: 297.9556\n",
      "Epoch 13/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 170.6276 - val_loss: 284.2820\n",
      "Epoch 14/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 174.1343 - val_loss: 208.6004\n",
      "Epoch 15/20\n",
      "278/278 [==============================] - 5s 16ms/step - loss: 181.9589 - val_loss: 281.7798\n",
      "Epoch 16/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 167.0072 - val_loss: 177.9913\n",
      "Epoch 17/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 162.8726 - val_loss: 185.2022\n",
      "Epoch 18/20\n",
      "278/278 [==============================] - 5s 16ms/step - loss: 165.6644 - val_loss: 230.4792\n",
      "Epoch 19/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 163.3365 - val_loss: 170.9268\n",
      "Epoch 20/20\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 160.3434 - val_loss: 168.3098\n",
      "4/4 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17731, 30, 15) (17731, 1) (100, 30, 15)\n",
      "Epoch 1/20\n",
      "278/278 [==============================] - 5s 16ms/step - loss: 2036.7043 - val_loss: 1121.9376\n",
      "Epoch 2/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 604.8963 - val_loss: 474.9707\n",
      "Epoch 3/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 448.0006 - val_loss: 532.3290\n",
      "Epoch 4/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 276.0841 - val_loss: 333.5952\n",
      "Epoch 5/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 227.4386 - val_loss: 538.3504\n",
      "Epoch 6/20\n",
      "278/278 [==============================] - 5s 16ms/step - loss: 220.6311 - val_loss: 231.7316\n",
      "Epoch 7/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 198.7297 - val_loss: 169.9673\n",
      "Epoch 8/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 197.7237 - val_loss: 208.0873\n",
      "Epoch 9/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 189.4026 - val_loss: 173.8299\n",
      "Epoch 10/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 187.4878 - val_loss: 172.0346\n",
      "Epoch 11/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 185.7192 - val_loss: 267.2697\n",
      "Epoch 12/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 184.6438 - val_loss: 183.5157\n",
      "Epoch 13/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 180.3916 - val_loss: 245.7163\n",
      "Epoch 14/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 177.2476 - val_loss: 179.5717\n",
      "Epoch 15/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 174.1862 - val_loss: 173.5979\n",
      "Epoch 16/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 166.0173 - val_loss: 196.4699\n",
      "Epoch 17/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 171.0186 - val_loss: 204.4547\n",
      "Epoch 18/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 166.2104 - val_loss: 258.3325\n",
      "Epoch 19/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 169.3962 - val_loss: 173.0251\n",
      "Epoch 20/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 162.3917 - val_loss: 203.0011\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "CPU times: user 10min 33s, sys: 1min 55s, total: 12min 28s\n",
      "Wall time: 7min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results_all001 = pd.DataFrame()\n",
    "for SEED in range(5):  \n",
    "    set_seed(SEED)\n",
    "    mse = []\n",
    "    R2_val = []\n",
    "    RMSE = []\n",
    "    score_val = []\n",
    "    \n",
    "    # parameter's sample\n",
    "    # weights_file = \"weights_file_lstm_optimalmodel_all.h5\"\n",
    "    alpha = 0.1\n",
    "    sequence_length = 30\n",
    "    epochs = 20\n",
    "    nodes_per_layer = [128]\n",
    "    dropout = 0.2\n",
    "    activation = 'tanh'\n",
    "    batch_size = 64\n",
    "    remaining_sensors = remaining_sensors\n",
    "    # create model\n",
    "    input_shape = (sequence_length, len(remaining_sensors))\n",
    "    model = model_lstm_1layer(input_shape, nodes_per_layer[0], dropout,\n",
    "                             activation)\n",
    "    \n",
    "    # Data prepration\n",
    "    X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "\n",
    "    # create sequences train, test\n",
    "    train_array = gen_data_wrapper(X_train_interim, sequence_length,remaining_sensors)\n",
    "    label_array = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'])\n",
    "\n",
    "    test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,remaining_sensors, -99.))\n",
    "               for unit_nr in X_test_interim['Unit'].unique())\n",
    "    \n",
    "    test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "    test_rul = rul_piecewise_fct(y_test,rul_piecewise)\n",
    "    print(train_array.shape, label_array.shape, test_array.shape)\n",
    "            \n",
    "    # Model fitting\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        start_time = time.time()\n",
    "        model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n",
    "        weights_file = model.get_weights()\n",
    "        model.set_weights(weights_file)  # reset optimizer and node weights before every training iteration\n",
    "        history = model.fit(train_array, label_array,\n",
    "                                validation_data=(test_array, test_rul),\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                # callbacks=[cb],\n",
    "                                verbose=1)\n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "        mse.append(history.history['val_loss'][-1])\n",
    "\n",
    "        y_hat_val_split = model.predict(test_array)\n",
    "        R2_val.append(r2_score(test_rul, y_hat_val_split))\n",
    "        RMSE.append(np.sqrt(mean_squared_error(test_rul, y_hat_val_split)))\n",
    "        score_val.append(compute_s_score(test_rul, y_hat_val_split))\n",
    "            \n",
    "        \n",
    "    #  append results\n",
    "    d = {'RMSE' :np.mean(RMSE), 'std_RMSE' :np.std(RMSE),\n",
    "         'S_score' :np.mean(score_val), 'std_S_score' :np.std(score_val),\n",
    "         'MSE':np.mean(mse), 'std_MSE':np.std(mse),\n",
    "         'nodes':str(nodes_per_layer), 'dropout':dropout, \n",
    "         'activation':activation, 'batch_size':batch_size, 'TW' : sequence_length,\n",
    "         'time':training_time}\n",
    "\n",
    "#     results = results.append(pd.DataFrame(d, index=[0]), ignore_index=True)\n",
    "    results_all001 = pd.concat([results_all001, pd.DataFrame(d, index=[0])], ignore_index=True)\n",
    "    results_all001.to_csv('results/all/fd001.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
