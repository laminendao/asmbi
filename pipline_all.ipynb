{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\ProgramData\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "from lime.lime_tabular import RecurrentTabularExplainer\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error, r2_score \n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn import preprocessing\n",
    "from keras import backend as K\n",
    "from sklearn.preprocessing import MinMaxScaler , StandardScaler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Activation, GRU\n",
    "from scipy import optimize\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "from sp_modif.model_function import *\n",
    "from sp_modif.methods import *\n",
    "from sp_modif.data_prep import *\n",
    "from sp_modif.evaluator import *\n",
    "from sp_modif.SHAP import *\n",
    "from sp_modif.L2X import *\n",
    "from methods import *\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEED = 0\n",
    "def set_seed(seed=SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    tf.random.set_seed(SEED)\n",
    "\n",
    "# Appeler la fonction pour fixer le seed\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61249, 27) (41214, 26) (248, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# Load data and preprocess\n",
    "train, test, y_test = prepare_data('FD004.txt')\n",
    "print(train.shape, test.shape, y_test.shape)\n",
    "sensor_names = ['T20','T24','T30','T50','P20','P15','P30','Nf','Nc','epr','Ps30','phi',\n",
    "                    'NRf','NRc','BPR','farB','htBleed','Nf_dmd','PCNfR_dmd','W31','W32']\n",
    "\n",
    "remaining_sensors = ['T24','T30','T50', 'P15', 'P30','Nf','Nc', 'epr','Ps30','phi',\n",
    "                    'NRf','NRc','BPR', 'farB','htBleed','W31','W32']\n",
    "drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "\n",
    "rul_piecewise = 120\n",
    "train['RUL'].clip(upper=rul_piecewise, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_lstm_1layer(input_shape, nodes_per_layer, dropout, activation):\n",
    "    \n",
    "    cb = keras.callbacks.EarlyStopping(monitor='loss', patience=4)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units = nodes_per_layer, activation=activation, \n",
    "                  input_shape=input_shape))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(256))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer=Adam(learning_rate=0.001))\n",
    "    # model.save_weights(weights_file)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\ProgramData\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "(51538, 40, 17) (51538, 1) (248, 40, 17)\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:From c:\\ProgramData\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "1611/1611 [==============================] - 20s 11ms/step - loss: 769.6389 - val_loss: 471.7919\n",
      "Epoch 2/20\n",
      "1611/1611 [==============================] - 18s 11ms/step - loss: 297.4277 - val_loss: 250.2302\n",
      "Epoch 3/20\n",
      "1611/1611 [==============================] - 18s 11ms/step - loss: 249.4327 - val_loss: 221.5461\n",
      "Epoch 4/20\n",
      "1611/1611 [==============================] - 19s 12ms/step - loss: 231.5601 - val_loss: 217.8264\n",
      "Epoch 5/20\n",
      "1611/1611 [==============================] - 21s 13ms/step - loss: 215.5899 - val_loss: 192.3274\n",
      "Epoch 6/20\n",
      "1611/1611 [==============================] - 23s 14ms/step - loss: 200.9751 - val_loss: 201.6725\n",
      "Epoch 7/20\n",
      "1611/1611 [==============================] - 21s 13ms/step - loss: 189.2692 - val_loss: 184.5225\n",
      "Epoch 8/20\n",
      "1611/1611 [==============================] - 22s 13ms/step - loss: 183.2186 - val_loss: 180.1667\n",
      "Epoch 9/20\n",
      "1611/1611 [==============================] - 23s 14ms/step - loss: 176.4012 - val_loss: 172.0417\n",
      "Epoch 10/20\n",
      "1611/1611 [==============================] - 19s 12ms/step - loss: 173.3885 - val_loss: 179.2094\n",
      "Epoch 11/20\n",
      "1611/1611 [==============================] - 17s 11ms/step - loss: 168.1069 - val_loss: 159.3325\n",
      "Epoch 12/20\n",
      "1611/1611 [==============================] - 18s 11ms/step - loss: 167.2028 - val_loss: 178.6848\n",
      "Epoch 13/20\n",
      "1611/1611 [==============================] - 18s 11ms/step - loss: 162.8031 - val_loss: 164.7776\n",
      "Epoch 14/20\n",
      "1611/1611 [==============================] - 19s 12ms/step - loss: 160.7663 - val_loss: 152.5817\n",
      "Epoch 15/20\n",
      "1611/1611 [==============================] - 19s 12ms/step - loss: 159.4288 - val_loss: 152.9747\n",
      "Epoch 16/20\n",
      "1611/1611 [==============================] - 19s 12ms/step - loss: 157.3829 - val_loss: 163.3507\n",
      "Epoch 17/20\n",
      "1611/1611 [==============================] - 20s 12ms/step - loss: 157.3238 - val_loss: 151.1780\n",
      "Epoch 18/20\n",
      "1611/1611 [==============================] - 21s 13ms/step - loss: 154.1897 - val_loss: 154.7153\n",
      "Epoch 19/20\n",
      "1611/1611 [==============================] - 22s 14ms/step - loss: 153.1814 - val_loss: 168.9713\n",
      "Epoch 20/20\n",
      "1611/1611 [==============================] - 23s 14ms/step - loss: 152.3367 - val_loss: 153.5847\n",
      "8/8 [==============================] - 1s 5ms/step\n",
      "(51538, 40, 17) (51538, 1) (248, 40, 17)\n",
      "Epoch 1/20\n",
      "1611/1611 [==============================] - 21s 12ms/step - loss: 759.6580 - val_loss: 333.3815\n",
      "Epoch 2/20\n",
      "1611/1611 [==============================] - 19s 12ms/step - loss: 294.3893 - val_loss: 252.4610\n",
      "Epoch 3/20\n",
      "1611/1611 [==============================] - 19s 12ms/step - loss: 258.8456 - val_loss: 269.0082\n",
      "Epoch 4/20\n",
      "1611/1611 [==============================] - 21s 13ms/step - loss: 237.9181 - val_loss: 212.2075\n",
      "Epoch 5/20\n",
      "1611/1611 [==============================] - 23s 14ms/step - loss: 223.5802 - val_loss: 214.9901\n",
      "Epoch 6/20\n",
      "1611/1611 [==============================] - 17s 11ms/step - loss: 209.7936 - val_loss: 205.9682\n",
      "Epoch 7/20\n",
      "1611/1611 [==============================] - 17s 11ms/step - loss: 197.3411 - val_loss: 184.5761\n",
      "Epoch 8/20\n",
      "1611/1611 [==============================] - 18s 11ms/step - loss: 184.9835 - val_loss: 183.4782\n",
      "Epoch 9/20\n",
      "1611/1611 [==============================] - 18s 11ms/step - loss: 178.5824 - val_loss: 173.7251\n",
      "Epoch 10/20\n",
      "1611/1611 [==============================] - 19s 12ms/step - loss: 172.5952 - val_loss: 162.7937\n",
      "Epoch 11/20\n",
      "1611/1611 [==============================] - 20s 12ms/step - loss: 169.7482 - val_loss: 166.4969\n",
      "Epoch 12/20\n",
      "1611/1611 [==============================] - 20s 13ms/step - loss: 165.8584 - val_loss: 156.2384\n",
      "Epoch 13/20\n",
      "1611/1611 [==============================] - 21s 13ms/step - loss: 164.1008 - val_loss: 168.3299\n",
      "Epoch 14/20\n",
      "1611/1611 [==============================] - 22s 14ms/step - loss: 162.9372 - val_loss: 163.6435\n",
      "Epoch 15/20\n",
      "1611/1611 [==============================] - 19s 12ms/step - loss: 160.8251 - val_loss: 154.6202\n",
      "Epoch 16/20\n",
      "1611/1611 [==============================] - 18s 11ms/step - loss: 158.6073 - val_loss: 152.7587\n",
      "Epoch 17/20\n",
      "1611/1611 [==============================] - 18s 11ms/step - loss: 156.3803 - val_loss: 157.9123\n",
      "Epoch 18/20\n",
      "1611/1611 [==============================] - 19s 12ms/step - loss: 155.7692 - val_loss: 167.5309\n",
      "Epoch 19/20\n",
      "1611/1611 [==============================] - 20s 12ms/step - loss: 155.4921 - val_loss: 162.8506\n",
      "Epoch 20/20\n",
      "1611/1611 [==============================] - 20s 12ms/step - loss: 152.7651 - val_loss: 153.0002\n",
      "8/8 [==============================] - 1s 6ms/step\n",
      "(51538, 40, 17) (51538, 1) (248, 40, 17)\n",
      "Epoch 1/20\n",
      "1611/1611 [==============================] - 24s 13ms/step - loss: 753.2983 - val_loss: 344.2400\n",
      "Epoch 2/20\n",
      "1611/1611 [==============================] - 23s 14ms/step - loss: 291.9753 - val_loss: 277.0363\n",
      "Epoch 3/20\n",
      "1611/1611 [==============================] - 19s 12ms/step - loss: 256.8614 - val_loss: 255.6139\n",
      "Epoch 4/20\n",
      "1611/1611 [==============================] - 18s 11ms/step - loss: 244.0661 - val_loss: 246.0829\n",
      "Epoch 5/20\n",
      "1611/1611 [==============================] - 19s 12ms/step - loss: 225.5040 - val_loss: 214.5333\n",
      "Epoch 6/20\n",
      "1611/1611 [==============================] - 19s 12ms/step - loss: 214.7024 - val_loss: 195.2882\n",
      "Epoch 7/20\n",
      "1611/1611 [==============================] - 23s 14ms/step - loss: 203.3391 - val_loss: 197.2109\n",
      "Epoch 8/20\n",
      "1611/1611 [==============================] - 21s 13ms/step - loss: 193.2973 - val_loss: 178.3009\n",
      "Epoch 9/20\n",
      "1611/1611 [==============================] - 22s 14ms/step - loss: 185.5094 - val_loss: 194.2675\n",
      "Epoch 10/20\n",
      "1611/1611 [==============================] - 21s 13ms/step - loss: 179.6812 - val_loss: 175.8315\n",
      "Epoch 11/20\n",
      "1611/1611 [==============================] - 18s 11ms/step - loss: 174.9429 - val_loss: 165.9721\n",
      "Epoch 12/20\n",
      "1611/1611 [==============================] - 18s 11ms/step - loss: 172.3465 - val_loss: 171.9647\n",
      "Epoch 13/20\n",
      "1611/1611 [==============================] - 19s 12ms/step - loss: 168.3879 - val_loss: 165.0414\n",
      "Epoch 14/20\n",
      "1611/1611 [==============================] - 20s 12ms/step - loss: 165.6106 - val_loss: 176.0186\n",
      "Epoch 15/20\n",
      "1611/1611 [==============================] - 21s 13ms/step - loss: 162.4550 - val_loss: 171.5215\n",
      "Epoch 16/20\n",
      "1611/1611 [==============================] - 22s 13ms/step - loss: 162.0993 - val_loss: 161.8842\n",
      "Epoch 17/20\n",
      "1611/1611 [==============================] - 22s 14ms/step - loss: 159.4152 - val_loss: 170.6503\n",
      "Epoch 18/20\n",
      "1611/1611 [==============================] - 21s 13ms/step - loss: 156.7803 - val_loss: 161.0606\n",
      "Epoch 19/20\n",
      "1611/1611 [==============================] - 18s 11ms/step - loss: 155.5693 - val_loss: 164.0349\n",
      "Epoch 20/20\n",
      "1611/1611 [==============================] - 19s 12ms/step - loss: 154.9985 - val_loss: 175.2650\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "(51538, 40, 17) (51538, 1) (248, 40, 17)\n",
      "Epoch 1/20\n",
      "1611/1611 [==============================] - 22s 13ms/step - loss: 816.7593 - val_loss: 339.0998\n",
      "Epoch 2/20\n",
      "1611/1611 [==============================] - 21s 13ms/step - loss: 295.1545 - val_loss: 243.2053\n",
      "Epoch 3/20\n",
      "1611/1611 [==============================] - 21s 13ms/step - loss: 255.7472 - val_loss: 246.5036\n",
      "Epoch 4/20\n",
      "1611/1611 [==============================] - 18s 11ms/step - loss: 235.8436 - val_loss: 240.3878\n",
      "Epoch 5/20\n",
      "1611/1611 [==============================] - 18s 11ms/step - loss: 223.6124 - val_loss: 243.1898\n",
      "Epoch 6/20\n",
      "1611/1611 [==============================] - 18s 11ms/step - loss: 206.5595 - val_loss: 200.2330\n",
      "Epoch 7/20\n",
      "1611/1611 [==============================] - 19s 12ms/step - loss: 196.2444 - val_loss: 193.6039\n",
      "Epoch 8/20\n",
      "1611/1611 [==============================] - 19s 12ms/step - loss: 186.6825 - val_loss: 241.8715\n",
      "Epoch 9/20\n",
      "1611/1611 [==============================] - 18s 11ms/step - loss: 179.1059 - val_loss: 181.8121\n",
      "Epoch 10/20\n",
      "1611/1611 [==============================] - 16s 10ms/step - loss: 174.1342 - val_loss: 195.5946\n",
      "Epoch 11/20\n",
      "1611/1611 [==============================] - 16s 10ms/step - loss: 170.4225 - val_loss: 226.1171\n",
      "Epoch 12/20\n",
      "1611/1611 [==============================] - 16s 10ms/step - loss: 165.8000 - val_loss: 188.5979\n",
      "Epoch 13/20\n",
      "1611/1611 [==============================] - 15s 9ms/step - loss: 162.3353 - val_loss: 170.4197\n",
      "Epoch 14/20\n",
      "1611/1611 [==============================] - 15s 9ms/step - loss: 160.7547 - val_loss: 167.1472\n",
      "Epoch 15/20\n",
      "1611/1611 [==============================] - 14s 9ms/step - loss: 159.4251 - val_loss: 156.2389\n",
      "Epoch 16/20\n",
      "1611/1611 [==============================] - 14s 9ms/step - loss: 156.3632 - val_loss: 168.0318\n",
      "Epoch 17/20\n",
      "1611/1611 [==============================] - 15s 9ms/step - loss: 154.9506 - val_loss: 161.4645\n",
      "Epoch 18/20\n",
      "1611/1611 [==============================] - 15s 9ms/step - loss: 153.5817 - val_loss: 161.5802\n",
      "Epoch 19/20\n",
      "1611/1611 [==============================] - 16s 10ms/step - loss: 151.9791 - val_loss: 155.4162\n",
      "Epoch 20/20\n",
      "1611/1611 [==============================] - 16s 10ms/step - loss: 151.0574 - val_loss: 173.5636\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "(51538, 40, 17) (51538, 1) (248, 40, 17)\n",
      "Epoch 1/20\n",
      "1611/1611 [==============================] - 19s 10ms/step - loss: 866.0872 - val_loss: 384.5283\n",
      "Epoch 2/20\n",
      "1611/1611 [==============================] - 16s 10ms/step - loss: 308.9417 - val_loss: 265.4250\n",
      "Epoch 3/20\n",
      "1611/1611 [==============================] - 16s 10ms/step - loss: 251.2590 - val_loss: 234.4135\n",
      "Epoch 4/20\n",
      "1611/1611 [==============================] - 16s 10ms/step - loss: 231.9413 - val_loss: 269.8399\n",
      "Epoch 5/20\n",
      "1611/1611 [==============================] - 17s 11ms/step - loss: 212.1810 - val_loss: 207.4045\n",
      "Epoch 6/20\n",
      "1611/1611 [==============================] - 16s 10ms/step - loss: 198.9570 - val_loss: 240.4154\n",
      "Epoch 7/20\n",
      "1611/1611 [==============================] - 16s 10ms/step - loss: 189.7676 - val_loss: 204.0620\n",
      "Epoch 8/20\n",
      "1611/1611 [==============================] - 17s 11ms/step - loss: 182.4231 - val_loss: 213.6190\n",
      "Epoch 9/20\n",
      "1611/1611 [==============================] - 17s 10ms/step - loss: 177.0965 - val_loss: 208.7494\n",
      "Epoch 10/20\n",
      "1611/1611 [==============================] - 16s 10ms/step - loss: 171.2697 - val_loss: 179.7131\n",
      "Epoch 11/20\n",
      "1611/1611 [==============================] - 17s 11ms/step - loss: 167.7757 - val_loss: 196.1525\n",
      "Epoch 12/20\n",
      "1611/1611 [==============================] - 16s 10ms/step - loss: 163.6719 - val_loss: 189.4343\n",
      "Epoch 13/20\n",
      "1611/1611 [==============================] - 16s 10ms/step - loss: 160.9909 - val_loss: 194.2283\n",
      "Epoch 14/20\n",
      "1611/1611 [==============================] - 16s 10ms/step - loss: 160.0361 - val_loss: 179.9895\n",
      "8/8 [==============================] - 0s 5ms/step\n",
      "CPU times: total: 1h 25min 19s\n",
      "Wall time: 29min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results_all = pd.DataFrame()\n",
    "for SEED in range(5):  \n",
    "    tf.random.set_seed(SEED)\n",
    "    mse = []\n",
    "    R2_val = []\n",
    "    RMSE = []\n",
    "    score_val = []\n",
    "    \n",
    "    # 0.20\t[64]\t0.3\ttanh\t32\t25\n",
    "    \n",
    "    # parameter's sample\n",
    "    # weights_file = \"weights_file_lstm_optimalmodel_all.h5\"\n",
    "    alpha = 0.3\n",
    "    sequence_length = 40\n",
    "    epochs = 20\n",
    "    nodes_per_layer = [64]\n",
    "    dropout = 0.2\n",
    "    activation = 'tanh'\n",
    "    batch_size = 32\n",
    "    remaining_sensors = remaining_sensors\n",
    "    # create model\n",
    "    input_shape = (sequence_length, len(remaining_sensors))\n",
    "    model = model_lstm_1layer(input_shape, nodes_per_layer[0], dropout, activation)\n",
    "    \n",
    "    # Data prepration\n",
    "    X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "\n",
    "    # create sequences train, test\n",
    "    train_array = gen_data_wrapper(X_train_interim, sequence_length,remaining_sensors)\n",
    "    label_array = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'])\n",
    "\n",
    "    test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,remaining_sensors, -99.))\n",
    "               for unit_nr in X_test_interim['Unit'].unique())\n",
    "    \n",
    "    test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "    test_rul = rul_piecewise_fct(y_test,rul_piecewise)\n",
    "    print(train_array.shape, label_array.shape, test_array.shape)\n",
    "            \n",
    "    # Model fitting\n",
    "    cb = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        start_time = time.time()\n",
    "        model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n",
    "        weights_file = model.get_weights()\n",
    "        model.set_weights(weights_file)  # reset optimizer and node weights before every training iteration\n",
    "        history = model.fit(train_array, label_array,\n",
    "                                validation_data=(test_array, test_rul),\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                callbacks=[cb],\n",
    "                                verbose=1)\n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "        mse.append(history.history['val_loss'][-1])\n",
    "\n",
    "        y_hat_val_split = model.predict(test_array)\n",
    "        R2_val.append(r2_score(test_rul, y_hat_val_split))\n",
    "        RMSE.append(np.sqrt(mean_squared_error(test_rul, y_hat_val_split)))\n",
    "        score_val.append(compute_s_score(test_rul, y_hat_val_split))\n",
    "            \n",
    "        \n",
    "    #  append results\n",
    "    d = {'RMSE' :np.mean(RMSE), 'std_RMSE' :np.std(RMSE),\n",
    "         'S_score' :np.mean(score_val), 'std_S_score' :np.std(score_val),\n",
    "         'MSE':np.mean(mse), 'std_MSE':np.std(mse),\n",
    "         'nodes':str(nodes_per_layer), 'dropout':dropout, \n",
    "         'activation':activation, 'batch_size':batch_size, 'TW' : sequence_length,\n",
    "         'time':training_time}\n",
    "\n",
    "#     results = results.append(pd.DataFrame(d, index=[0]), ignore_index=True)\n",
    "    results_all = pd.concat([results_all, pd.DataFrame(d, index=[0])], ignore_index=True)\n",
    "    results_all.to_csv('results/all/fd004.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>std_RMSE</th>\n",
       "      <th>S_score</th>\n",
       "      <th>std_S_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>TW</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.392929</td>\n",
       "      <td>0.0</td>\n",
       "      <td>687.639444</td>\n",
       "      <td>0.0</td>\n",
       "      <td>153.584702</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>398.625060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.359558</td>\n",
       "      <td>0.0</td>\n",
       "      <td>657.331306</td>\n",
       "      <td>0.0</td>\n",
       "      <td>153.000153</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>387.112779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.238767</td>\n",
       "      <td>0.0</td>\n",
       "      <td>875.264684</td>\n",
       "      <td>0.0</td>\n",
       "      <td>175.264954</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>409.568153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.174353</td>\n",
       "      <td>0.0</td>\n",
       "      <td>621.174271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>173.563568</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>340.484180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.405710</td>\n",
       "      <td>0.0</td>\n",
       "      <td>775.865868</td>\n",
       "      <td>0.0</td>\n",
       "      <td>179.989532</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>233.254431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RMSE  std_RMSE     S_score  std_S_score         MSE  std_MSE nodes  \\\n",
       "0  12.392929       0.0  687.639444          0.0  153.584702      0.0  [64]   \n",
       "1  12.359558       0.0  657.331306          0.0  153.000153      0.0  [64]   \n",
       "2  13.238767       0.0  875.264684          0.0  175.264954      0.0  [64]   \n",
       "3  13.174353       0.0  621.174271          0.0  173.563568      0.0  [64]   \n",
       "4  13.405710       0.0  775.865868          0.0  179.989532      0.0  [64]   \n",
       "\n",
       "   dropout activation  batch_size  TW        time  \n",
       "0      0.2       tanh          32  40  398.625060  \n",
       "1      0.2       tanh          32  40  387.112779  \n",
       "2      0.2       tanh          32  40  409.568153  \n",
       "3      0.2       tanh          32  40  340.484180  \n",
       "4      0.2       tanh          32  40  233.254431  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FD002 <a class=\"anchor\" id=\"fd002\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53759, 27) (33991, 26) (259, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# Load data and preprocess\n",
    "train, test, y_test = prepare_data('FD002.txt')\n",
    "print(train.shape, test.shape, y_test.shape)\n",
    "sensor_names = ['T20','T24','T30','T50','P20','P15','P30','Nf','Nc','epr','Ps30','phi',\n",
    "                    'NRf','NRc','BPR','farB','htBleed','Nf_dmd','PCNfR_dmd','W31','W32']\n",
    "\n",
    "remaining_sensors = ['T24','T30','T50', 'P15', 'P30','Nf','Nc', 'epr','Ps30','phi',\n",
    "                    'NRf','NRc','BPR','htBleed','W31','W32']\n",
    "drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "\n",
    "rul_piecewise = 125\n",
    "train['RUL'].clip(upper=rul_piecewise, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43619, 40, 16) (43619, 1) (259, 40, 16)\n",
      "Epoch 1/20\n",
      "341/341 [==============================] - 6s 13ms/step - loss: 2284.7844 - val_loss: 1184.4271\n",
      "Epoch 2/20\n",
      "341/341 [==============================] - 4s 11ms/step - loss: 489.2794 - val_loss: 232.4056\n",
      "Epoch 3/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 277.6627 - val_loss: 208.3045\n",
      "Epoch 4/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 252.8305 - val_loss: 209.8648\n",
      "Epoch 5/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 239.3363 - val_loss: 197.7292\n",
      "Epoch 6/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 235.1313 - val_loss: 200.9615\n",
      "Epoch 7/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 233.2259 - val_loss: 186.3433\n",
      "Epoch 8/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 230.1735 - val_loss: 191.8853\n",
      "Epoch 9/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 224.1112 - val_loss: 196.1663\n",
      "Epoch 10/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 224.0523 - val_loss: 188.4505\n",
      "Epoch 11/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 219.6696 - val_loss: 192.2417\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "(43619, 40, 16) (43619, 1) (259, 40, 16)\n",
      "Epoch 1/20\n",
      "341/341 [==============================] - 5s 11ms/step - loss: 2342.7778 - val_loss: 1485.9211\n",
      "Epoch 2/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 501.3220 - val_loss: 232.0872\n",
      "Epoch 3/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 284.5900 - val_loss: 210.1915\n",
      "Epoch 4/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 268.0760 - val_loss: 190.0890\n",
      "Epoch 5/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 245.6234 - val_loss: 201.6874\n",
      "Epoch 6/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 242.2218 - val_loss: 184.8362\n",
      "Epoch 7/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 235.2963 - val_loss: 181.2952\n",
      "Epoch 8/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 233.1501 - val_loss: 184.5910\n",
      "Epoch 9/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 228.3179 - val_loss: 183.7630\n",
      "Epoch 10/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 224.9238 - val_loss: 179.9642\n",
      "Epoch 11/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 224.0339 - val_loss: 189.4969\n",
      "Epoch 12/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 219.2482 - val_loss: 171.0726\n",
      "Epoch 13/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 217.3687 - val_loss: 177.9349\n",
      "Epoch 14/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 214.9243 - val_loss: 171.8634\n",
      "Epoch 15/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 212.3314 - val_loss: 170.9157\n",
      "Epoch 16/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 211.1714 - val_loss: 172.4135\n",
      "Epoch 17/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 209.3853 - val_loss: 174.2600\n",
      "Epoch 18/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 207.7493 - val_loss: 181.6360\n",
      "Epoch 19/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 203.8758 - val_loss: 171.4469\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "(43619, 40, 16) (43619, 1) (259, 40, 16)\n",
      "Epoch 1/20\n",
      "341/341 [==============================] - 6s 12ms/step - loss: 2312.6143 - val_loss: 1375.4160\n",
      "Epoch 2/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 517.5961 - val_loss: 241.0746\n",
      "Epoch 3/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 293.8042 - val_loss: 206.6071\n",
      "Epoch 4/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 264.7714 - val_loss: 202.7016\n",
      "Epoch 5/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 256.7435 - val_loss: 196.3409\n",
      "Epoch 6/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 245.3402 - val_loss: 191.3584\n",
      "Epoch 7/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 241.1003 - val_loss: 199.1156\n",
      "Epoch 8/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 235.3669 - val_loss: 200.3518\n",
      "Epoch 9/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 230.2449 - val_loss: 190.0343\n",
      "Epoch 10/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 231.6243 - val_loss: 206.2997\n",
      "Epoch 11/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 227.3732 - val_loss: 199.3625\n",
      "Epoch 12/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 228.5360 - val_loss: 198.8083\n",
      "Epoch 13/20\n",
      "341/341 [==============================] - 4s 10ms/step - loss: 219.6117 - val_loss: 202.7586\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "(43619, 40, 16) (43619, 1) (259, 40, 16)\n",
      "Epoch 1/20\n",
      "341/341 [==============================] - 5s 10ms/step - loss: 2347.4253 - val_loss: 1551.2679\n",
      "Epoch 2/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 551.0562 - val_loss: 245.4686\n",
      "Epoch 3/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 275.4075 - val_loss: 195.8344\n",
      "Epoch 4/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 253.0522 - val_loss: 223.9425\n",
      "Epoch 5/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 243.1503 - val_loss: 183.3989\n",
      "Epoch 6/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 232.0489 - val_loss: 181.8309\n",
      "Epoch 7/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 228.6903 - val_loss: 182.9480\n",
      "Epoch 8/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 228.1269 - val_loss: 176.8555\n",
      "Epoch 9/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 224.3699 - val_loss: 185.7150\n",
      "Epoch 10/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 221.6986 - val_loss: 177.5479\n",
      "Epoch 11/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 219.7643 - val_loss: 198.0124\n",
      "Epoch 12/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 218.9006 - val_loss: 183.4602\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "(43619, 40, 16) (43619, 1) (259, 40, 16)\n",
      "Epoch 1/20\n",
      "341/341 [==============================] - 5s 10ms/step - loss: 2275.6184 - val_loss: 1325.1212\n",
      "Epoch 2/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 536.0815 - val_loss: 243.3216\n",
      "Epoch 3/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 279.8815 - val_loss: 202.8547\n",
      "Epoch 4/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 252.0923 - val_loss: 207.0118\n",
      "Epoch 5/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 242.0057 - val_loss: 183.7374\n",
      "Epoch 6/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 231.4697 - val_loss: 178.4366\n",
      "Epoch 7/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 228.1922 - val_loss: 203.3118\n",
      "Epoch 8/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 226.2511 - val_loss: 173.8006\n",
      "Epoch 9/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 221.3582 - val_loss: 199.5806\n",
      "Epoch 10/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 218.5870 - val_loss: 174.7793\n",
      "Epoch 11/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 217.7645 - val_loss: 172.2044\n",
      "Epoch 12/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 215.8717 - val_loss: 174.4931\n",
      "Epoch 13/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 211.3976 - val_loss: 190.6755\n",
      "Epoch 14/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 211.8429 - val_loss: 205.0641\n",
      "Epoch 15/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 209.3679 - val_loss: 208.0574\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "CPU times: total: 11min 39s\n",
      "Wall time: 4min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results_all002 = pd.DataFrame()\n",
    "for SEED in range(5):  \n",
    "    tf.random.set_seed(SEED)\n",
    "    mse = []\n",
    "    R2_val = []\n",
    "    RMSE = []\n",
    "    score_val = []\n",
    "    \n",
    "    # parameter's sample\n",
    "    # weights_file = \"weights_file_lstm_optimalmodel_all.h5\"\n",
    "    alpha = 0.2\n",
    "    sequence_length = 40\n",
    "    epochs = 20\n",
    "    nodes_per_layer = [32]\n",
    "    dropout = 0.1\n",
    "    activation = 'tanh'\n",
    "    batch_size = 128\n",
    "    remaining_sensors = remaining_sensors\n",
    "    # create model\n",
    "    input_shape = (sequence_length, len(remaining_sensors))\n",
    "    model = model_lstm_1layer(input_shape, nodes_per_layer[0], dropout, activation)\n",
    "    \n",
    "    # Data prepration\n",
    "    X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "\n",
    "    # create sequences train, test\n",
    "    train_array = gen_data_wrapper(X_train_interim, sequence_length,remaining_sensors)\n",
    "    label_array = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'])\n",
    "\n",
    "    test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,remaining_sensors, -99.))\n",
    "               for unit_nr in X_test_interim['Unit'].unique())\n",
    "    \n",
    "    test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "    test_rul = rul_piecewise_fct(y_test,rul_piecewise)\n",
    "    print(train_array.shape, label_array.shape, test_array.shape)\n",
    "            \n",
    "    # Model fitting\n",
    "    cb = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        start_time = time.time()\n",
    "        model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n",
    "        weights_file = model.get_weights()\n",
    "        model.set_weights(weights_file)  # reset optimizer and node weights before every training iteration\n",
    "        history = model.fit(train_array, label_array,\n",
    "                                validation_data=(test_array, test_rul),\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                callbacks=[cb],\n",
    "                                verbose=1)\n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "        mse.append(history.history['val_loss'][-1])\n",
    "\n",
    "        y_hat_val_split = model.predict(test_array)\n",
    "        R2_val.append(r2_score(test_rul, y_hat_val_split))\n",
    "        RMSE.append(np.sqrt(mean_squared_error(test_rul, y_hat_val_split)))\n",
    "        score_val.append(compute_s_score(test_rul, y_hat_val_split))\n",
    "            \n",
    "        \n",
    "    #  append results\n",
    "    d = {'RMSE' :np.mean(RMSE), 'std_RMSE' :np.std(RMSE),\n",
    "         'S_score' :np.mean(score_val), 'std_S_score' :np.std(score_val),\n",
    "         'MSE':np.mean(mse), 'std_MSE':np.std(mse),\n",
    "         'nodes':str(nodes_per_layer), 'dropout':dropout, \n",
    "         'activation':activation, 'batch_size':batch_size, 'TW' : sequence_length,\n",
    "         'time':training_time}\n",
    "\n",
    "#     results = results.append(pd.DataFrame(d, index=[0]), ignore_index=True)\n",
    "    results_all002 = pd.concat([results_all002, pd.DataFrame(d, index=[0])], ignore_index=True)\n",
    "    results_all002.to_csv('results/all/fd002.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>std_RMSE</th>\n",
       "      <th>S_score</th>\n",
       "      <th>std_S_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>TW</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.650763</td>\n",
       "      <td>0.0</td>\n",
       "      <td>838.278069</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.241714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "      <td>40</td>\n",
       "      <td>39.600502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.073473</td>\n",
       "      <td>0.0</td>\n",
       "      <td>855.127888</td>\n",
       "      <td>0.0</td>\n",
       "      <td>171.446854</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "      <td>40</td>\n",
       "      <td>64.197236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.785293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>782.038338</td>\n",
       "      <td>0.0</td>\n",
       "      <td>202.758621</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "      <td>40</td>\n",
       "      <td>45.513574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.298704</td>\n",
       "      <td>0.0</td>\n",
       "      <td>744.324630</td>\n",
       "      <td>0.0</td>\n",
       "      <td>183.460220</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "      <td>40</td>\n",
       "      <td>41.246588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.122668</td>\n",
       "      <td>0.0</td>\n",
       "      <td>893.482596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>208.057388</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "      <td>40</td>\n",
       "      <td>50.774455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RMSE  std_RMSE     S_score  std_S_score         MSE  std_MSE nodes  \\\n",
       "0  13.650763       0.0  838.278069          0.0  192.241714      0.0  [32]   \n",
       "1  13.073473       0.0  855.127888          0.0  171.446854      0.0  [32]   \n",
       "2  13.785293       0.0  782.038338          0.0  202.758621      0.0  [32]   \n",
       "3  13.298704       0.0  744.324630          0.0  183.460220      0.0  [32]   \n",
       "4  13.122668       0.0  893.482596          0.0  208.057388      0.0  [32]   \n",
       "\n",
       "   dropout activation  batch_size  TW       time  \n",
       "0      0.1       tanh         128  40  39.600502  \n",
       "1      0.1       tanh         128  40  64.197236  \n",
       "2      0.1       tanh         128  40  45.513574  \n",
       "3      0.1       tanh         128  40  41.246588  \n",
       "4      0.1       tanh         128  40  50.774455  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_all002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FD003 <a class=\"anchor\" id=\"fd003\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53759, 27) (33991, 26) (259, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# Load data and preprocess\n",
    "train, test, y_test = prepare_data('FD002.txt')\n",
    "print(train.shape, test.shape, y_test.shape)\n",
    "sensor_names = ['T20','T24','T30','T50','P20','P15','P30','Nf','Nc','epr','Ps30','phi',\n",
    "                    'NRf','NRc','BPR','farB','htBleed','Nf_dmd','PCNfR_dmd','W31','W32']\n",
    "\n",
    "remaining_sensors = ['T24','T30','T50', 'P15', 'P30','Nf','Nc', 'epr','Ps30','phi',\n",
    "                    'NRf','NRc','BPR','htBleed','W31','W32']\n",
    "drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "\n",
    "rul_piecewise = 125\n",
    "train['RUL'].clip(upper=rul_piecewise, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44919, 35, 16) (44919, 1) (259, 35, 16)\n",
      "Epoch 1/20\n",
      "1404/1404 [==============================] - 15s 9ms/step - loss: 889.4980 - val_loss: 379.7305\n",
      "Epoch 2/20\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 288.2106 - val_loss: 228.2756\n",
      "Epoch 3/20\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 254.8648 - val_loss: 198.0745\n",
      "Epoch 4/20\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 238.6362 - val_loss: 201.6884\n",
      "Epoch 5/20\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 227.1073 - val_loss: 178.9996\n",
      "Epoch 6/20\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 219.1151 - val_loss: 168.1743\n",
      "Epoch 7/20\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 215.7577 - val_loss: 172.0746\n",
      "Epoch 8/20\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 207.8927 - val_loss: 176.6565\n",
      "Epoch 9/20\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 201.3665 - val_loss: 172.8277\n",
      "Epoch 10/20\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 196.4328 - val_loss: 185.3393\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "(44919, 35, 16) (44919, 1) (259, 35, 16)\n",
      "Epoch 1/20\n",
      "1404/1404 [==============================] - 14s 9ms/step - loss: 763.4709 - val_loss: 234.6158\n",
      "Epoch 2/20\n",
      "1404/1404 [==============================] - 12s 8ms/step - loss: 276.9986 - val_loss: 258.9073\n",
      "Epoch 3/20\n",
      "1404/1404 [==============================] - 11s 8ms/step - loss: 254.4066 - val_loss: 297.6714\n",
      "Epoch 4/20\n",
      "1404/1404 [==============================] - 12s 8ms/step - loss: 240.6349 - val_loss: 350.2117\n",
      "Epoch 5/20\n",
      "1404/1404 [==============================] - 12s 8ms/step - loss: 228.8252 - val_loss: 334.9779\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "(44919, 35, 16) (44919, 1) (259, 35, 16)\n",
      "Epoch 1/20\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 884.9369 - val_loss: 269.4133\n",
      "Epoch 2/20\n",
      "1404/1404 [==============================] - 12s 8ms/step - loss: 273.2741 - val_loss: 201.8191\n",
      "Epoch 3/20\n",
      "1404/1404 [==============================] - 11s 8ms/step - loss: 244.7325 - val_loss: 181.4541\n",
      "Epoch 4/20\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 229.5674 - val_loss: 228.7778\n",
      "Epoch 5/20\n",
      "1404/1404 [==============================] - 12s 8ms/step - loss: 222.4053 - val_loss: 170.1757\n",
      "Epoch 6/20\n",
      "1404/1404 [==============================] - 12s 8ms/step - loss: 214.2664 - val_loss: 184.7052\n",
      "Epoch 7/20\n",
      "1404/1404 [==============================] - 12s 8ms/step - loss: 209.6927 - val_loss: 173.0760\n",
      "Epoch 8/20\n",
      "1404/1404 [==============================] - 12s 8ms/step - loss: 203.9627 - val_loss: 185.4084\n",
      "Epoch 9/20\n",
      "1404/1404 [==============================] - 12s 8ms/step - loss: 199.0286 - val_loss: 176.1611\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "(44919, 35, 16) (44919, 1) (259, 35, 16)\n",
      "Epoch 1/20\n",
      "1404/1404 [==============================] - 14s 9ms/step - loss: 859.8171 - val_loss: 237.8763\n",
      "Epoch 2/20\n",
      "1404/1404 [==============================] - 11s 8ms/step - loss: 296.1645 - val_loss: 253.7853\n",
      "Epoch 3/20\n",
      "1404/1404 [==============================] - 12s 8ms/step - loss: 258.4725 - val_loss: 195.7230\n",
      "Epoch 4/20\n",
      "1404/1404 [==============================] - 11s 8ms/step - loss: 245.5692 - val_loss: 184.6949\n",
      "Epoch 5/20\n",
      "1404/1404 [==============================] - 11s 8ms/step - loss: 232.9518 - val_loss: 194.3784\n",
      "Epoch 6/20\n",
      "1404/1404 [==============================] - 11s 8ms/step - loss: 225.0643 - val_loss: 177.1243\n",
      "Epoch 7/20\n",
      "1404/1404 [==============================] - 11s 8ms/step - loss: 216.9414 - val_loss: 176.2999\n",
      "Epoch 8/20\n",
      "1404/1404 [==============================] - 11s 8ms/step - loss: 211.9650 - val_loss: 182.2157\n",
      "Epoch 9/20\n",
      "1404/1404 [==============================] - 11s 8ms/step - loss: 206.6429 - val_loss: 170.4252\n",
      "Epoch 10/20\n",
      "1404/1404 [==============================] - 11s 8ms/step - loss: 202.7622 - val_loss: 194.1206\n",
      "Epoch 11/20\n",
      "1404/1404 [==============================] - 11s 8ms/step - loss: 198.0966 - val_loss: 175.5229\n",
      "Epoch 12/20\n",
      "1404/1404 [==============================] - 11s 8ms/step - loss: 194.8307 - val_loss: 171.6649\n",
      "Epoch 13/20\n",
      "1404/1404 [==============================] - 11s 8ms/step - loss: 189.4930 - val_loss: 169.9847\n",
      "Epoch 14/20\n",
      "1404/1404 [==============================] - 11s 8ms/step - loss: 188.0224 - val_loss: 191.1419\n",
      "Epoch 15/20\n",
      "1404/1404 [==============================] - 11s 8ms/step - loss: 185.3135 - val_loss: 163.6010\n",
      "Epoch 16/20\n",
      "1404/1404 [==============================] - 11s 8ms/step - loss: 183.3041 - val_loss: 169.4994\n",
      "Epoch 17/20\n",
      "1404/1404 [==============================] - 12s 8ms/step - loss: 181.9405 - val_loss: 165.7081\n",
      "Epoch 18/20\n",
      "1404/1404 [==============================] - 12s 8ms/step - loss: 179.6193 - val_loss: 160.0417\n",
      "Epoch 19/20\n",
      "1404/1404 [==============================] - 11s 8ms/step - loss: 178.5730 - val_loss: 155.4015\n",
      "Epoch 20/20\n",
      "1404/1404 [==============================] - 12s 8ms/step - loss: 176.8720 - val_loss: 161.5361\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "(44919, 35, 16) (44919, 1) (259, 35, 16)\n",
      "Epoch 1/20\n",
      "1404/1404 [==============================] - 13s 9ms/step - loss: 798.1689 - val_loss: 265.2468\n",
      "Epoch 2/20\n",
      "1404/1404 [==============================] - 12s 9ms/step - loss: 275.5837 - val_loss: 215.4393\n",
      "Epoch 3/20\n",
      "1404/1404 [==============================] - 12s 8ms/step - loss: 250.4927 - val_loss: 193.2313\n",
      "Epoch 4/20\n",
      "1404/1404 [==============================] - 12s 8ms/step - loss: 236.3382 - val_loss: 232.4210\n",
      "Epoch 5/20\n",
      "1404/1404 [==============================] - 12s 8ms/step - loss: 225.3151 - val_loss: 202.5764\n",
      "Epoch 6/20\n",
      "1404/1404 [==============================] - 12s 8ms/step - loss: 216.4305 - val_loss: 187.2504\n",
      "Epoch 7/20\n",
      "1404/1404 [==============================] - 12s 8ms/step - loss: 211.0765 - val_loss: 186.8539\n",
      "Epoch 8/20\n",
      "1404/1404 [==============================] - 12s 8ms/step - loss: 204.7936 - val_loss: 177.8682\n",
      "Epoch 9/20\n",
      "1404/1404 [==============================] - 12s 8ms/step - loss: 197.8318 - val_loss: 177.0115\n",
      "Epoch 10/20\n",
      "1404/1404 [==============================] - 12s 8ms/step - loss: 196.4144 - val_loss: 173.7126\n",
      "Epoch 11/20\n",
      "1404/1404 [==============================] - 12s 8ms/step - loss: 195.3237 - val_loss: 173.6895\n",
      "Epoch 12/20\n",
      "1404/1404 [==============================] - 12s 8ms/step - loss: 187.5155 - val_loss: 171.5694\n",
      "Epoch 13/20\n",
      "1404/1404 [==============================] - 12s 8ms/step - loss: 186.0583 - val_loss: 187.4788\n",
      "Epoch 14/20\n",
      "1404/1404 [==============================] - 12s 8ms/step - loss: 184.6506 - val_loss: 161.8632\n",
      "Epoch 15/20\n",
      "1404/1404 [==============================] - 12s 8ms/step - loss: 183.0600 - val_loss: 186.0246\n",
      "Epoch 16/20\n",
      "1404/1404 [==============================] - 12s 8ms/step - loss: 180.9303 - val_loss: 192.4101\n",
      "Epoch 17/20\n",
      "1404/1404 [==============================] - 12s 8ms/step - loss: 179.2475 - val_loss: 193.3940\n",
      "Epoch 18/20\n",
      "1404/1404 [==============================] - 12s 8ms/step - loss: 178.6083 - val_loss: 185.6096\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "CPU times: total: 34min 18s\n",
      "Wall time: 12min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results_all003 = pd.DataFrame()\n",
    "for SEED in range(5):  \n",
    "    tf.random.set_seed(SEED)\n",
    "    mse = []\n",
    "    R2_val = []\n",
    "    RMSE = []\n",
    "    score_val = []\n",
    "    \n",
    "    # parameter's sample\n",
    "    # weights_file = \"weights_file_lstm_optimalmodel_all.h5\"\n",
    "    alpha = 0.1\n",
    "    sequence_length = 35\n",
    "    epochs = 20\n",
    "    nodes_per_layer = [64]\n",
    "    dropout = 0.2\n",
    "    activation = 'tanh'\n",
    "    batch_size = 32\n",
    "    remaining_sensors = remaining_sensors\n",
    "    # create model\n",
    "    input_shape = (sequence_length, len(remaining_sensors))\n",
    "    model = model_lstm_1layer(input_shape, nodes_per_layer[0], dropout,\n",
    "                             activation)\n",
    "    \n",
    "    # Data prepration\n",
    "    X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "\n",
    "    # create sequences train, test\n",
    "    train_array = gen_data_wrapper(X_train_interim, sequence_length,remaining_sensors)\n",
    "    label_array = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'])\n",
    "\n",
    "    test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,remaining_sensors, -99.))\n",
    "               for unit_nr in X_test_interim['Unit'].unique())\n",
    "    \n",
    "    test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "    test_rul = rul_piecewise_fct(y_test,rul_piecewise)\n",
    "    print(train_array.shape, label_array.shape, test_array.shape)\n",
    "            \n",
    "    # Model fitting\n",
    "    cb = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        start_time = time.time()\n",
    "        model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n",
    "        weights_file = model.get_weights()\n",
    "        model.set_weights(weights_file)  # reset optimizer and node weights before every training iteration\n",
    "        history = model.fit(train_array, label_array,\n",
    "                                validation_data=(test_array, test_rul),\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                callbacks=[cb],\n",
    "                                verbose=1)\n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "        mse.append(history.history['val_loss'][-1])\n",
    "\n",
    "        y_hat_val_split = model.predict(test_array)\n",
    "        R2_val.append(r2_score(test_rul, y_hat_val_split))\n",
    "        RMSE.append(np.sqrt(mean_squared_error(test_rul, y_hat_val_split)))\n",
    "        score_val.append(compute_s_score(test_rul, y_hat_val_split))\n",
    "            \n",
    "        \n",
    "    #  append results\n",
    "    d = {'RMSE' :np.mean(RMSE), 'std_RMSE' :np.std(RMSE),\n",
    "         'S_score' :np.mean(score_val), 'std_S_score' :np.std(score_val),\n",
    "         'MSE':np.mean(mse), 'std_MSE':np.std(mse),\n",
    "         'nodes':str(nodes_per_layer), 'dropout':dropout, \n",
    "         'activation':activation, 'batch_size':batch_size, 'TW' : sequence_length,\n",
    "         'time':training_time}\n",
    "\n",
    "#     results = results.append(pd.DataFrame(d, index=[0]), ignore_index=True)\n",
    "    results_all003 = pd.concat([results_all003, pd.DataFrame(d, index=[0])], ignore_index=True)\n",
    "    results_all003.to_csv('results/all/fd003.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>std_RMSE</th>\n",
       "      <th>S_score</th>\n",
       "      <th>std_S_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>TW</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.968202</td>\n",
       "      <td>0.0</td>\n",
       "      <td>773.809820</td>\n",
       "      <td>0.0</td>\n",
       "      <td>185.339279</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "      <td>131.362647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.317172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1007.629807</td>\n",
       "      <td>0.0</td>\n",
       "      <td>334.977905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "      <td>60.344443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.045141</td>\n",
       "      <td>0.0</td>\n",
       "      <td>724.167435</td>\n",
       "      <td>0.0</td>\n",
       "      <td>176.161057</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "      <td>106.996501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.709683</td>\n",
       "      <td>0.0</td>\n",
       "      <td>821.695969</td>\n",
       "      <td>0.0</td>\n",
       "      <td>161.536057</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "      <td>231.366024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.722547</td>\n",
       "      <td>0.0</td>\n",
       "      <td>684.642265</td>\n",
       "      <td>0.0</td>\n",
       "      <td>185.609589</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "      <td>212.687683</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RMSE  std_RMSE      S_score  std_S_score         MSE  std_MSE nodes  \\\n",
       "0  12.968202       0.0   773.809820          0.0  185.339279      0.0  [64]   \n",
       "1  15.317172       0.0  1007.629807          0.0  334.977905      0.0  [64]   \n",
       "2  13.045141       0.0   724.167435          0.0  176.161057      0.0  [64]   \n",
       "3  12.709683       0.0   821.695969          0.0  161.536057      0.0  [64]   \n",
       "4  12.722547       0.0   684.642265          0.0  185.609589      0.0  [64]   \n",
       "\n",
       "   dropout activation  batch_size  TW        time  \n",
       "0      0.2       tanh          32  35  131.362647  \n",
       "1      0.2       tanh          32  35   60.344443  \n",
       "2      0.2       tanh          32  35  106.996501  \n",
       "3      0.2       tanh          32  35  231.366024  \n",
       "4      0.2       tanh          32  35  212.687683  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_all003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FD001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20631, 27) (13096, 26) (100, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# Load data and preprocess\n",
    "train, test, y_test = prepare_data('FD001.txt')\n",
    "print(train.shape, test.shape, y_test.shape)\n",
    "sensor_names = ['T20','T24','T30','T50','P20','P15','P30','Nf','Nc','epr','Ps30','phi',\n",
    "                    'NRf','NRc','BPR','farB','htBleed','Nf_dmd','PCNfR_dmd','W31','W32']\n",
    "remaining_sensors = ['T24','T30','T50', 'P15', 'P30','Nf','Nc','Ps30','phi',\n",
    "                    'NRf','NRc','BPR','htBleed','W31','W32']\n",
    "drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "\n",
    "rul_piecewise = 125\n",
    "train['RUL'].clip(upper=rul_piecewise, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17731, 30, 15) (17731, 1) (100, 30, 15)\n",
      "Epoch 1/20\n",
      "278/278 [==============================] - 7s 18ms/step - loss: 1945.0713 - val_loss: 1142.2662\n",
      "Epoch 2/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 594.9089 - val_loss: 336.2231\n",
      "Epoch 3/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 349.9518 - val_loss: 213.0034\n",
      "Epoch 4/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 243.1687 - val_loss: 380.7289\n",
      "Epoch 5/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 222.0929 - val_loss: 180.0421\n",
      "Epoch 6/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 203.6436 - val_loss: 266.0940\n",
      "Epoch 7/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 200.1159 - val_loss: 201.9042\n",
      "Epoch 8/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 189.3215 - val_loss: 217.6176\n",
      "Epoch 9/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 187.8194 - val_loss: 170.0289\n",
      "Epoch 10/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 182.5679 - val_loss: 182.4919\n",
      "Epoch 11/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 171.0725 - val_loss: 165.3862\n",
      "Epoch 12/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 174.3374 - val_loss: 189.9700\n",
      "Epoch 13/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 171.1269 - val_loss: 189.6711\n",
      "Epoch 14/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 168.6948 - val_loss: 221.8667\n",
      "Epoch 15/20\n",
      "278/278 [==============================] - 5s 16ms/step - loss: 171.7371 - val_loss: 184.4069\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "(17731, 30, 15) (17731, 1) (100, 30, 15)\n",
      "Epoch 1/20\n",
      "278/278 [==============================] - 6s 16ms/step - loss: 1952.8530 - val_loss: 990.8794\n",
      "Epoch 2/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 608.1251 - val_loss: 449.7914\n",
      "Epoch 3/20\n",
      "278/278 [==============================] - 5s 16ms/step - loss: 346.0687 - val_loss: 450.3850\n",
      "Epoch 4/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 243.6275 - val_loss: 235.8803\n",
      "Epoch 5/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 208.5704 - val_loss: 230.4949\n",
      "Epoch 6/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 204.7195 - val_loss: 177.5652\n",
      "Epoch 7/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 194.7312 - val_loss: 240.1607\n",
      "Epoch 8/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 193.7889 - val_loss: 169.3054\n",
      "Epoch 9/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 185.7281 - val_loss: 193.4146\n",
      "Epoch 10/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 172.9265 - val_loss: 311.7087\n",
      "Epoch 11/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 175.0038 - val_loss: 177.7875\n",
      "Epoch 12/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 174.8739 - val_loss: 165.3689\n",
      "Epoch 13/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 168.0203 - val_loss: 168.2959\n",
      "Epoch 14/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 169.6170 - val_loss: 169.7762\n",
      "Epoch 15/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 166.2830 - val_loss: 244.7424\n",
      "Epoch 16/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 167.2132 - val_loss: 264.9800\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "(17731, 30, 15) (17731, 1) (100, 30, 15)\n",
      "Epoch 1/20\n",
      "278/278 [==============================] - 6s 17ms/step - loss: 1975.2908 - val_loss: 915.3917\n",
      "Epoch 2/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 554.9566 - val_loss: 632.6024\n",
      "Epoch 3/20\n",
      "278/278 [==============================] - 5s 16ms/step - loss: 324.1010 - val_loss: 241.9236\n",
      "Epoch 4/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 256.7356 - val_loss: 331.1473\n",
      "Epoch 5/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 230.4926 - val_loss: 210.3607\n",
      "Epoch 6/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 217.0039 - val_loss: 168.7097\n",
      "Epoch 7/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 200.9943 - val_loss: 185.5622\n",
      "Epoch 8/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 193.8797 - val_loss: 180.1497\n",
      "Epoch 9/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 188.0363 - val_loss: 166.0519\n",
      "Epoch 10/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 182.0837 - val_loss: 303.9132\n",
      "Epoch 11/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 184.5538 - val_loss: 173.8203\n",
      "Epoch 12/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 177.8627 - val_loss: 167.6324\n",
      "Epoch 13/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 171.5343 - val_loss: 198.0423\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "(17731, 30, 15) (17731, 1) (100, 30, 15)\n",
      "Epoch 1/20\n",
      "278/278 [==============================] - 7s 17ms/step - loss: 1984.7781 - val_loss: 944.8501\n",
      "Epoch 2/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 595.3312 - val_loss: 796.7349\n",
      "Epoch 3/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 487.3704 - val_loss: 448.8324\n",
      "Epoch 4/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 300.1323 - val_loss: 243.3225\n",
      "Epoch 5/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 227.1539 - val_loss: 307.1239\n",
      "Epoch 6/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 217.6322 - val_loss: 243.2936\n",
      "Epoch 7/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 205.4608 - val_loss: 583.1284\n",
      "Epoch 8/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 199.0123 - val_loss: 202.9959\n",
      "Epoch 9/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 192.3214 - val_loss: 306.6041\n",
      "Epoch 10/20\n",
      "278/278 [==============================] - 5s 16ms/step - loss: 191.9700 - val_loss: 228.2826\n",
      "Epoch 11/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 181.8290 - val_loss: 174.9664\n",
      "Epoch 12/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 176.6738 - val_loss: 270.9405\n",
      "Epoch 13/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 176.1307 - val_loss: 226.5398\n",
      "Epoch 14/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 171.4944 - val_loss: 292.2494\n",
      "Epoch 15/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 184.8886 - val_loss: 307.2288\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "(17731, 30, 15) (17731, 1) (100, 30, 15)\n",
      "Epoch 1/20\n",
      "278/278 [==============================] - 6s 16ms/step - loss: 2036.7047 - val_loss: 1121.9025\n",
      "Epoch 2/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 608.3298 - val_loss: 511.1324\n",
      "Epoch 3/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 451.4001 - val_loss: 397.6616\n",
      "Epoch 4/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 283.1056 - val_loss: 220.1386\n",
      "Epoch 5/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 225.7812 - val_loss: 362.0092\n",
      "Epoch 6/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 220.5620 - val_loss: 203.5515\n",
      "Epoch 7/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 196.6146 - val_loss: 160.1352\n",
      "Epoch 8/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 192.0621 - val_loss: 206.8795\n",
      "Epoch 9/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 187.9201 - val_loss: 161.9342\n",
      "Epoch 10/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 184.2150 - val_loss: 168.6344\n",
      "Epoch 11/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 180.1347 - val_loss: 256.5178\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "CPU times: total: 13min 17s\n",
      "Wall time: 5min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results_all001 = pd.DataFrame()\n",
    "for SEED in range(5):  \n",
    "    set_seed(SEED)\n",
    "    mse = []\n",
    "    R2_val = []\n",
    "    RMSE = []\n",
    "    score_val = []\n",
    "    \n",
    "    # parameter's sample\n",
    "    # weights_file = \"weights_file_lstm_optimalmodel_all.h5\"\n",
    "    alpha = 0.1\n",
    "    sequence_length = 30\n",
    "    epochs = 20\n",
    "    nodes_per_layer = [128]\n",
    "    dropout = 0.2\n",
    "    activation = 'tanh'\n",
    "    batch_size = 64\n",
    "    remaining_sensors = remaining_sensors\n",
    "    # create model\n",
    "    input_shape = (sequence_length, len(remaining_sensors))\n",
    "    model = model_lstm_1layer(input_shape, nodes_per_layer[0], dropout,\n",
    "                             activation)\n",
    "    \n",
    "    # Data prepration\n",
    "    X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "\n",
    "    # create sequences train, test\n",
    "    train_array = gen_data_wrapper(X_train_interim, sequence_length,remaining_sensors)\n",
    "    label_array = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'])\n",
    "\n",
    "    test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,remaining_sensors, -99.))\n",
    "               for unit_nr in X_test_interim['Unit'].unique())\n",
    "    \n",
    "    test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "    test_rul = rul_piecewise_fct(y_test,rul_piecewise)\n",
    "    print(train_array.shape, label_array.shape, test_array.shape)\n",
    "            \n",
    "    # Model fitting\n",
    "    cb = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        start_time = time.time()\n",
    "        model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n",
    "        weights_file = model.get_weights()\n",
    "        model.set_weights(weights_file)  # reset optimizer and node weights before every training iteration\n",
    "        history = model.fit(train_array, label_array,\n",
    "                                validation_data=(test_array, test_rul),\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                callbacks=[cb],\n",
    "                                verbose=1)\n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "        mse.append(history.history['val_loss'][-1])\n",
    "\n",
    "        y_hat_val_split = model.predict(test_array)\n",
    "        R2_val.append(r2_score(test_rul, y_hat_val_split))\n",
    "        RMSE.append(np.sqrt(mean_squared_error(test_rul, y_hat_val_split)))\n",
    "        score_val.append(compute_s_score(test_rul, y_hat_val_split))\n",
    "            \n",
    "        \n",
    "    #  append results\n",
    "    d = {'RMSE' :np.mean(RMSE), 'std_RMSE' :np.std(RMSE),\n",
    "         'S_score' :np.mean(score_val), 'std_S_score' :np.std(score_val),\n",
    "         'MSE':np.mean(mse), 'std_MSE':np.std(mse),\n",
    "         'nodes':str(nodes_per_layer), 'dropout':dropout, \n",
    "         'activation':activation, 'batch_size':batch_size, 'TW' : sequence_length,\n",
    "         'time':training_time}\n",
    "\n",
    "#     results = results.append(pd.DataFrame(d, index=[0]), ignore_index=True)\n",
    "    results_all001 = pd.concat([results_all001, pd.DataFrame(d, index=[0])], ignore_index=True)\n",
    "    results_all001.to_csv('results/all/fd001.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>std_RMSE</th>\n",
       "      <th>S_score</th>\n",
       "      <th>std_S_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>TW</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.860256</td>\n",
       "      <td>0.0</td>\n",
       "      <td>286.252775</td>\n",
       "      <td>0.0</td>\n",
       "      <td>184.406876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "      <td>69.097678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.859582</td>\n",
       "      <td>0.0</td>\n",
       "      <td>229.533913</td>\n",
       "      <td>0.0</td>\n",
       "      <td>264.980011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "      <td>70.237192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.886112</td>\n",
       "      <td>0.0</td>\n",
       "      <td>220.296867</td>\n",
       "      <td>0.0</td>\n",
       "      <td>198.042267</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "      <td>58.060213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.227489</td>\n",
       "      <td>0.0</td>\n",
       "      <td>262.783810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>307.228760</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "      <td>67.580328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.654454</td>\n",
       "      <td>0.0</td>\n",
       "      <td>237.325803</td>\n",
       "      <td>0.0</td>\n",
       "      <td>256.517761</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "      <td>48.572393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RMSE  std_RMSE     S_score  std_S_score         MSE  std_MSE  nodes  \\\n",
       "0  12.860256       0.0  286.252775          0.0  184.406876      0.0  [128]   \n",
       "1  12.859582       0.0  229.533913          0.0  264.980011      0.0  [128]   \n",
       "2  12.886112       0.0  220.296867          0.0  198.042267      0.0  [128]   \n",
       "3  13.227489       0.0  262.783810          0.0  307.228760      0.0  [128]   \n",
       "4  12.654454       0.0  237.325803          0.0  256.517761      0.0  [128]   \n",
       "\n",
       "   dropout activation  batch_size  TW       time  \n",
       "0      0.2       tanh          64  30  69.097678  \n",
       "1      0.2       tanh          64  30  70.237192  \n",
       "2      0.2       tanh          64  30  58.060213  \n",
       "3      0.2       tanh          64  30  67.580328  \n",
       "4      0.2       tanh          64  30  48.572393  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_all001"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
