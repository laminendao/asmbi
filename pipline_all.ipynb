{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "from lime.lime_tabular import RecurrentTabularExplainer\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error, r2_score \n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn import preprocessing\n",
    "from keras import backend as K\n",
    "from sklearn.preprocessing import MinMaxScaler , StandardScaler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Activation, GRU\n",
    "from scipy import optimize\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "\n",
    "from sp_modif.model_function import *\n",
    "from sp_modif.methods import *\n",
    "from sp_modif.data_prep import *\n",
    "from sp_modif.evaluator import *\n",
    "from sp_modif.SHAP import *\n",
    "from sp_modif.L2X import *\n",
    "from methods import *\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEED = 0\n",
    "def set_seed(seed=SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    tf.random.set_seed(SEED)\n",
    "\n",
    "# Appeler la fonction pour fixer le seed\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61249, 27) (41214, 26) (248, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# Load data and preprocess\n",
    "train, test, y_test = prepare_data('FD004.txt')\n",
    "print(train.shape, test.shape, y_test.shape)\n",
    "sensor_names = ['T20','T24','T30','T50','P20','P15','P30','Nf','Nc','epr','Ps30','phi',\n",
    "                    'NRf','NRc','BPR','farB','htBleed','Nf_dmd','PCNfR_dmd','W31','W32']\n",
    "\n",
    "remaining_sensors = ['T24','T30','T50', 'P15', 'P30','Nf','Nc', 'epr','Ps30','phi',\n",
    "                    'NRf','NRc','BPR', 'farB','htBleed','W31','W32']\n",
    "drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "\n",
    "rul_piecewise = 120\n",
    "train['RUL'].clip(upper=rul_piecewise, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_lstm_1layer(input_shape, nodes_per_layer, dropout, activation):\n",
    "    \n",
    "    cb = keras.callbacks.EarlyStopping(monitor='loss', patience=4)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units = nodes_per_layer, activation=activation, \n",
    "                  input_shape=input_shape))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(256))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer=Adam(learning_rate=0.001))\n",
    "    # model.save_weights(weights_file)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51538, 40, 17) (51538, 1) (248, 40, 17)\n",
      "Epoch 1/20\n",
      "1611/1611 [==============================] - 14s 8ms/step - loss: 762.7661 - val_loss: 328.0673\n",
      "Epoch 2/20\n",
      "1611/1611 [==============================] - 12s 8ms/step - loss: 294.9990 - val_loss: 232.8337\n",
      "Epoch 3/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 249.8761 - val_loss: 213.0574\n",
      "Epoch 4/20\n",
      "1611/1611 [==============================] - 14s 8ms/step - loss: 231.6885 - val_loss: 207.1930\n",
      "Epoch 5/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 215.2335 - val_loss: 189.2900\n",
      "Epoch 6/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 200.3619 - val_loss: 187.0126\n",
      "Epoch 7/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 189.2251 - val_loss: 180.8434\n",
      "Epoch 8/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 182.3082 - val_loss: 169.7781\n",
      "Epoch 9/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 175.0479 - val_loss: 163.9495\n",
      "Epoch 10/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 172.0753 - val_loss: 171.1262\n",
      "Epoch 11/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 166.3886 - val_loss: 154.3001\n",
      "Epoch 12/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 165.4960 - val_loss: 174.5352\n",
      "Epoch 13/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 161.2135 - val_loss: 161.3920\n",
      "Epoch 14/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 159.8887 - val_loss: 153.0198\n",
      "Epoch 15/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 157.9835 - val_loss: 159.0417\n",
      "Epoch 16/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 156.1850 - val_loss: 168.4477\n",
      "Epoch 17/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 156.3740 - val_loss: 155.2228\n",
      "Epoch 18/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 153.7750 - val_loss: 153.6516\n",
      "Epoch 19/20\n",
      "1611/1611 [==============================] - 11s 7ms/step - loss: 152.8708 - val_loss: 157.1522\n",
      "Epoch 20/20\n",
      "1611/1611 [==============================] - 341s 212ms/step - loss: 151.8726 - val_loss: 157.5444\n",
      "8/8 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 89] Operation canceled: 'results/all/fd004.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:71\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/pandas/core/generic.py:3902\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3891\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[1;32m   3893\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[1;32m   3894\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[1;32m   3895\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3899\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[1;32m   3900\u001b[0m )\n\u001b[0;32m-> 3902\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3903\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3904\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3905\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3906\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3907\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3908\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3909\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3910\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3911\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3912\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3913\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3914\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/pandas/io/formats/format.py:1152\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1131\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[1;32m   1134\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[1;32m   1135\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[1;32m   1151\u001b[0m )\n\u001b[0;32m-> 1152\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[1;32m   1155\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/pandas/io/formats/csvs.py:247\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[0;32m--> 247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[1;32m    257\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[1;32m    258\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[1;32m    264\u001b[0m     )\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 89] Operation canceled: 'results/all/fd004.csv'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results_all = pd.DataFrame()\n",
    "for SEED in range(5):  \n",
    "    tf.random.set_seed(SEED)\n",
    "    mse = []\n",
    "    R2_val = []\n",
    "    RMSE = []\n",
    "    score_val = []\n",
    "    \n",
    "    # 0.20\t[64]\t0.3\ttanh\t32\t25\n",
    "    \n",
    "    # parameter's sample\n",
    "    # weights_file = \"weights_file_lstm_optimalmodel_all.h5\"\n",
    "    alpha = 0.3\n",
    "    sequence_length = 40\n",
    "    epochs = 20\n",
    "    nodes_per_layer = [64]\n",
    "    dropout = 0.2\n",
    "    activation = 'tanh'\n",
    "    batch_size = 32\n",
    "    remaining_sensors = remaining_sensors\n",
    "    # create model\n",
    "    input_shape = (sequence_length, len(remaining_sensors))\n",
    "    model = model_lstm_1layer(input_shape, nodes_per_layer[0], dropout, activation)\n",
    "    \n",
    "    # Data prepration\n",
    "    X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "\n",
    "    # create sequences train, test\n",
    "    train_array = gen_data_wrapper(X_train_interim, sequence_length,remaining_sensors)\n",
    "    label_array = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'])\n",
    "\n",
    "    test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,remaining_sensors, -99.))\n",
    "               for unit_nr in X_test_interim['Unit'].unique())\n",
    "    \n",
    "    test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "    test_rul = rul_piecewise_fct(y_test,rul_piecewise)\n",
    "    print(train_array.shape, label_array.shape, test_array.shape)\n",
    "            \n",
    "    # Model fitting\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        start_time = time.time()\n",
    "        model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n",
    "        weights_file = model.get_weights()\n",
    "        model.set_weights(weights_file)  # reset optimizer and node weights before every training iteration\n",
    "        history = model.fit(train_array, label_array,\n",
    "                                validation_data=(test_array, test_rul),\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                # callbacks=[cb],\n",
    "                                verbose=1)\n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "        mse.append(history.history['val_loss'][-1])\n",
    "\n",
    "        y_hat_val_split = model.predict(test_array)\n",
    "        R2_val.append(r2_score(test_rul, y_hat_val_split))\n",
    "        RMSE.append(np.sqrt(mean_squared_error(test_rul, y_hat_val_split)))\n",
    "        score_val.append(compute_s_score(test_rul, y_hat_val_split))\n",
    "            \n",
    "        \n",
    "    #  append results\n",
    "    d = {'RMSE' :np.mean(RMSE), 'std_RMSE' :np.std(RMSE),\n",
    "         'S_score' :np.mean(score_val), 'std_S_score' :np.std(score_val),\n",
    "         'MSE':np.mean(mse), 'std_MSE':np.std(mse),\n",
    "         'nodes':str(nodes_per_layer), 'dropout':dropout, \n",
    "         'activation':activation, 'batch_size':batch_size, 'TW' : sequence_length,\n",
    "         'time':training_time}\n",
    "\n",
    "#     results = results.append(pd.DataFrame(d, index=[0]), ignore_index=True)\n",
    "    results_all = pd.concat([results_all, pd.DataFrame(d, index=[0])], ignore_index=True)\n",
    "    results_all.to_csv('results/all/fd004.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>std_RMSE</th>\n",
       "      <th>S_score</th>\n",
       "      <th>std_S_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>TW</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.551669</td>\n",
       "      <td>0.0</td>\n",
       "      <td>741.853794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>157.544388</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>560.656871</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RMSE  std_RMSE     S_score  std_S_score         MSE  std_MSE nodes  \\\n",
       "0  12.551669       0.0  741.853794          0.0  157.544388      0.0  [64]   \n",
       "\n",
       "   dropout activation  batch_size  TW        time  \n",
       "0      0.2       tanh          32  40  560.656871  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FD002 <a class=\"anchor\" id=\"fd002\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53759, 27) (33991, 26) (259, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# Load data and preprocess\n",
    "train, test, y_test = prepare_data('FD002.txt')\n",
    "print(train.shape, test.shape, y_test.shape)\n",
    "sensor_names = ['T20','T24','T30','T50','P20','P15','P30','Nf','Nc','epr','Ps30','phi',\n",
    "                    'NRf','NRc','BPR','farB','htBleed','Nf_dmd','PCNfR_dmd','W31','W32']\n",
    "\n",
    "remaining_sensors = ['T24','T30','T50', 'P15', 'P30','Nf','Nc', 'epr','Ps30','phi',\n",
    "                    'NRf','NRc','BPR','htBleed','W31','W32']\n",
    "drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "\n",
    "rul_piecewise = 125\n",
    "train['RUL'].clip(upper=rul_piecewise, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43619, 40, 16) (43619, 1) (259, 40, 16)\n",
      "Epoch 1/20\n",
      "341/341 [==============================] - 4s 10ms/step - loss: 2300.0369 - val_loss: 1411.3716\n",
      "Epoch 2/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 552.4333 - val_loss: 282.0428\n",
      "Epoch 3/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 301.3049 - val_loss: 205.2548\n",
      "Epoch 4/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 257.1705 - val_loss: 195.6782\n",
      "Epoch 5/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 243.2023 - val_loss: 182.1209\n",
      "Epoch 6/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 237.6594 - val_loss: 189.3480\n",
      "Epoch 7/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 233.1519 - val_loss: 176.0869\n",
      "Epoch 8/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 229.4595 - val_loss: 192.5895\n",
      "Epoch 9/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 222.5657 - val_loss: 189.8916\n",
      "Epoch 10/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 223.3686 - val_loss: 174.8348\n",
      "Epoch 11/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 218.9524 - val_loss: 166.9273\n",
      "Epoch 12/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 217.9912 - val_loss: 176.2374\n",
      "Epoch 13/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 214.3904 - val_loss: 168.3577\n",
      "Epoch 14/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 211.4370 - val_loss: 169.8201\n",
      "Epoch 15/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 209.6512 - val_loss: 170.7457\n",
      "Epoch 16/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 206.3834 - val_loss: 165.8818\n",
      "Epoch 17/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 205.5633 - val_loss: 175.7752\n",
      "Epoch 18/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 202.2817 - val_loss: 159.5289\n",
      "Epoch 19/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 201.8445 - val_loss: 162.5980\n",
      "Epoch 20/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 202.4951 - val_loss: 171.6662\n",
      "9/9 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43619, 40, 16) (43619, 1) (259, 40, 16)\n",
      "Epoch 1/20\n",
      "341/341 [==============================] - 4s 10ms/step - loss: 2267.1296 - val_loss: 1253.7079\n",
      "Epoch 2/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 469.4091 - val_loss: 206.8166\n",
      "Epoch 3/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 269.0750 - val_loss: 196.0345\n",
      "Epoch 4/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 258.0416 - val_loss: 184.8317\n",
      "Epoch 5/20\n",
      "341/341 [==============================] - 4s 10ms/step - loss: 239.6045 - val_loss: 223.4978\n",
      "Epoch 6/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 236.3609 - val_loss: 191.5826\n",
      "Epoch 7/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 232.2512 - val_loss: 194.0178\n",
      "Epoch 8/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 230.5434 - val_loss: 195.0752\n",
      "Epoch 9/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 226.8534 - val_loss: 193.9557\n",
      "Epoch 10/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 223.3961 - val_loss: 194.4494\n",
      "Epoch 11/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 222.7485 - val_loss: 206.4819\n",
      "Epoch 12/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 219.9887 - val_loss: 180.5555\n",
      "Epoch 13/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 217.1783 - val_loss: 191.9256\n",
      "Epoch 14/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 213.4852 - val_loss: 189.8820\n",
      "Epoch 15/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 212.2027 - val_loss: 181.6108\n",
      "Epoch 16/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 210.8948 - val_loss: 181.4195\n",
      "Epoch 17/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 210.8467 - val_loss: 177.7582\n",
      "Epoch 18/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 209.1120 - val_loss: 203.0759\n",
      "Epoch 19/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 204.4888 - val_loss: 181.4556\n",
      "Epoch 20/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 205.9262 - val_loss: 174.9888\n",
      "9/9 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43619, 40, 16) (43619, 1) (259, 40, 16)\n",
      "Epoch 1/20\n",
      "341/341 [==============================] - 4s 9ms/step - loss: 2347.6218 - val_loss: 1665.2920\n",
      "Epoch 2/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 639.2714 - val_loss: 271.6960\n",
      "Epoch 3/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 295.7878 - val_loss: 219.6517\n",
      "Epoch 4/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 269.8353 - val_loss: 196.1009\n",
      "Epoch 5/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 255.6487 - val_loss: 189.8894\n",
      "Epoch 6/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 245.1190 - val_loss: 186.4090\n",
      "Epoch 7/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 239.0444 - val_loss: 189.2510\n",
      "Epoch 8/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 234.4229 - val_loss: 198.7714\n",
      "Epoch 9/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 228.7659 - val_loss: 179.4258\n",
      "Epoch 10/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 230.0809 - val_loss: 195.5463\n",
      "Epoch 11/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 223.3112 - val_loss: 188.1932\n",
      "Epoch 12/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 228.0670 - val_loss: 199.9387\n",
      "Epoch 13/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 218.6890 - val_loss: 191.3290\n",
      "Epoch 14/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 219.2293 - val_loss: 176.3441\n",
      "Epoch 15/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 215.7475 - val_loss: 173.9639\n",
      "Epoch 16/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 215.6133 - val_loss: 178.2995\n",
      "Epoch 17/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 214.0622 - val_loss: 176.9291\n",
      "Epoch 18/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 208.8153 - val_loss: 181.1961\n",
      "Epoch 19/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 209.5532 - val_loss: 179.8370\n",
      "Epoch 20/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 211.4874 - val_loss: 171.8412\n",
      "9/9 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43619, 40, 16) (43619, 1) (259, 40, 16)\n",
      "Epoch 1/20\n",
      "341/341 [==============================] - 4s 11ms/step - loss: 2280.4766 - val_loss: 1257.2867\n",
      "Epoch 2/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 560.7480 - val_loss: 355.6381\n",
      "Epoch 3/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 321.4266 - val_loss: 247.3982\n",
      "Epoch 4/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 254.9767 - val_loss: 293.5551\n",
      "Epoch 5/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 248.4174 - val_loss: 187.3849\n",
      "Epoch 6/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 237.5092 - val_loss: 199.7161\n",
      "Epoch 7/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 231.6589 - val_loss: 180.8135\n",
      "Epoch 8/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 229.7975 - val_loss: 179.4874\n",
      "Epoch 9/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 229.6983 - val_loss: 189.3564\n",
      "Epoch 10/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 223.2585 - val_loss: 175.8907\n",
      "Epoch 11/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 220.2992 - val_loss: 210.9411\n",
      "Epoch 12/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 218.8551 - val_loss: 179.5188\n",
      "Epoch 13/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 218.6045 - val_loss: 177.0604\n",
      "Epoch 14/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 214.7548 - val_loss: 169.3944\n",
      "Epoch 15/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 213.9806 - val_loss: 179.6974\n",
      "Epoch 16/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 210.7931 - val_loss: 172.6668\n",
      "Epoch 17/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 211.0189 - val_loss: 178.7685\n",
      "Epoch 18/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 205.5394 - val_loss: 172.4739\n",
      "Epoch 19/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 205.7848 - val_loss: 175.3003\n",
      "Epoch 20/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 202.4214 - val_loss: 176.7587\n",
      "9/9 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43619, 40, 16) (43619, 1) (259, 40, 16)\n",
      "Epoch 1/20\n",
      "341/341 [==============================] - 4s 10ms/step - loss: 2291.9856 - val_loss: 1213.2866\n",
      "Epoch 2/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 498.0440 - val_loss: 221.4526\n",
      "Epoch 3/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 273.8747 - val_loss: 216.0187\n",
      "Epoch 4/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 257.3766 - val_loss: 222.0951\n",
      "Epoch 5/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 243.6585 - val_loss: 204.7415\n",
      "Epoch 6/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 237.9813 - val_loss: 201.8890\n",
      "Epoch 7/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 232.4224 - val_loss: 230.1372\n",
      "Epoch 8/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 231.1712 - val_loss: 195.0920\n",
      "Epoch 9/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 225.9395 - val_loss: 205.4482\n",
      "Epoch 10/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 222.0289 - val_loss: 192.8229\n",
      "Epoch 11/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 221.8868 - val_loss: 185.8038\n",
      "Epoch 12/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 221.3308 - val_loss: 182.8925\n",
      "Epoch 13/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 215.6891 - val_loss: 175.0577\n",
      "Epoch 14/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 214.5800 - val_loss: 175.4368\n",
      "Epoch 15/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 213.4915 - val_loss: 170.7048\n",
      "Epoch 16/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 212.8579 - val_loss: 181.9998\n",
      "Epoch 17/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 209.8317 - val_loss: 174.6894\n",
      "Epoch 18/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 209.2997 - val_loss: 173.6261\n",
      "Epoch 19/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 205.7202 - val_loss: 166.5505\n",
      "Epoch 20/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 206.6086 - val_loss: 177.0442\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "CPU times: user 5min 4s, sys: 32.5 s, total: 5min 36s\n",
      "Wall time: 5min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results_all002 = pd.DataFrame()\n",
    "for SEED in range(5):  \n",
    "    tf.random.set_seed(SEED)\n",
    "    mse = []\n",
    "    R2_val = []\n",
    "    RMSE = []\n",
    "    score_val = []\n",
    "    \n",
    "    # parameter's sample\n",
    "    # weights_file = \"weights_file_lstm_optimalmodel_all.h5\"\n",
    "    alpha = 0.2\n",
    "    sequence_length = 40\n",
    "    epochs = 20\n",
    "    nodes_per_layer = [32]\n",
    "    dropout = 0.1\n",
    "    activation = 'tanh'\n",
    "    batch_size = 128\n",
    "    remaining_sensors = remaining_sensors\n",
    "    # create model\n",
    "    input_shape = (sequence_length, len(remaining_sensors))\n",
    "    model = model_lstm_1layer(input_shape, nodes_per_layer[0], dropout, activation)\n",
    "    \n",
    "    # Data prepration\n",
    "    X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "\n",
    "    # create sequences train, test\n",
    "    train_array = gen_data_wrapper(X_train_interim, sequence_length,remaining_sensors)\n",
    "    label_array = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'])\n",
    "\n",
    "    test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,remaining_sensors, -99.))\n",
    "               for unit_nr in X_test_interim['Unit'].unique())\n",
    "    \n",
    "    test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "    test_rul = rul_piecewise_fct(y_test,rul_piecewise)\n",
    "    print(train_array.shape, label_array.shape, test_array.shape)\n",
    "            \n",
    "    # Model fitting\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        start_time = time.time()\n",
    "        model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n",
    "        weights_file = model.get_weights()\n",
    "        model.set_weights(weights_file)  # reset optimizer and node weights before every training iteration\n",
    "        history = model.fit(train_array, label_array,\n",
    "                                validation_data=(test_array, test_rul),\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                # callbacks=[cb],\n",
    "                                verbose=1)\n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "        mse.append(history.history['val_loss'][-1])\n",
    "\n",
    "        y_hat_val_split = model.predict(test_array)\n",
    "        R2_val.append(r2_score(test_rul, y_hat_val_split))\n",
    "        RMSE.append(np.sqrt(mean_squared_error(test_rul, y_hat_val_split)))\n",
    "        score_val.append(compute_s_score(test_rul, y_hat_val_split))\n",
    "            \n",
    "        \n",
    "    #  append results\n",
    "    d = {'RMSE' :np.mean(RMSE), 'std_RMSE' :np.std(RMSE),\n",
    "         'S_score' :np.mean(score_val), 'std_S_score' :np.std(score_val),\n",
    "         'MSE':np.mean(mse), 'std_MSE':np.std(mse),\n",
    "         'nodes':str(nodes_per_layer), 'dropout':dropout, \n",
    "         'activation':activation, 'batch_size':batch_size, 'TW' : sequence_length,\n",
    "         'time':training_time}\n",
    "\n",
    "#     results = results.append(pd.DataFrame(d, index=[0]), ignore_index=True)\n",
    "    results_all002 = pd.concat([results_all002, pd.DataFrame(d, index=[0])], ignore_index=True)\n",
    "    results_all002.to_csv('results/all/fd002.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>std_RMSE</th>\n",
       "      <th>S_score</th>\n",
       "      <th>std_S_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>TW</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.102145</td>\n",
       "      <td>0.0</td>\n",
       "      <td>825.334576</td>\n",
       "      <td>0.0</td>\n",
       "      <td>171.666199</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "      <td>40</td>\n",
       "      <td>60.258299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.228334</td>\n",
       "      <td>0.0</td>\n",
       "      <td>850.169654</td>\n",
       "      <td>0.0</td>\n",
       "      <td>174.988831</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "      <td>40</td>\n",
       "      <td>60.961792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.108823</td>\n",
       "      <td>0.0</td>\n",
       "      <td>824.245323</td>\n",
       "      <td>0.0</td>\n",
       "      <td>171.841248</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "      <td>40</td>\n",
       "      <td>60.634674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.295064</td>\n",
       "      <td>0.0</td>\n",
       "      <td>888.831273</td>\n",
       "      <td>0.0</td>\n",
       "      <td>176.758713</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "      <td>40</td>\n",
       "      <td>63.824795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.305796</td>\n",
       "      <td>0.0</td>\n",
       "      <td>685.164746</td>\n",
       "      <td>0.0</td>\n",
       "      <td>177.044189</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "      <td>40</td>\n",
       "      <td>65.198323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RMSE  std_RMSE     S_score  std_S_score         MSE  std_MSE nodes  \\\n",
       "0  13.102145       0.0  825.334576          0.0  171.666199      0.0  [32]   \n",
       "1  13.228334       0.0  850.169654          0.0  174.988831      0.0  [32]   \n",
       "2  13.108823       0.0  824.245323          0.0  171.841248      0.0  [32]   \n",
       "3  13.295064       0.0  888.831273          0.0  176.758713      0.0  [32]   \n",
       "4  13.305796       0.0  685.164746          0.0  177.044189      0.0  [32]   \n",
       "\n",
       "   dropout activation  batch_size  TW       time  \n",
       "0      0.1       tanh         128  40  60.258299  \n",
       "1      0.1       tanh         128  40  60.961792  \n",
       "2      0.1       tanh         128  40  60.634674  \n",
       "3      0.1       tanh         128  40  63.824795  \n",
       "4      0.1       tanh         128  40  65.198323  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_all002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FD003 <a class=\"anchor\" id=\"fd003\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53759, 27) (33991, 26) (259, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# Load data and preprocess\n",
    "train, test, y_test = prepare_data('FD002.txt')\n",
    "print(train.shape, test.shape, y_test.shape)\n",
    "sensor_names = ['T20','T24','T30','T50','P20','P15','P30','Nf','Nc','epr','Ps30','phi',\n",
    "                    'NRf','NRc','BPR','farB','htBleed','Nf_dmd','PCNfR_dmd','W31','W32']\n",
    "\n",
    "remaining_sensors = ['T24','T30','T50', 'P15', 'P30','Nf','Nc', 'epr','Ps30','phi',\n",
    "                    'NRf','NRc','BPR','htBleed','W31','W32']\n",
    "drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "\n",
    "rul_piecewise = 125\n",
    "train['RUL'].clip(upper=rul_piecewise, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44919, 35, 16) (44919, 1) (259, 35, 16)\n",
      "Epoch 1/20\n",
      "1404/1404 [==============================] - 12s 8ms/step - loss: 906.4077 - val_loss: 297.4064\n",
      "Epoch 2/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 278.6239 - val_loss: 234.8844\n",
      "Epoch 3/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 248.1741 - val_loss: 195.1236\n",
      "Epoch 4/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 234.3593 - val_loss: 199.3235\n",
      "Epoch 5/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 223.6005 - val_loss: 174.3159\n",
      "Epoch 6/20\n",
      "1404/1404 [==============================] - 9s 7ms/step - loss: 213.5243 - val_loss: 167.1391\n",
      "Epoch 7/20\n",
      "1404/1404 [==============================] - 9s 7ms/step - loss: 208.4235 - val_loss: 174.1897\n",
      "Epoch 8/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 201.9832 - val_loss: 174.3790\n",
      "Epoch 9/20\n",
      "1404/1404 [==============================] - 9s 7ms/step - loss: 196.2110 - val_loss: 172.3315\n",
      "Epoch 10/20\n",
      "1404/1404 [==============================] - 9s 7ms/step - loss: 191.1977 - val_loss: 181.4727\n",
      "Epoch 11/20\n",
      "1404/1404 [==============================] - 9s 7ms/step - loss: 189.1268 - val_loss: 188.0092\n",
      "Epoch 12/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 186.7239 - val_loss: 175.8956\n",
      "Epoch 13/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 182.8045 - val_loss: 166.8444\n",
      "Epoch 14/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 180.9116 - val_loss: 159.4391\n",
      "Epoch 15/20\n",
      "1404/1404 [==============================] - 9s 7ms/step - loss: 180.3442 - val_loss: 160.9109\n",
      "Epoch 16/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 177.5476 - val_loss: 170.4704\n",
      "Epoch 17/20\n",
      "1404/1404 [==============================] - 9s 7ms/step - loss: 175.6091 - val_loss: 161.2366\n",
      "Epoch 18/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 174.5745 - val_loss: 153.9965\n",
      "Epoch 19/20\n",
      "1404/1404 [==============================] - 9s 7ms/step - loss: 171.6616 - val_loss: 162.2866\n",
      "Epoch 20/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 171.5919 - val_loss: 152.8463\n",
      "9/9 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44919, 35, 16) (44919, 1) (259, 35, 16)\n",
      "Epoch 1/20\n",
      "1404/1404 [==============================] - 11s 7ms/step - loss: 830.6611 - val_loss: 235.3775\n",
      "Epoch 2/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 298.7138 - val_loss: 215.1112\n",
      "Epoch 3/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 257.1817 - val_loss: 185.4624\n",
      "Epoch 4/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 246.8819 - val_loss: 181.2589\n",
      "Epoch 5/20\n",
      "1404/1404 [==============================] - 9s 7ms/step - loss: 232.8512 - val_loss: 187.8301\n",
      "Epoch 6/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 223.6786 - val_loss: 175.8104\n",
      "Epoch 7/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 218.0415 - val_loss: 168.6120\n",
      "Epoch 8/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 208.3120 - val_loss: 187.1732\n",
      "Epoch 9/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 204.9613 - val_loss: 176.7870\n",
      "Epoch 10/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 197.0298 - val_loss: 166.7704\n",
      "Epoch 11/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 194.9357 - val_loss: 160.0524\n",
      "Epoch 12/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 191.5682 - val_loss: 183.8430\n",
      "Epoch 13/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 189.6020 - val_loss: 192.0444\n",
      "Epoch 14/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 185.8213 - val_loss: 160.9120\n",
      "Epoch 15/20\n",
      "1404/1404 [==============================] - 9s 7ms/step - loss: 184.1720 - val_loss: 158.0382\n",
      "Epoch 16/20\n",
      "1404/1404 [==============================] - 9s 7ms/step - loss: 181.9867 - val_loss: 163.7094\n",
      "Epoch 17/20\n",
      "1404/1404 [==============================] - 9s 7ms/step - loss: 179.2176 - val_loss: 167.3739\n",
      "Epoch 18/20\n",
      "1404/1404 [==============================] - 9s 7ms/step - loss: 177.5820 - val_loss: 164.0641\n",
      "Epoch 19/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 174.4505 - val_loss: 187.7096\n",
      "Epoch 20/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 173.0491 - val_loss: 160.8781\n",
      "9/9 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44919, 35, 16) (44919, 1) (259, 35, 16)\n",
      "Epoch 1/20\n",
      "1404/1404 [==============================] - 10s 6ms/step - loss: 928.9767 - val_loss: 240.5249\n",
      "Epoch 2/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 274.2685 - val_loss: 202.2992\n",
      "Epoch 3/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 240.3042 - val_loss: 182.1285\n",
      "Epoch 4/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 225.5036 - val_loss: 221.9795\n",
      "Epoch 5/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 219.4206 - val_loss: 181.7211\n",
      "Epoch 6/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 210.7127 - val_loss: 192.3570\n",
      "Epoch 7/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 207.0006 - val_loss: 181.0675\n",
      "Epoch 8/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 199.9305 - val_loss: 188.1906\n",
      "Epoch 9/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 195.2570 - val_loss: 171.5017\n",
      "Epoch 10/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 191.2260 - val_loss: 171.8849\n",
      "Epoch 11/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 186.9044 - val_loss: 165.6958\n",
      "Epoch 12/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 185.2535 - val_loss: 176.2680\n",
      "Epoch 13/20\n",
      "1404/1404 [==============================] - 9s 7ms/step - loss: 182.4928 - val_loss: 168.5497\n",
      "Epoch 14/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 181.0162 - val_loss: 182.0496\n",
      "Epoch 15/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 179.2506 - val_loss: 170.0747\n",
      "Epoch 16/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 176.4727 - val_loss: 175.8750\n",
      "Epoch 17/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 176.5207 - val_loss: 173.8904\n",
      "Epoch 18/20\n",
      "1404/1404 [==============================] - 9s 7ms/step - loss: 172.8772 - val_loss: 169.9757\n",
      "Epoch 19/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 172.9821 - val_loss: 179.7646\n",
      "Epoch 20/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 171.3277 - val_loss: 168.5291\n",
      "9/9 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44919, 35, 16) (44919, 1) (259, 35, 16)\n",
      "Epoch 1/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 877.4888 - val_loss: 323.8827\n",
      "Epoch 2/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 290.3685 - val_loss: 216.2555\n",
      "Epoch 3/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 252.0514 - val_loss: 206.6399\n",
      "Epoch 4/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 238.7656 - val_loss: 181.9300\n",
      "Epoch 5/20\n",
      "1404/1404 [==============================] - 9s 7ms/step - loss: 225.9218 - val_loss: 189.2038\n",
      "Epoch 6/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 216.4854 - val_loss: 188.2886\n",
      "Epoch 7/20\n",
      "1404/1404 [==============================] - 9s 7ms/step - loss: 209.0519 - val_loss: 174.5823\n",
      "Epoch 8/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 202.9315 - val_loss: 177.9902\n",
      "Epoch 9/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 197.5704 - val_loss: 161.3453\n",
      "Epoch 10/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 192.9104 - val_loss: 182.6534\n",
      "Epoch 11/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 188.9660 - val_loss: 168.6949\n",
      "Epoch 12/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 187.4769 - val_loss: 164.0001\n",
      "Epoch 13/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 181.6261 - val_loss: 161.6334\n",
      "Epoch 14/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 180.8196 - val_loss: 189.2018\n",
      "Epoch 15/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 178.0474 - val_loss: 156.7468\n",
      "Epoch 16/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 177.2024 - val_loss: 169.8159\n",
      "Epoch 17/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 176.1087 - val_loss: 159.9241\n",
      "Epoch 18/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 173.4106 - val_loss: 156.8228\n",
      "Epoch 19/20\n",
      "1404/1404 [==============================] - 9s 7ms/step - loss: 172.7243 - val_loss: 155.0491\n",
      "Epoch 20/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 171.7959 - val_loss: 162.9093\n",
      "9/9 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44919, 35, 16) (44919, 1) (259, 35, 16)\n",
      "Epoch 1/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 838.2953 - val_loss: 222.6092\n",
      "Epoch 2/20\n",
      "1404/1404 [==============================] - 9s 7ms/step - loss: 271.8162 - val_loss: 193.0789\n",
      "Epoch 3/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 244.3942 - val_loss: 183.2736\n",
      "Epoch 4/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 232.2839 - val_loss: 225.5190\n",
      "Epoch 5/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 222.7054 - val_loss: 188.9413\n",
      "Epoch 6/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 215.0832 - val_loss: 165.5778\n",
      "Epoch 7/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 208.8830 - val_loss: 173.6259\n",
      "Epoch 8/20\n",
      "1404/1404 [==============================] - 9s 7ms/step - loss: 201.7564 - val_loss: 165.5113\n",
      "Epoch 9/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 195.4309 - val_loss: 170.5264\n",
      "Epoch 10/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 194.4122 - val_loss: 166.8309\n",
      "Epoch 11/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 191.7360 - val_loss: 162.5114\n",
      "Epoch 12/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 185.5587 - val_loss: 156.0481\n",
      "Epoch 13/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 182.8754 - val_loss: 171.2062\n",
      "Epoch 14/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 181.9922 - val_loss: 152.7304\n",
      "Epoch 15/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 180.0750 - val_loss: 168.1986\n",
      "Epoch 16/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 176.5597 - val_loss: 178.8348\n",
      "Epoch 17/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 175.3523 - val_loss: 170.9589\n",
      "Epoch 18/20\n",
      "1404/1404 [==============================] - 11s 8ms/step - loss: 173.6092 - val_loss: 154.9990\n",
      "Epoch 19/20\n",
      "1404/1404 [==============================] - 9s 7ms/step - loss: 172.0032 - val_loss: 159.6801\n",
      "Epoch 20/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 171.2485 - val_loss: 161.9050\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "CPU times: user 15min 29s, sys: 57.9 s, total: 16min 26s\n",
      "Wall time: 15min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results_all003 = pd.DataFrame()\n",
    "for SEED in range(5):  \n",
    "    tf.random.set_seed(SEED)\n",
    "    mse = []\n",
    "    R2_val = []\n",
    "    RMSE = []\n",
    "    score_val = []\n",
    "    \n",
    "    # parameter's sample\n",
    "    # weights_file = \"weights_file_lstm_optimalmodel_all.h5\"\n",
    "    alpha = 0.1\n",
    "    sequence_length = 35\n",
    "    epochs = 20\n",
    "    nodes_per_layer = [64]\n",
    "    dropout = 0.2\n",
    "    activation = 'tanh'\n",
    "    batch_size = 32\n",
    "    remaining_sensors = remaining_sensors\n",
    "    # create model\n",
    "    input_shape = (sequence_length, len(remaining_sensors))\n",
    "    model = model_lstm_1layer(input_shape, nodes_per_layer[0], dropout,\n",
    "                             activation)\n",
    "    \n",
    "    # Data prepration\n",
    "    X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "\n",
    "    # create sequences train, test\n",
    "    train_array = gen_data_wrapper(X_train_interim, sequence_length,remaining_sensors)\n",
    "    label_array = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'])\n",
    "\n",
    "    test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,remaining_sensors, -99.))\n",
    "               for unit_nr in X_test_interim['Unit'].unique())\n",
    "    \n",
    "    test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "    test_rul = rul_piecewise_fct(y_test,rul_piecewise)\n",
    "    print(train_array.shape, label_array.shape, test_array.shape)\n",
    "            \n",
    "    # Model fitting\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        start_time = time.time()\n",
    "        model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n",
    "        weights_file = model.get_weights()\n",
    "        model.set_weights(weights_file)  # reset optimizer and node weights before every training iteration\n",
    "        history = model.fit(train_array, label_array,\n",
    "                                validation_data=(test_array, test_rul),\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                # callbacks=[cb],\n",
    "                                verbose=1)\n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "        mse.append(history.history['val_loss'][-1])\n",
    "\n",
    "        y_hat_val_split = model.predict(test_array)\n",
    "        R2_val.append(r2_score(test_rul, y_hat_val_split))\n",
    "        RMSE.append(np.sqrt(mean_squared_error(test_rul, y_hat_val_split)))\n",
    "        score_val.append(compute_s_score(test_rul, y_hat_val_split))\n",
    "            \n",
    "        \n",
    "    #  append results\n",
    "    d = {'RMSE' :np.mean(RMSE), 'std_RMSE' :np.std(RMSE),\n",
    "         'S_score' :np.mean(score_val), 'std_S_score' :np.std(score_val),\n",
    "         'MSE':np.mean(mse), 'std_MSE':np.std(mse),\n",
    "         'nodes':str(nodes_per_layer), 'dropout':dropout, \n",
    "         'activation':activation, 'batch_size':batch_size, 'TW' : sequence_length,\n",
    "         'time':training_time}\n",
    "\n",
    "#     results = results.append(pd.DataFrame(d, index=[0]), ignore_index=True)\n",
    "    results_all003 = pd.concat([results_all003, pd.DataFrame(d, index=[0])], ignore_index=True)\n",
    "    results_all003.to_csv('results/all/fd003.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>std_RMSE</th>\n",
       "      <th>S_score</th>\n",
       "      <th>std_S_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>TW</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.363102</td>\n",
       "      <td>0.0</td>\n",
       "      <td>667.250275</td>\n",
       "      <td>0.0</td>\n",
       "      <td>152.846313</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "      <td>190.864901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.683775</td>\n",
       "      <td>0.0</td>\n",
       "      <td>668.448743</td>\n",
       "      <td>0.0</td>\n",
       "      <td>160.878143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "      <td>183.677209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.981874</td>\n",
       "      <td>0.0</td>\n",
       "      <td>740.183392</td>\n",
       "      <td>0.0</td>\n",
       "      <td>168.529068</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "      <td>182.398291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.763592</td>\n",
       "      <td>0.0</td>\n",
       "      <td>795.465533</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.909286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "      <td>182.180160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.724188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>630.904330</td>\n",
       "      <td>0.0</td>\n",
       "      <td>161.904953</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "      <td>185.120741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RMSE  std_RMSE     S_score  std_S_score         MSE  std_MSE nodes  \\\n",
       "0  12.363102       0.0  667.250275          0.0  152.846313      0.0  [64]   \n",
       "1  12.683775       0.0  668.448743          0.0  160.878143      0.0  [64]   \n",
       "2  12.981874       0.0  740.183392          0.0  168.529068      0.0  [64]   \n",
       "3  12.763592       0.0  795.465533          0.0  162.909286      0.0  [64]   \n",
       "4  12.724188       0.0  630.904330          0.0  161.904953      0.0  [64]   \n",
       "\n",
       "   dropout activation  batch_size  TW        time  \n",
       "0      0.2       tanh          32  35  190.864901  \n",
       "1      0.2       tanh          32  35  183.677209  \n",
       "2      0.2       tanh          32  35  182.398291  \n",
       "3      0.2       tanh          32  35  182.180160  \n",
       "4      0.2       tanh          32  35  185.120741  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_all003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FD001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20631, 27) (13096, 26) (100, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# Load data and preprocess\n",
    "train, test, y_test = prepare_data('FD001.txt')\n",
    "print(train.shape, test.shape, y_test.shape)\n",
    "sensor_names = ['T20','T24','T30','T50','P20','P15','P30','Nf','Nc','epr','Ps30','phi',\n",
    "                    'NRf','NRc','BPR','farB','htBleed','Nf_dmd','PCNfR_dmd','W31','W32']\n",
    "remaining_sensors = ['T24','T30','T50', 'P15', 'P30','Nf','Nc','Ps30','phi',\n",
    "                    'NRf','NRc','BPR','htBleed','W31','W32']\n",
    "drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "\n",
    "rul_piecewise = 125\n",
    "train['RUL'].clip(upper=rul_piecewise, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17731, 30, 15) (17731, 1) (100, 30, 15)\n",
      "Epoch 1/20\n",
      "278/278 [==============================] - 6s 20ms/step - loss: 1945.8270 - val_loss: 1011.8328\n",
      "Epoch 2/20\n",
      "278/278 [==============================] - 5s 19ms/step - loss: 557.1711 - val_loss: 283.6362\n",
      "Epoch 3/20\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 336.0125 - val_loss: 402.8049\n",
      "Epoch 4/20\n",
      "278/278 [==============================] - 5s 19ms/step - loss: 256.9118 - val_loss: 234.6717\n",
      "Epoch 5/20\n",
      "278/278 [==============================] - 6s 22ms/step - loss: 240.9247 - val_loss: 188.8959\n",
      "Epoch 6/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 215.9529 - val_loss: 246.9015\n",
      "Epoch 7/20\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 212.4774 - val_loss: 200.7240\n",
      "Epoch 8/20\n",
      "278/278 [==============================] - 5s 16ms/step - loss: 196.8468 - val_loss: 287.7190\n",
      "Epoch 9/20\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 198.1949 - val_loss: 179.0840\n",
      "Epoch 10/20\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 189.8759 - val_loss: 183.7105\n",
      "Epoch 11/20\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 178.0664 - val_loss: 206.1296\n",
      "Epoch 12/20\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 180.6395 - val_loss: 204.1548\n",
      "Epoch 13/20\n",
      "278/278 [==============================] - 6s 20ms/step - loss: 176.2058 - val_loss: 185.8178\n",
      "Epoch 14/20\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 175.0867 - val_loss: 266.6446\n",
      "Epoch 15/20\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 181.0719 - val_loss: 195.0177\n",
      "Epoch 16/20\n",
      "278/278 [==============================] - 5s 20ms/step - loss: 174.0650 - val_loss: 225.4183\n",
      "Epoch 17/20\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 167.9206 - val_loss: 170.4974\n",
      "Epoch 18/20\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 169.1513 - val_loss: 168.4302\n",
      "Epoch 19/20\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 168.1328 - val_loss: 181.4885\n",
      "Epoch 20/20\n",
      "278/278 [==============================] - 5s 19ms/step - loss: 163.0819 - val_loss: 179.4211\n",
      "4/4 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17731, 30, 15) (17731, 1) (100, 30, 15)\n",
      "Epoch 1/20\n",
      "278/278 [==============================] - 6s 19ms/step - loss: 1952.8248 - val_loss: 989.5231\n",
      "Epoch 2/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 608.6642 - val_loss: 457.7419\n",
      "Epoch 3/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 358.4405 - val_loss: 682.9811\n",
      "Epoch 4/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 251.7239 - val_loss: 341.1137\n",
      "Epoch 5/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 211.5727 - val_loss: 217.0099\n",
      "Epoch 6/20\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 201.9030 - val_loss: 207.4494\n",
      "Epoch 7/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 192.0596 - val_loss: 207.3164\n",
      "Epoch 8/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 189.4069 - val_loss: 176.4110\n",
      "Epoch 9/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 183.2050 - val_loss: 192.1585\n",
      "Epoch 10/20\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 170.9815 - val_loss: 273.9358\n",
      "Epoch 11/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 172.2622 - val_loss: 175.6589\n",
      "Epoch 12/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 176.8715 - val_loss: 169.4872\n",
      "Epoch 13/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 172.5498 - val_loss: 159.8728\n",
      "Epoch 14/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 168.3288 - val_loss: 175.7836\n",
      "Epoch 15/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 165.6964 - val_loss: 230.1221\n",
      "Epoch 16/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 168.5158 - val_loss: 279.9387\n",
      "Epoch 17/20\n",
      "278/278 [==============================] - 5s 19ms/step - loss: 166.2818 - val_loss: 208.7392\n",
      "Epoch 18/20\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 165.4128 - val_loss: 167.0048\n",
      "Epoch 19/20\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 161.3411 - val_loss: 167.7612\n",
      "Epoch 20/20\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 160.7052 - val_loss: 173.7776\n",
      "4/4 [==============================] - 1s 12ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17731, 30, 15) (17731, 1) (100, 30, 15)\n",
      "Epoch 1/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 1969.4290 - val_loss: 1184.3573\n",
      "Epoch 2/20\n",
      "278/278 [==============================] - 6s 20ms/step - loss: 569.7576 - val_loss: 422.7188\n",
      "Epoch 3/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 306.3215 - val_loss: 222.5002\n",
      "Epoch 4/20\n",
      "278/278 [==============================] - 5s 16ms/step - loss: 235.5519 - val_loss: 214.6935\n",
      "Epoch 5/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 214.0097 - val_loss: 179.1317\n",
      "Epoch 6/20\n",
      "278/278 [==============================] - 5s 16ms/step - loss: 201.6206 - val_loss: 170.3593\n",
      "Epoch 7/20\n",
      "278/278 [==============================] - 5s 19ms/step - loss: 195.8453 - val_loss: 250.8557\n",
      "Epoch 8/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 191.1101 - val_loss: 202.9509\n",
      "Epoch 9/20\n",
      "278/278 [==============================] - 5s 16ms/step - loss: 186.9187 - val_loss: 181.4738\n",
      "Epoch 10/20\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 186.7070 - val_loss: 211.4318\n",
      "Epoch 11/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 185.1064 - val_loss: 229.2648\n",
      "Epoch 12/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 184.7687 - val_loss: 178.2850\n",
      "Epoch 13/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 177.5807 - val_loss: 234.1747\n",
      "Epoch 14/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 181.5688 - val_loss: 163.8609\n",
      "Epoch 15/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 178.6478 - val_loss: 187.1144\n",
      "Epoch 16/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 170.8074 - val_loss: 174.3833\n",
      "Epoch 17/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 169.8305 - val_loss: 236.5552\n",
      "Epoch 18/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 174.3710 - val_loss: 168.9100\n",
      "Epoch 19/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 166.4170 - val_loss: 193.3184\n",
      "Epoch 20/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 163.4410 - val_loss: 180.6208\n",
      "4/4 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17731, 30, 15) (17731, 1) (100, 30, 15)\n",
      "Epoch 1/20\n",
      "278/278 [==============================] - 6s 18ms/step - loss: 1981.1896 - val_loss: 917.8284\n",
      "Epoch 2/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 589.6582 - val_loss: 387.9435\n",
      "Epoch 3/20\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 354.6008 - val_loss: 215.8253\n",
      "Epoch 4/20\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 249.8626 - val_loss: 274.0669\n",
      "Epoch 5/20\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 218.3911 - val_loss: 303.3423\n",
      "Epoch 6/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 212.5883 - val_loss: 251.4697\n",
      "Epoch 7/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 202.4792 - val_loss: 616.0234\n",
      "Epoch 8/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 196.4964 - val_loss: 172.3805\n",
      "Epoch 9/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 184.9444 - val_loss: 233.0498\n",
      "Epoch 10/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 184.0321 - val_loss: 215.6457\n",
      "Epoch 11/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 176.5638 - val_loss: 175.0033\n",
      "Epoch 12/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 173.6998 - val_loss: 297.9556\n",
      "Epoch 13/20\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 170.6276 - val_loss: 284.2820\n",
      "Epoch 14/20\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 174.1343 - val_loss: 208.6004\n",
      "Epoch 15/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 181.9589 - val_loss: 281.7798\n",
      "Epoch 16/20\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 167.0072 - val_loss: 177.9913\n",
      "Epoch 17/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 162.8726 - val_loss: 185.2022\n",
      "Epoch 18/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 165.6644 - val_loss: 230.4792\n",
      "Epoch 19/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 163.3365 - val_loss: 170.9268\n",
      "Epoch 20/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 160.3434 - val_loss: 168.3098\n",
      "4/4 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17731, 30, 15) (17731, 1) (100, 30, 15)\n",
      "Epoch 1/20\n",
      "278/278 [==============================] - 6s 19ms/step - loss: 2036.7043 - val_loss: 1121.9376\n",
      "Epoch 2/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 604.8963 - val_loss: 474.9707\n",
      "Epoch 3/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 448.0006 - val_loss: 532.3290\n",
      "Epoch 4/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 276.0841 - val_loss: 333.5952\n",
      "Epoch 5/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 227.4386 - val_loss: 538.3504\n",
      "Epoch 6/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 220.6311 - val_loss: 231.7316\n",
      "Epoch 7/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 198.7297 - val_loss: 169.9673\n",
      "Epoch 8/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 197.7237 - val_loss: 208.0873\n",
      "Epoch 9/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 189.4026 - val_loss: 173.8299\n",
      "Epoch 10/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 187.4878 - val_loss: 172.0346\n",
      "Epoch 11/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 185.7192 - val_loss: 267.2697\n",
      "Epoch 12/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 184.6438 - val_loss: 183.5157\n",
      "Epoch 13/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 180.3916 - val_loss: 245.7163\n",
      "Epoch 14/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 177.2476 - val_loss: 179.5717\n",
      "Epoch 15/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 174.1862 - val_loss: 173.5979\n",
      "Epoch 16/20\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 166.0173 - val_loss: 196.4699\n",
      "Epoch 17/20\n",
      "278/278 [==============================] - 5s 18ms/step - loss: 171.0186 - val_loss: 204.4547\n",
      "Epoch 18/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 166.2104 - val_loss: 258.3325\n",
      "Epoch 19/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 169.3962 - val_loss: 173.0251\n",
      "Epoch 20/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 162.3917 - val_loss: 203.0011\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "CPU times: user 10min 47s, sys: 2min 16s, total: 13min 3s\n",
      "Wall time: 8min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results_all001 = pd.DataFrame()\n",
    "for SEED in range(5):  \n",
    "    set_seed(SEED)\n",
    "    mse = []\n",
    "    R2_val = []\n",
    "    RMSE = []\n",
    "    score_val = []\n",
    "    \n",
    "    # parameter's sample\n",
    "    # weights_file = \"weights_file_lstm_optimalmodel_all.h5\"\n",
    "    alpha = 0.1\n",
    "    sequence_length = 30\n",
    "    epochs = 20\n",
    "    nodes_per_layer = [128]\n",
    "    dropout = 0.2\n",
    "    activation = 'tanh'\n",
    "    batch_size = 64\n",
    "    remaining_sensors = remaining_sensors\n",
    "    # create model\n",
    "    input_shape = (sequence_length, len(remaining_sensors))\n",
    "    model = model_lstm_1layer(input_shape, nodes_per_layer[0], dropout,\n",
    "                             activation)\n",
    "    \n",
    "    # Data prepration\n",
    "    X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "\n",
    "    # create sequences train, test\n",
    "    train_array = gen_data_wrapper(X_train_interim, sequence_length,remaining_sensors)\n",
    "    label_array = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'])\n",
    "\n",
    "    test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,remaining_sensors, -99.))\n",
    "               for unit_nr in X_test_interim['Unit'].unique())\n",
    "    \n",
    "    test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "    test_rul = rul_piecewise_fct(y_test,rul_piecewise)\n",
    "    print(train_array.shape, label_array.shape, test_array.shape)\n",
    "            \n",
    "    # Model fitting\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        start_time = time.time()\n",
    "        model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n",
    "        weights_file = model.get_weights()\n",
    "        model.set_weights(weights_file)  # reset optimizer and node weights before every training iteration\n",
    "        history = model.fit(train_array, label_array,\n",
    "                                validation_data=(test_array, test_rul),\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                # callbacks=[cb],\n",
    "                                verbose=1)\n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "        mse.append(history.history['val_loss'][-1])\n",
    "\n",
    "        y_hat_val_split = model.predict(test_array)\n",
    "        R2_val.append(r2_score(test_rul, y_hat_val_split))\n",
    "        RMSE.append(np.sqrt(mean_squared_error(test_rul, y_hat_val_split)))\n",
    "        score_val.append(compute_s_score(test_rul, y_hat_val_split))\n",
    "            \n",
    "        \n",
    "    #  append results\n",
    "    d = {'RMSE' :np.mean(RMSE), 'std_RMSE' :np.std(RMSE),\n",
    "         'S_score' :np.mean(score_val), 'std_S_score' :np.std(score_val),\n",
    "         'MSE':np.mean(mse), 'std_MSE':np.std(mse),\n",
    "         'nodes':str(nodes_per_layer), 'dropout':dropout, \n",
    "         'activation':activation, 'batch_size':batch_size, 'TW' : sequence_length,\n",
    "         'time':training_time}\n",
    "\n",
    "#     results = results.append(pd.DataFrame(d, index=[0]), ignore_index=True)\n",
    "    results_all001 = pd.concat([results_all001, pd.DataFrame(d, index=[0])], ignore_index=True)\n",
    "    results_all001.to_csv('results/all/fd001.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
