{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "from lime.lime_tabular import RecurrentTabularExplainer\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error, r2_score \n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn import preprocessing\n",
    "from keras import backend as K\n",
    "from sklearn.preprocessing import MinMaxScaler , StandardScaler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Activation, GRU\n",
    "from scipy import optimize\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "\n",
    "from sp_modif.model_function import *\n",
    "from sp_modif.methods import *\n",
    "from sp_modif.data_prep import *\n",
    "from sp_modif.evaluator import *\n",
    "from sp_modif.SHAP import *\n",
    "from sp_modif.L2X import *\n",
    "from methods import *\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEED = 0\n",
    "def set_seed(seed=SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    tf.random.set_seed(SEED)\n",
    "\n",
    "# Appeler la fonction pour fixer le seed\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61249, 27) (41214, 26) (248, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# Load data and preprocess\n",
    "train, test, y_test = prepare_data('FD004.txt')\n",
    "print(train.shape, test.shape, y_test.shape)\n",
    "sensor_names = ['T20','T24','T30','T50','P20','P15','P30','Nf','Nc','epr','Ps30','phi',\n",
    "                    'NRf','NRc','BPR','farB','htBleed','Nf_dmd','PCNfR_dmd','W31','W32']\n",
    "\n",
    "remaining_sensors = ['T24','T30','T50', 'P15', 'P30','Nf','Nc', 'epr','Ps30','phi',\n",
    "                    'NRf','NRc','BPR', 'farB','htBleed','W31','W32']\n",
    "drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "\n",
    "rul_piecewise = 120\n",
    "train['RUL'].clip(upper=rul_piecewise, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_lstm_1layer(input_shape, nodes_per_layer, dropout, activation):\n",
    "    \n",
    "    cb = keras.callbacks.EarlyStopping(monitor='loss', patience=4)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units = nodes_per_layer, activation=activation, \n",
    "                  input_shape=input_shape))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(256))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer=Adam(learning_rate=0.001))\n",
    "    # model.save_weights(weights_file)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51538, 40, 17) (51538, 1) (248, 40, 17)\n",
      "Epoch 1/20\n",
      "1611/1611 [==============================] - 13s 8ms/step - loss: 890.6100 - val_loss: 475.7899\n",
      "Epoch 2/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 326.5423 - val_loss: 272.6232\n",
      "Epoch 3/20\n",
      "1611/1611 [==============================] - 12s 8ms/step - loss: 283.3440 - val_loss: 248.6553\n",
      "Epoch 4/20\n",
      "1611/1611 [==============================] - 12s 8ms/step - loss: 263.2531 - val_loss: 267.4348\n",
      "Epoch 5/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 246.8677 - val_loss: 244.3410\n",
      "Epoch 6/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 229.2129 - val_loss: 284.2966\n",
      "Epoch 7/20\n",
      "1611/1611 [==============================] - 13s 8ms/step - loss: 216.3931 - val_loss: 273.7994\n",
      "Epoch 8/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 207.4074 - val_loss: 224.3848\n",
      "Epoch 9/20\n",
      "1611/1611 [==============================] - 12s 8ms/step - loss: 199.3400 - val_loss: 221.5757\n",
      "Epoch 10/20\n",
      "1611/1611 [==============================] - 12s 8ms/step - loss: 195.4732 - val_loss: 212.8451\n",
      "Epoch 11/20\n",
      "1611/1611 [==============================] - 14s 9ms/step - loss: 190.1536 - val_loss: 190.4642\n",
      "Epoch 12/20\n",
      "1611/1611 [==============================] - 12s 8ms/step - loss: 188.8325 - val_loss: 218.4247\n",
      "Epoch 13/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 183.9026 - val_loss: 195.0005\n",
      "Epoch 14/20\n",
      "1611/1611 [==============================] - 12s 8ms/step - loss: 181.5426 - val_loss: 191.3546\n",
      "Epoch 15/20\n",
      "1611/1611 [==============================] - 12s 8ms/step - loss: 179.9852 - val_loss: 192.9463\n",
      "Epoch 16/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 177.3543 - val_loss: 187.5354\n",
      "Epoch 17/20\n",
      "1611/1611 [==============================] - 12s 8ms/step - loss: 175.8430 - val_loss: 197.3994\n",
      "Epoch 18/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 174.5192 - val_loss: 201.2247\n",
      "Epoch 19/20\n",
      "1611/1611 [==============================] - 12s 8ms/step - loss: 172.6689 - val_loss: 202.8077\n",
      "Epoch 20/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 171.1598 - val_loss: 195.4058\n",
      "8/8 [==============================] - 0s 4ms/step\n",
      "(51538, 40, 17) (51538, 1) (248, 40, 17)\n",
      "Epoch 1/20\n",
      "1611/1611 [==============================] - 14s 8ms/step - loss: 907.9397 - val_loss: 351.7355\n",
      "Epoch 2/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 325.9836 - val_loss: 281.2424\n",
      "Epoch 3/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 287.9590 - val_loss: 272.4932\n",
      "Epoch 4/20\n",
      "1611/1611 [==============================] - 12s 8ms/step - loss: 263.3506 - val_loss: 245.7258\n",
      "Epoch 5/20\n",
      "1611/1611 [==============================] - 13s 8ms/step - loss: 245.3058 - val_loss: 235.8599\n",
      "Epoch 6/20\n",
      "1611/1611 [==============================] - 12s 8ms/step - loss: 228.2294 - val_loss: 229.4239\n",
      "Epoch 7/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 215.2952 - val_loss: 191.7772\n",
      "Epoch 8/20\n",
      "1611/1611 [==============================] - 12s 8ms/step - loss: 206.0288 - val_loss: 206.2485\n",
      "Epoch 9/20\n",
      "1611/1611 [==============================] - 13s 8ms/step - loss: 199.6609 - val_loss: 194.2472\n",
      "Epoch 10/20\n",
      "1611/1611 [==============================] - 12s 8ms/step - loss: 193.6101 - val_loss: 181.7888\n",
      "Epoch 11/20\n",
      "1611/1611 [==============================] - 12s 8ms/step - loss: 190.8133 - val_loss: 186.9317\n",
      "Epoch 12/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 188.1528 - val_loss: 179.1124\n",
      "Epoch 13/20\n",
      "1611/1611 [==============================] - 12s 8ms/step - loss: 185.3014 - val_loss: 190.3025\n",
      "Epoch 14/20\n",
      "1611/1611 [==============================] - 12s 8ms/step - loss: 183.9024 - val_loss: 181.3515\n",
      "Epoch 15/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 182.6747 - val_loss: 172.8399\n",
      "Epoch 16/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 180.0094 - val_loss: 178.2898\n",
      "Epoch 17/20\n",
      "1611/1611 [==============================] - 12s 8ms/step - loss: 178.4022 - val_loss: 178.7921\n",
      "Epoch 18/20\n",
      "1611/1611 [==============================] - 12s 8ms/step - loss: 176.8000 - val_loss: 186.8902\n",
      "Epoch 19/20\n",
      "1611/1611 [==============================] - 13s 8ms/step - loss: 176.5778 - val_loss: 186.0584\n",
      "Epoch 20/20\n",
      "1611/1611 [==============================] - 12s 8ms/step - loss: 174.3748 - val_loss: 179.7981\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "(51538, 40, 17) (51538, 1) (248, 40, 17)\n",
      "Epoch 1/20\n",
      "1611/1611 [==============================] - 14s 8ms/step - loss: 918.8009 - val_loss: 404.8393\n",
      "Epoch 2/20\n",
      "1611/1611 [==============================] - 12s 8ms/step - loss: 336.9583 - val_loss: 287.5470\n",
      "Epoch 3/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 286.1130 - val_loss: 297.1749\n",
      "Epoch 4/20\n",
      "1611/1611 [==============================] - 12s 8ms/step - loss: 269.0158 - val_loss: 268.0314\n",
      "Epoch 5/20\n",
      "1611/1611 [==============================] - 12s 8ms/step - loss: 249.2160 - val_loss: 273.7767\n",
      "Epoch 6/20\n",
      "1611/1611 [==============================] - 12s 7ms/step - loss: 237.9058 - val_loss: 307.2651\n",
      "Epoch 7/20\n",
      "1611/1611 [==============================] - 9s 6ms/step - loss: 225.2753 - val_loss: 323.0816\n",
      "Epoch 8/20\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 214.4031 - val_loss: 282.6369\n",
      "Epoch 9/20\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 204.2761 - val_loss: 277.7866\n",
      "Epoch 10/20\n",
      "1611/1611 [==============================] - 9s 6ms/step - loss: 196.6767 - val_loss: 246.0267\n",
      "Epoch 11/20\n",
      "1611/1611 [==============================] - 9s 6ms/step - loss: 192.9408 - val_loss: 245.7401\n",
      "Epoch 12/20\n",
      "1611/1611 [==============================] - 10s 6ms/step - loss: 188.7905 - val_loss: 234.5207\n",
      "Epoch 13/20\n",
      "1611/1611 [==============================] - 9s 6ms/step - loss: 185.5654 - val_loss: 228.7337\n",
      "Epoch 14/20\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 182.4359 - val_loss: 226.1045\n",
      "Epoch 15/20\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 180.8267 - val_loss: 240.7004\n",
      "Epoch 16/20\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 180.4777 - val_loss: 215.2132\n",
      "Epoch 17/20\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 176.4889 - val_loss: 215.8995\n",
      "Epoch 18/20\n",
      "1611/1611 [==============================] - 9s 6ms/step - loss: 175.7762 - val_loss: 196.8721\n",
      "Epoch 19/20\n",
      "1611/1611 [==============================] - 9s 6ms/step - loss: 173.9647 - val_loss: 199.7266\n",
      "Epoch 20/20\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 173.6041 - val_loss: 209.5043\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "(51538, 40, 17) (51538, 1) (248, 40, 17)\n",
      "Epoch 1/20\n",
      "1611/1611 [==============================] - 10s 5ms/step - loss: 834.0012 - val_loss: 363.4899\n",
      "Epoch 2/20\n",
      "1611/1611 [==============================] - 9s 6ms/step - loss: 331.4420 - val_loss: 301.8438\n",
      "Epoch 3/20\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 288.2322 - val_loss: 293.3867\n",
      "Epoch 4/20\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 269.9775 - val_loss: 255.3262\n",
      "Epoch 5/20\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 256.8059 - val_loss: 284.9621\n",
      "Epoch 6/20\n",
      "1611/1611 [==============================] - 9s 6ms/step - loss: 240.8896 - val_loss: 238.4052\n",
      "Epoch 7/20\n",
      "1611/1611 [==============================] - 9s 6ms/step - loss: 230.5095 - val_loss: 230.2475\n",
      "Epoch 8/20\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 222.2730 - val_loss: 272.7285\n",
      "Epoch 9/20\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 213.8104 - val_loss: 214.8833\n",
      "Epoch 10/20\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 207.3825 - val_loss: 232.3595\n",
      "Epoch 11/20\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 201.6070 - val_loss: 254.3529\n",
      "Epoch 12/20\n",
      "1611/1611 [==============================] - 9s 6ms/step - loss: 195.6733 - val_loss: 209.0304\n",
      "Epoch 13/20\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 190.9456 - val_loss: 198.5160\n",
      "Epoch 14/20\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 189.3754 - val_loss: 194.7478\n",
      "Epoch 15/20\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 185.9479 - val_loss: 185.0119\n",
      "Epoch 16/20\n",
      "1611/1611 [==============================] - 9s 6ms/step - loss: 183.5604 - val_loss: 200.6177\n",
      "Epoch 17/20\n",
      "1611/1611 [==============================] - 9s 6ms/step - loss: 182.1541 - val_loss: 179.4189\n",
      "Epoch 18/20\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 179.2463 - val_loss: 191.2955\n",
      "Epoch 19/20\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 177.3300 - val_loss: 189.9267\n",
      "Epoch 20/20\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 174.8044 - val_loss: 197.5988\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "(51538, 40, 17) (51538, 1) (248, 40, 17)\n",
      "Epoch 1/20\n",
      "1611/1611 [==============================] - 10s 5ms/step - loss: 866.1832 - val_loss: 382.5016\n",
      "Epoch 2/20\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 347.1746 - val_loss: 302.5602\n",
      "Epoch 3/20\n",
      "1611/1611 [==============================] - 9s 6ms/step - loss: 286.5663 - val_loss: 282.5158\n",
      "Epoch 4/20\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 265.8542 - val_loss: 292.7091\n",
      "Epoch 5/20\n",
      "1611/1611 [==============================] - 10s 6ms/step - loss: 242.1989 - val_loss: 221.4620\n",
      "Epoch 6/20\n",
      "1611/1611 [==============================] - 9s 6ms/step - loss: 228.9058 - val_loss: 236.7717\n",
      "Epoch 7/20\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 215.9624 - val_loss: 209.2911\n",
      "Epoch 8/20\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 208.6221 - val_loss: 199.8043\n",
      "Epoch 9/20\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 199.9875 - val_loss: 216.0540\n",
      "Epoch 10/20\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 193.5286 - val_loss: 195.2257\n",
      "Epoch 11/20\n",
      "1611/1611 [==============================] - 9s 6ms/step - loss: 189.2888 - val_loss: 186.1788\n",
      "Epoch 12/20\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 185.0451 - val_loss: 190.1372\n",
      "Epoch 13/20\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 183.3298 - val_loss: 177.2440\n",
      "Epoch 14/20\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 180.4638 - val_loss: 182.5235\n",
      "Epoch 15/20\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 178.6072 - val_loss: 176.6330\n",
      "Epoch 16/20\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 177.2566 - val_loss: 171.7110\n",
      "Epoch 17/20\n",
      "1611/1611 [==============================] - 9s 6ms/step - loss: 176.0098 - val_loss: 185.7031\n",
      "Epoch 18/20\n",
      "1611/1611 [==============================] - 9s 6ms/step - loss: 172.8228 - val_loss: 179.2506\n",
      "Epoch 19/20\n",
      "1611/1611 [==============================] - 9s 5ms/step - loss: 170.9572 - val_loss: 176.2312\n",
      "Epoch 20/20\n",
      "1611/1611 [==============================] - 8s 5ms/step - loss: 171.2446 - val_loss: 171.2723\n",
      "8/8 [==============================] - 0s 3ms/step\n",
      "CPU times: total: 3min 49s\n",
      "Wall time: 17min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results_all = pd.DataFrame()\n",
    "for SEED in range(5):  \n",
    "    tf.random.set_seed(SEED)\n",
    "    mse = []\n",
    "    R2_val = []\n",
    "    RMSE = []\n",
    "    score_val = []\n",
    "    \n",
    "    # 0.20\t[64]\t0.3\ttanh\t32\t25\n",
    "    \n",
    "    # parameter's sample\n",
    "    # weights_file = \"weights_file_lstm_optimalmodel_all.h5\"\n",
    "    alpha = 0.3\n",
    "    sequence_length = 40\n",
    "    epochs = 20\n",
    "    nodes_per_layer = [64]\n",
    "    dropout = 0.2\n",
    "    activation = 'tanh'\n",
    "    batch_size = 32\n",
    "    remaining_sensors = remaining_sensors\n",
    "    # create model\n",
    "    input_shape = (sequence_length, len(remaining_sensors))\n",
    "    model = model_lstm_1layer(input_shape, nodes_per_layer[0], dropout, activation)\n",
    "    \n",
    "    # Data prepration\n",
    "    X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "\n",
    "    # create sequences train, test\n",
    "    train_array = gen_data_wrapper(X_train_interim, sequence_length,remaining_sensors)\n",
    "    label_array = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'])\n",
    "\n",
    "    test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,remaining_sensors, -99.))\n",
    "               for unit_nr in X_test_interim['Unit'].unique())\n",
    "    \n",
    "    test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "    test_rul = rul_piecewise_fct(y_test,rul_piecewise)\n",
    "    print(train_array.shape, label_array.shape, test_array.shape)\n",
    "            \n",
    "    # Model fitting\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        start_time = time.time()\n",
    "        model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n",
    "        weights_file = model.get_weights()\n",
    "        model.set_weights(weights_file)  # reset optimizer and node weights before every training iteration\n",
    "        history = model.fit(train_array, label_array,\n",
    "                                validation_data=(test_array, test_rul),\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                # callbacks=[cb],\n",
    "                                verbose=1)\n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "        mse.append(history.history['val_loss'][-1])\n",
    "\n",
    "        y_hat_val_split = model.predict(test_array)\n",
    "        R2_val.append(r2_score(test_rul, y_hat_val_split))\n",
    "        RMSE.append(np.sqrt(mean_squared_error(test_rul, y_hat_val_split)))\n",
    "        score_val.append(compute_s_score(test_rul, y_hat_val_split))\n",
    "            \n",
    "        \n",
    "    #  append results\n",
    "    d = {'RMSE' :np.mean(RMSE), 'std_RMSE' :np.std(RMSE),\n",
    "         'S_score' :np.mean(score_val), 'std_S_score' :np.std(score_val),\n",
    "         'MSE':np.mean(mse), 'std_MSE':np.std(mse),\n",
    "         'nodes':str(nodes_per_layer), 'dropout':dropout, \n",
    "         'activation':activation, 'batch_size':batch_size, 'TW' : sequence_length,\n",
    "         'time':training_time}\n",
    "\n",
    "#     results = results.append(pd.DataFrame(d, index=[0]), ignore_index=True)\n",
    "    results_all = pd.concat([results_all, pd.DataFrame(d, index=[0])], ignore_index=True)\n",
    "    results_all.to_csv('results/all/fd004.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>std_RMSE</th>\n",
       "      <th>S_score</th>\n",
       "      <th>std_S_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>TW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.978763</td>\n",
       "      <td>0.0</td>\n",
       "      <td>942.298756</td>\n",
       "      <td>0.0</td>\n",
       "      <td>195.405823</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.408881</td>\n",
       "      <td>0.0</td>\n",
       "      <td>893.612929</td>\n",
       "      <td>0.0</td>\n",
       "      <td>179.798080</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.474265</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1015.154789</td>\n",
       "      <td>0.0</td>\n",
       "      <td>209.504333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.056982</td>\n",
       "      <td>0.0</td>\n",
       "      <td>756.770995</td>\n",
       "      <td>0.0</td>\n",
       "      <td>197.598755</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.087103</td>\n",
       "      <td>0.0</td>\n",
       "      <td>766.305731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>171.272263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RMSE  std_RMSE      S_score  std_S_score         MSE  std_MSE nodes  \\\n",
       "0  13.978763       0.0   942.298756          0.0  195.405823      0.0  [64]   \n",
       "1  13.408881       0.0   893.612929          0.0  179.798080      0.0  [64]   \n",
       "2  14.474265       0.0  1015.154789          0.0  209.504333      0.0  [64]   \n",
       "3  14.056982       0.0   756.770995          0.0  197.598755      0.0  [64]   \n",
       "4  13.087103       0.0   766.305731          0.0  171.272263      0.0  [64]   \n",
       "\n",
       "   dropout activation  batch_size  TW  \n",
       "0      0.2       tanh          32  40  \n",
       "1      0.2       tanh          32  40  \n",
       "2      0.2       tanh          32  40  \n",
       "3      0.2       tanh          32  40  \n",
       "4      0.2       tanh          32  40  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FD002 <a class=\"anchor\" id=\"fd002\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53759, 27) (33991, 26) (259, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# Load data and preprocess\n",
    "train, test, y_test = prepare_data('FD002.txt')\n",
    "print(train.shape, test.shape, y_test.shape)\n",
    "sensor_names = ['T20','T24','T30','T50','P20','P15','P30','Nf','Nc','epr','Ps30','phi',\n",
    "                    'NRf','NRc','BPR','farB','htBleed','Nf_dmd','PCNfR_dmd','W31','W32']\n",
    "\n",
    "remaining_sensors = ['T24','T30','T50', 'P15', 'P30','Nf','Nc', 'epr','Ps30','phi',\n",
    "                    'NRf','NRc','BPR','htBleed','W31','W32']\n",
    "drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "\n",
    "rul_piecewise = 125\n",
    "train['RUL'].clip(upper=rul_piecewise, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43619, 40, 16) (43619, 1) (259, 40, 16)\n",
      "Epoch 1/20\n",
      "341/341 [==============================] - 5s 11ms/step - loss: 2299.9543 - val_loss: 1355.9552\n",
      "Epoch 2/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 445.4123 - val_loss: 230.4746\n",
      "Epoch 3/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 272.6965 - val_loss: 213.4318\n",
      "Epoch 4/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 250.4840 - val_loss: 199.3014\n",
      "Epoch 5/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 241.4520 - val_loss: 191.6943\n",
      "Epoch 6/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 235.6269 - val_loss: 194.9564\n",
      "Epoch 7/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 231.8452 - val_loss: 183.0522\n",
      "Epoch 8/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 229.6745 - val_loss: 196.7726\n",
      "Epoch 9/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 222.4664 - val_loss: 195.1641\n",
      "Epoch 10/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 223.1750 - val_loss: 185.2132\n",
      "Epoch 11/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 219.1122 - val_loss: 179.7257\n",
      "Epoch 12/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 218.8516 - val_loss: 181.6497\n",
      "Epoch 13/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 215.3036 - val_loss: 171.9112\n",
      "Epoch 14/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 212.3706 - val_loss: 171.0079\n",
      "Epoch 15/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 210.6467 - val_loss: 177.7342\n",
      "Epoch 16/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 208.1605 - val_loss: 175.0124\n",
      "Epoch 17/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 206.8861 - val_loss: 184.8799\n",
      "Epoch 18/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 204.5631 - val_loss: 164.3059\n",
      "Epoch 19/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 202.6550 - val_loss: 169.2653\n",
      "Epoch 20/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 203.9782 - val_loss: 178.8925\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "(43619, 40, 16) (43619, 1) (259, 40, 16)\n",
      "Epoch 1/20\n",
      "341/341 [==============================] - 4s 10ms/step - loss: 2232.0757 - val_loss: 704.6970\n",
      "Epoch 2/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 443.6121 - val_loss: 244.0991\n",
      "Epoch 3/20\n",
      "341/341 [==============================] - 4s 10ms/step - loss: 287.9110 - val_loss: 212.7909\n",
      "Epoch 4/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 264.4105 - val_loss: 189.5235\n",
      "Epoch 5/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 248.6176 - val_loss: 220.7590\n",
      "Epoch 6/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 243.5679 - val_loss: 184.4724\n",
      "Epoch 7/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 237.9630 - val_loss: 184.1959\n",
      "Epoch 8/20\n",
      "341/341 [==============================] - 4s 10ms/step - loss: 235.3258 - val_loss: 185.4442\n",
      "Epoch 9/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 233.8174 - val_loss: 184.8150\n",
      "Epoch 10/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 229.9077 - val_loss: 180.9779\n",
      "Epoch 11/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 228.2594 - val_loss: 177.0404\n",
      "Epoch 12/20\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 224.7370 - val_loss: 174.8084\n",
      "Epoch 13/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 222.5493 - val_loss: 179.2083\n",
      "Epoch 14/20\n",
      "341/341 [==============================] - 4s 10ms/step - loss: 219.8665 - val_loss: 182.8904\n",
      "Epoch 15/20\n",
      "341/341 [==============================] - 4s 12ms/step - loss: 218.4063 - val_loss: 174.6106\n",
      "Epoch 16/20\n",
      "341/341 [==============================] - 4s 11ms/step - loss: 216.7654 - val_loss: 174.2862\n",
      "Epoch 17/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 215.4766 - val_loss: 175.5257\n",
      "Epoch 18/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 214.2859 - val_loss: 178.5487\n",
      "Epoch 19/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 211.5074 - val_loss: 173.6649\n",
      "Epoch 20/20\n",
      "341/341 [==============================] - 4s 12ms/step - loss: 211.6954 - val_loss: 186.0008\n",
      "9/9 [==============================] - 0s 5ms/step\n",
      "(43619, 40, 16) (43619, 1) (259, 40, 16)\n",
      "Epoch 1/20\n",
      "341/341 [==============================] - 6s 12ms/step - loss: 2324.2356 - val_loss: 1511.8579\n",
      "Epoch 2/20\n",
      "341/341 [==============================] - 3s 8ms/step - loss: 546.0389 - val_loss: 230.0466\n",
      "Epoch 3/20\n",
      "341/341 [==============================] - 4s 12ms/step - loss: 281.8280 - val_loss: 201.8108\n",
      "Epoch 4/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 264.1778 - val_loss: 194.8761\n",
      "Epoch 5/20\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 247.6445 - val_loss: 179.8723\n",
      "Epoch 6/20\n",
      "341/341 [==============================] - 4s 11ms/step - loss: 239.8680 - val_loss: 178.1005\n",
      "Epoch 7/20\n",
      "341/341 [==============================] - 4s 11ms/step - loss: 234.3060 - val_loss: 176.5033\n",
      "Epoch 8/20\n",
      "341/341 [==============================] - 4s 11ms/step - loss: 230.7732 - val_loss: 193.1380\n",
      "Epoch 9/20\n",
      "341/341 [==============================] - 4s 11ms/step - loss: 225.0836 - val_loss: 176.3103\n",
      "Epoch 10/20\n",
      "341/341 [==============================] - 4s 12ms/step - loss: 227.4609 - val_loss: 197.6737\n",
      "Epoch 11/20\n",
      "341/341 [==============================] - 4s 12ms/step - loss: 223.1590 - val_loss: 181.9537\n",
      "Epoch 12/20\n",
      "341/341 [==============================] - 4s 12ms/step - loss: 225.1518 - val_loss: 188.0636\n",
      "Epoch 13/20\n",
      "341/341 [==============================] - 4s 13ms/step - loss: 216.8997 - val_loss: 180.8867\n",
      "Epoch 14/20\n",
      "341/341 [==============================] - 5s 13ms/step - loss: 217.3053 - val_loss: 174.2835\n",
      "Epoch 15/20\n",
      "341/341 [==============================] - 4s 13ms/step - loss: 213.0188 - val_loss: 173.9678\n",
      "Epoch 16/20\n",
      "341/341 [==============================] - 4s 12ms/step - loss: 212.8440 - val_loss: 171.7827\n",
      "Epoch 17/20\n",
      "341/341 [==============================] - 4s 12ms/step - loss: 212.4325 - val_loss: 175.8321\n",
      "Epoch 18/20\n",
      "341/341 [==============================] - 4s 12ms/step - loss: 207.3703 - val_loss: 175.0504\n",
      "Epoch 19/20\n",
      "341/341 [==============================] - 4s 12ms/step - loss: 206.6709 - val_loss: 172.8644\n",
      "Epoch 20/20\n",
      "341/341 [==============================] - 4s 12ms/step - loss: 207.9409 - val_loss: 172.8197\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "(43619, 40, 16) (43619, 1) (259, 40, 16)\n",
      "Epoch 1/20\n",
      "341/341 [==============================] - 7s 14ms/step - loss: 2302.6631 - val_loss: 1401.1649\n",
      "Epoch 2/20\n",
      "341/341 [==============================] - 5s 13ms/step - loss: 536.6458 - val_loss: 262.1743\n",
      "Epoch 3/20\n",
      "341/341 [==============================] - 5s 13ms/step - loss: 295.2174 - val_loss: 204.3243\n",
      "Epoch 4/20\n",
      "341/341 [==============================] - 4s 13ms/step - loss: 260.8228 - val_loss: 208.4018\n",
      "Epoch 5/20\n",
      "341/341 [==============================] - 4s 13ms/step - loss: 250.9730 - val_loss: 193.7172\n",
      "Epoch 6/20\n",
      "341/341 [==============================] - 4s 12ms/step - loss: 239.0885 - val_loss: 198.9499\n",
      "Epoch 7/20\n",
      "341/341 [==============================] - 4s 12ms/step - loss: 234.1671 - val_loss: 188.4711\n",
      "Epoch 8/20\n",
      "341/341 [==============================] - 4s 13ms/step - loss: 232.8324 - val_loss: 177.7818\n",
      "Epoch 9/20\n",
      "341/341 [==============================] - 4s 13ms/step - loss: 230.8300 - val_loss: 194.5389\n",
      "Epoch 10/20\n",
      "341/341 [==============================] - 4s 13ms/step - loss: 226.2832 - val_loss: 178.8285\n",
      "Epoch 11/20\n",
      "341/341 [==============================] - 5s 14ms/step - loss: 222.8701 - val_loss: 200.7646\n",
      "Epoch 12/20\n",
      "341/341 [==============================] - 5s 14ms/step - loss: 222.2377 - val_loss: 185.4484\n",
      "Epoch 13/20\n",
      "341/341 [==============================] - 5s 14ms/step - loss: 222.0511 - val_loss: 178.2418\n",
      "Epoch 14/20\n",
      "341/341 [==============================] - 5s 14ms/step - loss: 218.8535 - val_loss: 173.3578\n",
      "Epoch 15/20\n",
      "341/341 [==============================] - 5s 14ms/step - loss: 216.7917 - val_loss: 180.9339\n",
      "Epoch 16/20\n",
      "341/341 [==============================] - 5s 14ms/step - loss: 213.7044 - val_loss: 178.2787\n",
      "Epoch 17/20\n",
      "341/341 [==============================] - 5s 14ms/step - loss: 212.1002 - val_loss: 186.9784\n",
      "Epoch 18/20\n",
      "341/341 [==============================] - 5s 14ms/step - loss: 208.6355 - val_loss: 179.6404\n",
      "Epoch 19/20\n",
      "341/341 [==============================] - 5s 14ms/step - loss: 209.2087 - val_loss: 186.2742\n",
      "Epoch 20/20\n",
      "341/341 [==============================] - 5s 15ms/step - loss: 204.3598 - val_loss: 184.7609\n",
      "9/9 [==============================] - 1s 4ms/step\n",
      "(43619, 40, 16) (43619, 1) (259, 40, 16)\n",
      "Epoch 1/20\n",
      "341/341 [==============================] - 8s 16ms/step - loss: 2305.6499 - val_loss: 1427.9204\n",
      "Epoch 2/20\n",
      "341/341 [==============================] - 5s 14ms/step - loss: 518.5306 - val_loss: 247.3526\n",
      "Epoch 3/20\n",
      "341/341 [==============================] - 5s 14ms/step - loss: 281.0569 - val_loss: 208.3090\n",
      "Epoch 4/20\n",
      "341/341 [==============================] - 5s 14ms/step - loss: 261.0931 - val_loss: 217.8884\n",
      "Epoch 5/20\n",
      "341/341 [==============================] - 5s 14ms/step - loss: 251.7061 - val_loss: 220.6385\n",
      "Epoch 6/20\n",
      "341/341 [==============================] - 5s 15ms/step - loss: 245.5070 - val_loss: 224.3246\n",
      "Epoch 7/20\n",
      "341/341 [==============================] - 5s 14ms/step - loss: 238.1009 - val_loss: 281.0799\n",
      "Epoch 8/20\n",
      "341/341 [==============================] - 5s 14ms/step - loss: 237.2474 - val_loss: 218.3790\n",
      "Epoch 9/20\n",
      "341/341 [==============================] - 5s 14ms/step - loss: 231.2024 - val_loss: 219.6410\n",
      "Epoch 10/20\n",
      "341/341 [==============================] - 5s 14ms/step - loss: 228.2218 - val_loss: 226.9520\n",
      "Epoch 11/20\n",
      "341/341 [==============================] - 5s 14ms/step - loss: 229.1404 - val_loss: 218.4174\n",
      "Epoch 12/20\n",
      "341/341 [==============================] - 5s 14ms/step - loss: 226.2499 - val_loss: 214.0266\n",
      "Epoch 13/20\n",
      "341/341 [==============================] - 5s 14ms/step - loss: 221.6015 - val_loss: 241.9001\n",
      "Epoch 14/20\n",
      "341/341 [==============================] - 4s 12ms/step - loss: 220.6835 - val_loss: 217.5892\n",
      "Epoch 15/20\n",
      "341/341 [==============================] - 4s 12ms/step - loss: 220.4787 - val_loss: 213.2642\n",
      "Epoch 16/20\n",
      "341/341 [==============================] - 4s 11ms/step - loss: 217.5017 - val_loss: 213.3933\n",
      "Epoch 17/20\n",
      "341/341 [==============================] - 4s 11ms/step - loss: 216.0247 - val_loss: 217.3120\n",
      "Epoch 18/20\n",
      "341/341 [==============================] - 4s 11ms/step - loss: 216.4898 - val_loss: 200.8186\n",
      "Epoch 19/20\n",
      "341/341 [==============================] - 4s 11ms/step - loss: 210.3845 - val_loss: 196.0275\n",
      "Epoch 20/20\n",
      "341/341 [==============================] - 4s 11ms/step - loss: 210.0985 - val_loss: 215.7707\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "CPU times: total: 1min 26s\n",
      "Wall time: 6min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results_all002 = pd.DataFrame()\n",
    "for SEED in range(5):  \n",
    "    tf.random.set_seed(SEED)\n",
    "    mse = []\n",
    "    R2_val = []\n",
    "    RMSE = []\n",
    "    score_val = []\n",
    "    \n",
    "    # parameter's sample\n",
    "    # weights_file = \"weights_file_lstm_optimalmodel_all.h5\"\n",
    "    alpha = 0.2\n",
    "    sequence_length = 40\n",
    "    epochs = 20\n",
    "    nodes_per_layer = [32]\n",
    "    dropout = 0.1\n",
    "    activation = 'tanh'\n",
    "    batch_size = 128\n",
    "    remaining_sensors = remaining_sensors\n",
    "    # create model\n",
    "    input_shape = (sequence_length, len(remaining_sensors))\n",
    "    model = model_lstm_1layer(input_shape, nodes_per_layer[0], dropout, activation)\n",
    "    \n",
    "    # Data prepration\n",
    "    X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "\n",
    "    # create sequences train, test\n",
    "    train_array = gen_data_wrapper(X_train_interim, sequence_length,remaining_sensors)\n",
    "    label_array = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'])\n",
    "\n",
    "    test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,remaining_sensors, -99.))\n",
    "               for unit_nr in X_test_interim['Unit'].unique())\n",
    "    \n",
    "    test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "    test_rul = rul_piecewise_fct(y_test,rul_piecewise)\n",
    "    print(train_array.shape, label_array.shape, test_array.shape)\n",
    "            \n",
    "    # Model fitting\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        start_time = time.time()\n",
    "        model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n",
    "        weights_file = model.get_weights()\n",
    "        model.set_weights(weights_file)  # reset optimizer and node weights before every training iteration\n",
    "        history = model.fit(train_array, label_array,\n",
    "                                validation_data=(test_array, test_rul),\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                # callbacks=[cb],\n",
    "                                verbose=1)\n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "        mse.append(history.history['val_loss'][-1])\n",
    "\n",
    "        y_hat_val_split = model.predict(test_array)\n",
    "        R2_val.append(r2_score(test_rul, y_hat_val_split))\n",
    "        RMSE.append(np.sqrt(mean_squared_error(test_rul, y_hat_val_split)))\n",
    "        score_val.append(compute_s_score(test_rul, y_hat_val_split))\n",
    "            \n",
    "        \n",
    "    #  append results\n",
    "    d = {'RMSE' :np.mean(RMSE), 'std_RMSE' :np.std(RMSE),\n",
    "         'S_score' :np.mean(score_val), 'std_S_score' :np.std(score_val),\n",
    "         'MSE':np.mean(mse), 'std_MSE':np.std(mse),\n",
    "         'nodes':str(nodes_per_layer), 'dropout':dropout, \n",
    "         'activation':activation, 'batch_size':batch_size, 'TW' : sequence_length,\n",
    "         'time':training_time}\n",
    "\n",
    "#     results = results.append(pd.DataFrame(d, index=[0]), ignore_index=True)\n",
    "    results_all002 = pd.concat([results_all002, pd.DataFrame(d, index=[0])], ignore_index=True)\n",
    "    results_all002.to_csv('results/all/fd002.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>std_RMSE</th>\n",
       "      <th>S_score</th>\n",
       "      <th>std_S_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>TW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.375069</td>\n",
       "      <td>0.0</td>\n",
       "      <td>880.479340</td>\n",
       "      <td>0.0</td>\n",
       "      <td>178.892471</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.638212</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1063.441856</td>\n",
       "      <td>0.0</td>\n",
       "      <td>186.000839</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.146090</td>\n",
       "      <td>0.0</td>\n",
       "      <td>883.644756</td>\n",
       "      <td>0.0</td>\n",
       "      <td>172.819672</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.592679</td>\n",
       "      <td>0.0</td>\n",
       "      <td>916.008385</td>\n",
       "      <td>0.0</td>\n",
       "      <td>184.760925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14.689135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>831.363601</td>\n",
       "      <td>0.0</td>\n",
       "      <td>215.770660</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>128</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RMSE  std_RMSE      S_score  std_S_score         MSE  std_MSE nodes  \\\n",
       "0  13.375069       0.0   880.479340          0.0  178.892471      0.0  [32]   \n",
       "1  13.638212       0.0  1063.441856          0.0  186.000839      0.0  [32]   \n",
       "2  13.146090       0.0   883.644756          0.0  172.819672      0.0  [32]   \n",
       "3  13.592679       0.0   916.008385          0.0  184.760925      0.0  [32]   \n",
       "4  14.689135       0.0   831.363601          0.0  215.770660      0.0  [32]   \n",
       "\n",
       "   dropout activation  batch_size  TW  \n",
       "0      0.1       tanh         128  40  \n",
       "1      0.1       tanh         128  40  \n",
       "2      0.1       tanh         128  40  \n",
       "3      0.1       tanh         128  40  \n",
       "4      0.1       tanh         128  40  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_all002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FD003 <a class=\"anchor\" id=\"fd003\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53759, 27) (33991, 26) (259, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# Load data and preprocess\n",
    "train, test, y_test = prepare_data('FD002.txt')\n",
    "print(train.shape, test.shape, y_test.shape)\n",
    "sensor_names = ['T20','T24','T30','T50','P20','P15','P30','Nf','Nc','epr','Ps30','phi',\n",
    "                    'NRf','NRc','BPR','farB','htBleed','Nf_dmd','PCNfR_dmd','W31','W32']\n",
    "\n",
    "remaining_sensors = ['T24','T30','T50', 'P15', 'P30','Nf','Nc', 'epr','Ps30','phi',\n",
    "                    'NRf','NRc','BPR','htBleed','W31','W32']\n",
    "drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "\n",
    "rul_piecewise = 125\n",
    "train['RUL'].clip(upper=rul_piecewise, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44919, 35, 16) (44919, 1) (259, 35, 16)\n",
      "Epoch 1/20\n",
      "1404/1404 [==============================] - 9s 5ms/step - loss: 875.2603 - val_loss: 262.0320\n",
      "Epoch 2/20\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 267.9233 - val_loss: 222.1751\n",
      "Epoch 3/20\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 244.3946 - val_loss: 201.0150\n",
      "Epoch 4/20\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 230.2976 - val_loss: 183.0546\n",
      "Epoch 5/20\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 219.2994 - val_loss: 172.6591\n",
      "Epoch 6/20\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 210.1452 - val_loss: 166.0017\n",
      "Epoch 7/20\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 204.7170 - val_loss: 171.0595\n",
      "Epoch 8/20\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 198.9625 - val_loss: 175.7807\n",
      "Epoch 9/20\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 193.3466 - val_loss: 175.8752\n",
      "Epoch 10/20\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 189.4008 - val_loss: 187.8429\n",
      "Epoch 11/20\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 187.8526 - val_loss: 177.0571\n",
      "Epoch 12/20\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 184.3921 - val_loss: 177.2586\n",
      "Epoch 13/20\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 181.9678 - val_loss: 170.4041\n",
      "Epoch 14/20\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 180.3086 - val_loss: 165.4949\n",
      "Epoch 15/20\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 178.6143 - val_loss: 165.1598\n",
      "Epoch 16/20\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 176.5515 - val_loss: 174.7616\n",
      "Epoch 17/20\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 174.5516 - val_loss: 167.5320\n",
      "Epoch 18/20\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 173.1660 - val_loss: 161.0701\n",
      "Epoch 19/20\n",
      "1404/1404 [==============================] - 6s 5ms/step - loss: 171.6355 - val_loss: 171.6180\n",
      "Epoch 20/20\n",
      "1404/1404 [==============================] - 6s 4ms/step - loss: 171.0572 - val_loss: 154.8353\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "(44919, 35, 16) (44919, 1) (259, 35, 16)\n",
      "Epoch 1/20\n",
      "1404/1404 [==============================] - 8s 5ms/step - loss: 910.1124 - val_loss: 229.7614\n",
      "Epoch 2/20\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 278.4445 - val_loss: 192.9326\n",
      "Epoch 3/20\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 247.7780 - val_loss: 193.9095\n",
      "Epoch 4/20\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 238.6549 - val_loss: 197.5541\n",
      "Epoch 5/20\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 227.3999 - val_loss: 187.8183\n",
      "Epoch 6/20\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 218.1131 - val_loss: 177.2463\n",
      "Epoch 7/20\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 209.5540 - val_loss: 185.9273\n",
      "Epoch 8/20\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 202.1811 - val_loss: 188.3519\n",
      "Epoch 9/20\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 198.7030 - val_loss: 176.2229\n",
      "Epoch 10/20\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 192.3596 - val_loss: 171.4011\n",
      "Epoch 11/20\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 190.4723 - val_loss: 170.3011\n",
      "Epoch 12/20\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 187.2610 - val_loss: 173.2126\n",
      "Epoch 13/20\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 184.5229 - val_loss: 180.0232\n",
      "Epoch 14/20\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 181.2082 - val_loss: 164.4053\n",
      "Epoch 15/20\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 180.3000 - val_loss: 154.0524\n",
      "Epoch 16/20\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 178.5068 - val_loss: 160.0963\n",
      "Epoch 17/20\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 177.5240 - val_loss: 162.1289\n",
      "Epoch 18/20\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 174.4527 - val_loss: 162.6550\n",
      "Epoch 19/20\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 172.8288 - val_loss: 170.2475\n",
      "Epoch 20/20\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 171.6910 - val_loss: 153.2658\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "(44919, 35, 16) (44919, 1) (259, 35, 16)\n",
      "Epoch 1/20\n",
      "1404/1404 [==============================] - 8s 5ms/step - loss: 893.1146 - val_loss: 243.0144\n",
      "Epoch 2/20\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 281.7503 - val_loss: 206.3676\n",
      "Epoch 3/20\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 248.8516 - val_loss: 183.5868\n",
      "Epoch 4/20\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 231.5806 - val_loss: 220.0802\n",
      "Epoch 5/20\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 223.5201 - val_loss: 178.1985\n",
      "Epoch 6/20\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 215.1734 - val_loss: 183.0918\n",
      "Epoch 7/20\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 210.2218 - val_loss: 174.8994\n",
      "Epoch 8/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 203.7128 - val_loss: 181.7912\n",
      "Epoch 9/20\n",
      "1404/1404 [==============================] - 9s 7ms/step - loss: 197.1553 - val_loss: 160.8288\n",
      "Epoch 10/20\n",
      "1404/1404 [==============================] - 9s 7ms/step - loss: 193.7434 - val_loss: 168.2807\n",
      "Epoch 11/20\n",
      "1404/1404 [==============================] - 9s 7ms/step - loss: 187.9479 - val_loss: 161.2915\n",
      "Epoch 12/20\n",
      "1404/1404 [==============================] - 9s 7ms/step - loss: 186.1505 - val_loss: 166.7559\n",
      "Epoch 13/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 183.3483 - val_loss: 157.0010\n",
      "Epoch 14/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 181.8893 - val_loss: 162.2323\n",
      "Epoch 15/20\n",
      "1404/1404 [==============================] - 9s 7ms/step - loss: 179.2278 - val_loss: 162.4207\n",
      "Epoch 16/20\n",
      "1404/1404 [==============================] - 9s 7ms/step - loss: 176.2255 - val_loss: 166.5013\n",
      "Epoch 17/20\n",
      "1404/1404 [==============================] - 9s 7ms/step - loss: 176.3942 - val_loss: 159.4240\n",
      "Epoch 18/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 172.8109 - val_loss: 161.6228\n",
      "Epoch 19/20\n",
      "1404/1404 [==============================] - 9s 7ms/step - loss: 172.9817 - val_loss: 162.6590\n",
      "Epoch 20/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 171.3737 - val_loss: 163.6155\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "(44919, 35, 16) (44919, 1) (259, 35, 16)\n",
      "Epoch 1/20\n",
      "1404/1404 [==============================] - 11s 7ms/step - loss: 732.2673 - val_loss: 231.6324\n",
      "Epoch 2/20\n",
      "1404/1404 [==============================] - 9s 7ms/step - loss: 275.8623 - val_loss: 240.2034\n",
      "Epoch 3/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 247.3790 - val_loss: 189.8604\n",
      "Epoch 4/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 239.3029 - val_loss: 185.6404\n",
      "Epoch 5/20\n",
      "1404/1404 [==============================] - 9s 7ms/step - loss: 228.0259 - val_loss: 195.9941\n",
      "Epoch 6/20\n",
      "1404/1404 [==============================] - 9s 7ms/step - loss: 219.7353 - val_loss: 190.6827\n",
      "Epoch 7/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 214.2814 - val_loss: 184.7584\n",
      "Epoch 8/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 208.4737 - val_loss: 186.9551\n",
      "Epoch 9/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 203.4108 - val_loss: 166.9525\n",
      "Epoch 10/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 199.0721 - val_loss: 194.8114\n",
      "Epoch 11/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 194.8080 - val_loss: 173.1296\n",
      "Epoch 12/20\n",
      "1404/1404 [==============================] - 9s 7ms/step - loss: 193.1625 - val_loss: 170.7480\n",
      "Epoch 13/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 186.7074 - val_loss: 162.5448\n",
      "Epoch 14/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 185.7125 - val_loss: 179.7623\n",
      "Epoch 15/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 183.4756 - val_loss: 159.3191\n",
      "Epoch 16/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 182.6881 - val_loss: 176.6120\n",
      "Epoch 17/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 181.5810 - val_loss: 161.5260\n",
      "Epoch 18/20\n",
      "1404/1404 [==============================] - 9s 6ms/step - loss: 178.1893 - val_loss: 163.6708\n",
      "Epoch 19/20\n",
      "1404/1404 [==============================] - 9s 7ms/step - loss: 177.4631 - val_loss: 164.0528\n",
      "Epoch 20/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 176.1715 - val_loss: 170.1382\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "(44919, 35, 16) (44919, 1) (259, 35, 16)\n",
      "Epoch 1/20\n",
      "1404/1404 [==============================] - 12s 7ms/step - loss: 835.0447 - val_loss: 255.5831\n",
      "Epoch 2/20\n",
      "1404/1404 [==============================] - 12s 8ms/step - loss: 280.4329 - val_loss: 196.3710\n",
      "Epoch 3/20\n",
      "1404/1404 [==============================] - 11s 8ms/step - loss: 250.6456 - val_loss: 192.0607\n",
      "Epoch 4/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 236.0033 - val_loss: 273.1840\n",
      "Epoch 5/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 223.5217 - val_loss: 231.9574\n",
      "Epoch 6/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 215.5301 - val_loss: 232.3107\n",
      "Epoch 7/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 210.5696 - val_loss: 205.0358\n",
      "Epoch 8/20\n",
      "1404/1404 [==============================] - 11s 8ms/step - loss: 202.8464 - val_loss: 172.6426\n",
      "Epoch 9/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 195.6943 - val_loss: 195.0517\n",
      "Epoch 10/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 192.7972 - val_loss: 173.7727\n",
      "Epoch 11/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 191.6629 - val_loss: 166.5533\n",
      "Epoch 12/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 184.1188 - val_loss: 162.1942\n",
      "Epoch 13/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 182.1788 - val_loss: 176.8495\n",
      "Epoch 14/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 180.6967 - val_loss: 164.7471\n",
      "Epoch 15/20\n",
      "1404/1404 [==============================] - 10s 7ms/step - loss: 178.1700 - val_loss: 171.0175\n",
      "Epoch 16/20\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 175.4335 - val_loss: 191.6127\n",
      "Epoch 17/20\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 174.1473 - val_loss: 177.6422\n",
      "Epoch 18/20\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 173.0318 - val_loss: 160.2557\n",
      "Epoch 19/20\n",
      "1404/1404 [==============================] - 8s 5ms/step - loss: 171.6459 - val_loss: 154.7711\n",
      "Epoch 20/20\n",
      "1404/1404 [==============================] - 7s 5ms/step - loss: 169.7309 - val_loss: 161.5576\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "CPU times: total: 1min 47s\n",
      "Wall time: 13min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results_all003 = pd.DataFrame()\n",
    "for SEED in range(5):  \n",
    "    tf.random.set_seed(SEED)\n",
    "    mse = []\n",
    "    R2_val = []\n",
    "    RMSE = []\n",
    "    score_val = []\n",
    "    \n",
    "    # parameter's sample\n",
    "    # weights_file = \"weights_file_lstm_optimalmodel_all.h5\"\n",
    "    alpha = 0.1\n",
    "    sequence_length = 35\n",
    "    epochs = 20\n",
    "    nodes_per_layer = [64]\n",
    "    dropout = 0.2\n",
    "    activation = 'tanh'\n",
    "    batch_size = 32\n",
    "    remaining_sensors = remaining_sensors\n",
    "    # create model\n",
    "    input_shape = (sequence_length, len(remaining_sensors))\n",
    "    model = model_lstm_1layer(input_shape, nodes_per_layer[0], dropout,\n",
    "                             activation)\n",
    "    \n",
    "    # Data prepration\n",
    "    X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "\n",
    "    # create sequences train, test\n",
    "    train_array = gen_data_wrapper(X_train_interim, sequence_length,remaining_sensors)\n",
    "    label_array = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'])\n",
    "\n",
    "    test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,remaining_sensors, -99.))\n",
    "               for unit_nr in X_test_interim['Unit'].unique())\n",
    "    \n",
    "    test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "    test_rul = rul_piecewise_fct(y_test,rul_piecewise)\n",
    "    print(train_array.shape, label_array.shape, test_array.shape)\n",
    "            \n",
    "    # Model fitting\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        start_time = time.time()\n",
    "        model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n",
    "        weights_file = model.get_weights()\n",
    "        model.set_weights(weights_file)  # reset optimizer and node weights before every training iteration\n",
    "        history = model.fit(train_array, label_array,\n",
    "                                validation_data=(test_array, test_rul),\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                # callbacks=[cb],\n",
    "                                verbose=1)\n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "        mse.append(history.history['val_loss'][-1])\n",
    "\n",
    "        y_hat_val_split = model.predict(test_array)\n",
    "        R2_val.append(r2_score(test_rul, y_hat_val_split))\n",
    "        RMSE.append(np.sqrt(mean_squared_error(test_rul, y_hat_val_split)))\n",
    "        score_val.append(compute_s_score(test_rul, y_hat_val_split))\n",
    "            \n",
    "        \n",
    "    #  append results\n",
    "    d = {'RMSE' :np.mean(RMSE), 'std_RMSE' :np.std(RMSE),\n",
    "         'S_score' :np.mean(score_val), 'std_S_score' :np.std(score_val),\n",
    "         'MSE':np.mean(mse), 'std_MSE':np.std(mse),\n",
    "         'nodes':str(nodes_per_layer), 'dropout':dropout, \n",
    "         'activation':activation, 'batch_size':batch_size, 'TW' : sequence_length,\n",
    "         'time':training_time}\n",
    "\n",
    "#     results = results.append(pd.DataFrame(d, index=[0]), ignore_index=True)\n",
    "    results_all003 = pd.concat([results_all003, pd.DataFrame(d, index=[0])], ignore_index=True)\n",
    "    results_all003.to_csv('results/all/fd003.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>std_RMSE</th>\n",
       "      <th>S_score</th>\n",
       "      <th>std_S_score</th>\n",
       "      <th>MSE</th>\n",
       "      <th>std_MSE</th>\n",
       "      <th>nodes</th>\n",
       "      <th>dropout</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>TW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.443282</td>\n",
       "      <td>0.0</td>\n",
       "      <td>669.450046</td>\n",
       "      <td>0.0</td>\n",
       "      <td>154.835281</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.380057</td>\n",
       "      <td>0.0</td>\n",
       "      <td>648.540484</td>\n",
       "      <td>0.0</td>\n",
       "      <td>153.265808</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.791228</td>\n",
       "      <td>0.0</td>\n",
       "      <td>744.168177</td>\n",
       "      <td>0.0</td>\n",
       "      <td>163.615524</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.043703</td>\n",
       "      <td>0.0</td>\n",
       "      <td>931.461167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>170.138184</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.710529</td>\n",
       "      <td>0.0</td>\n",
       "      <td>611.947600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>161.557556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RMSE  std_RMSE     S_score  std_S_score         MSE  std_MSE nodes  \\\n",
       "0  12.443282       0.0  669.450046          0.0  154.835281      0.0  [64]   \n",
       "1  12.380057       0.0  648.540484          0.0  153.265808      0.0  [64]   \n",
       "2  12.791228       0.0  744.168177          0.0  163.615524      0.0  [64]   \n",
       "3  13.043703       0.0  931.461167          0.0  170.138184      0.0  [64]   \n",
       "4  12.710529       0.0  611.947600          0.0  161.557556      0.0  [64]   \n",
       "\n",
       "   dropout activation  batch_size  TW  \n",
       "0      0.2       tanh          32  35  \n",
       "1      0.2       tanh          32  35  \n",
       "2      0.2       tanh          32  35  \n",
       "3      0.2       tanh          32  35  \n",
       "4      0.2       tanh          32  35  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_all003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FD001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20631, 27) (13096, 26) (100, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# Load data and preprocess\n",
    "train, test, y_test = prepare_data('FD001.txt')\n",
    "print(train.shape, test.shape, y_test.shape)\n",
    "sensor_names = ['T20','T24','T30','T50','P20','P15','P30','Nf','Nc','epr','Ps30','phi',\n",
    "                    'NRf','NRc','BPR','farB','htBleed','Nf_dmd','PCNfR_dmd','W31','W32']\n",
    "remaining_sensors = ['T24','T30','T50', 'P15', 'P30','Nf','Nc','Ps30','phi',\n",
    "                    'NRf','NRc','BPR','htBleed','W31','W32']\n",
    "drop_sensors = [element for element in sensor_names if element not in remaining_sensors]\n",
    "\n",
    "rul_piecewise = 125\n",
    "train['RUL'].clip(upper=rul_piecewise, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17731, 30, 15) (17731, 1) (100, 30, 15)\n",
      "Epoch 1/20\n",
      "278/278 [==============================] - 4s 11ms/step - loss: 1945.0713 - val_loss: 1142.2662\n",
      "Epoch 2/20\n",
      "278/278 [==============================] - 3s 9ms/step - loss: 594.9089 - val_loss: 336.2231\n",
      "Epoch 3/20\n",
      "278/278 [==============================] - 3s 10ms/step - loss: 349.9518 - val_loss: 213.0034\n",
      "Epoch 4/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 243.1687 - val_loss: 380.7289\n",
      "Epoch 5/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 222.0929 - val_loss: 180.0421\n",
      "Epoch 6/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 203.6436 - val_loss: 266.0940\n",
      "Epoch 7/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 200.1159 - val_loss: 201.9042\n",
      "Epoch 8/20\n",
      "278/278 [==============================] - 3s 10ms/step - loss: 189.3215 - val_loss: 217.6176\n",
      "Epoch 9/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 187.8194 - val_loss: 170.0289\n",
      "Epoch 10/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 182.5679 - val_loss: 182.4919\n",
      "Epoch 11/20\n",
      "278/278 [==============================] - 3s 12ms/step - loss: 171.0725 - val_loss: 165.3862\n",
      "Epoch 12/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 174.3374 - val_loss: 189.9700\n",
      "Epoch 13/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 171.1269 - val_loss: 189.6711\n",
      "Epoch 14/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 168.6948 - val_loss: 221.8667\n",
      "Epoch 15/20\n",
      "278/278 [==============================] - 3s 10ms/step - loss: 171.7371 - val_loss: 184.4069\n",
      "Epoch 16/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 169.3708 - val_loss: 218.9014\n",
      "Epoch 17/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 163.9546 - val_loss: 189.2866\n",
      "Epoch 18/20\n",
      "278/278 [==============================] - 3s 12ms/step - loss: 166.1172 - val_loss: 175.7189\n",
      "Epoch 19/20\n",
      "278/278 [==============================] - 3s 10ms/step - loss: 164.7826 - val_loss: 177.0341\n",
      "Epoch 20/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 160.3572 - val_loss: 188.1117\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "(17731, 30, 15) (17731, 1) (100, 30, 15)\n",
      "Epoch 1/20\n",
      "278/278 [==============================] - 5s 12ms/step - loss: 1952.8530 - val_loss: 990.8794\n",
      "Epoch 2/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 608.1251 - val_loss: 449.7914\n",
      "Epoch 3/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 346.0687 - val_loss: 450.3850\n",
      "Epoch 4/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 243.6275 - val_loss: 235.8803\n",
      "Epoch 5/20\n",
      "278/278 [==============================] - 3s 12ms/step - loss: 208.5704 - val_loss: 230.4949\n",
      "Epoch 6/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 204.7195 - val_loss: 177.5652\n",
      "Epoch 7/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 194.7312 - val_loss: 240.1607\n",
      "Epoch 8/20\n",
      "278/278 [==============================] - 3s 10ms/step - loss: 193.7889 - val_loss: 169.3054\n",
      "Epoch 9/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 185.7281 - val_loss: 193.4146\n",
      "Epoch 10/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 172.9265 - val_loss: 311.7087\n",
      "Epoch 11/20\n",
      "278/278 [==============================] - 3s 10ms/step - loss: 175.0038 - val_loss: 177.7875\n",
      "Epoch 12/20\n",
      "278/278 [==============================] - 3s 12ms/step - loss: 174.8739 - val_loss: 165.3689\n",
      "Epoch 13/20\n",
      "278/278 [==============================] - 3s 10ms/step - loss: 168.0203 - val_loss: 168.2959\n",
      "Epoch 14/20\n",
      "278/278 [==============================] - 3s 10ms/step - loss: 169.6170 - val_loss: 169.7762\n",
      "Epoch 15/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 166.2830 - val_loss: 244.7424\n",
      "Epoch 16/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 167.2132 - val_loss: 264.9800\n",
      "Epoch 17/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 166.7649 - val_loss: 213.0706\n",
      "Epoch 18/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 164.3820 - val_loss: 167.3575\n",
      "Epoch 19/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 159.4921 - val_loss: 161.1136\n",
      "Epoch 20/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 161.9956 - val_loss: 171.4814\n",
      "4/4 [==============================] - 0s 823us/step\n",
      "(17731, 30, 15) (17731, 1) (100, 30, 15)\n",
      "Epoch 1/20\n",
      "278/278 [==============================] - 4s 11ms/step - loss: 1975.2908 - val_loss: 915.3917\n",
      "Epoch 2/20\n",
      "278/278 [==============================] - 3s 10ms/step - loss: 554.9566 - val_loss: 632.6024\n",
      "Epoch 3/20\n",
      "278/278 [==============================] - 3s 12ms/step - loss: 324.1010 - val_loss: 241.9236\n",
      "Epoch 4/20\n",
      "278/278 [==============================] - 3s 11ms/step - loss: 256.7356 - val_loss: 331.1473\n",
      "Epoch 5/20\n",
      "278/278 [==============================] - 4s 13ms/step - loss: 230.4926 - val_loss: 210.3607\n",
      "Epoch 6/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 217.0039 - val_loss: 168.7097\n",
      "Epoch 7/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 200.9943 - val_loss: 185.5622\n",
      "Epoch 8/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 193.8797 - val_loss: 180.1497\n",
      "Epoch 9/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 188.0363 - val_loss: 166.0519\n",
      "Epoch 10/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 182.0837 - val_loss: 303.9132\n",
      "Epoch 11/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 184.5538 - val_loss: 173.8203\n",
      "Epoch 12/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 177.8627 - val_loss: 167.6324\n",
      "Epoch 13/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 171.5343 - val_loss: 198.0423\n",
      "Epoch 14/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 176.1982 - val_loss: 164.5182\n",
      "Epoch 15/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 174.8412 - val_loss: 176.3169\n",
      "Epoch 16/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 166.5816 - val_loss: 174.9482\n",
      "Epoch 17/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 165.2536 - val_loss: 220.5133\n",
      "Epoch 18/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 167.8338 - val_loss: 165.8206\n",
      "Epoch 19/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 163.2469 - val_loss: 186.5652\n",
      "Epoch 20/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 160.5341 - val_loss: 187.6648\n",
      "4/4 [==============================] - 0s 0s/step\n",
      "(17731, 30, 15) (17731, 1) (100, 30, 15)\n",
      "Epoch 1/20\n",
      "278/278 [==============================] - 6s 16ms/step - loss: 1984.7781 - val_loss: 944.8501\n",
      "Epoch 2/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 595.3312 - val_loss: 796.7349\n",
      "Epoch 3/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 487.3704 - val_loss: 448.8324\n",
      "Epoch 4/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 300.1323 - val_loss: 243.3225\n",
      "Epoch 5/20\n",
      "278/278 [==============================] - 4s 13ms/step - loss: 227.1539 - val_loss: 307.1239\n",
      "Epoch 6/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 217.6322 - val_loss: 243.2936\n",
      "Epoch 7/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 205.4608 - val_loss: 583.1284\n",
      "Epoch 8/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 199.0123 - val_loss: 202.9959\n",
      "Epoch 9/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 192.3214 - val_loss: 306.6041\n",
      "Epoch 10/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 191.9700 - val_loss: 228.2826\n",
      "Epoch 11/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 181.8290 - val_loss: 174.9664\n",
      "Epoch 12/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 176.6738 - val_loss: 270.9405\n",
      "Epoch 13/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 176.1307 - val_loss: 226.5398\n",
      "Epoch 14/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 171.4944 - val_loss: 292.2494\n",
      "Epoch 15/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 184.8886 - val_loss: 307.2288\n",
      "Epoch 16/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 172.8716 - val_loss: 178.8398\n",
      "Epoch 17/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 170.3085 - val_loss: 177.2670\n",
      "Epoch 18/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 170.6612 - val_loss: 230.9477\n",
      "Epoch 19/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 166.8671 - val_loss: 175.9093\n",
      "Epoch 20/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 164.6228 - val_loss: 177.2207\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "(17731, 30, 15) (17731, 1) (100, 30, 15)\n",
      "Epoch 1/20\n",
      "278/278 [==============================] - 6s 16ms/step - loss: 2036.7047 - val_loss: 1121.9025\n",
      "Epoch 2/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 608.3298 - val_loss: 511.1324\n",
      "Epoch 3/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 451.4001 - val_loss: 397.6616\n",
      "Epoch 4/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 283.1056 - val_loss: 220.1386\n",
      "Epoch 5/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 225.7812 - val_loss: 362.0092\n",
      "Epoch 6/20\n",
      "278/278 [==============================] - 3s 12ms/step - loss: 220.5620 - val_loss: 203.5515\n",
      "Epoch 7/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 196.6146 - val_loss: 160.1352\n",
      "Epoch 8/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 192.0621 - val_loss: 206.8795\n",
      "Epoch 9/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 187.9201 - val_loss: 161.9342\n",
      "Epoch 10/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 184.2150 - val_loss: 168.6344\n",
      "Epoch 11/20\n",
      "278/278 [==============================] - 5s 16ms/step - loss: 180.1347 - val_loss: 256.5178\n",
      "Epoch 12/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 180.4802 - val_loss: 166.5536\n",
      "Epoch 13/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 177.3801 - val_loss: 240.7767\n",
      "Epoch 14/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 172.2560 - val_loss: 161.7316\n",
      "Epoch 15/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 169.4863 - val_loss: 169.6277\n",
      "Epoch 16/20\n",
      "278/278 [==============================] - 5s 17ms/step - loss: 164.6095 - val_loss: 177.5231\n",
      "Epoch 17/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 168.5148 - val_loss: 172.9324\n",
      "Epoch 18/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 163.0150 - val_loss: 263.9401\n",
      "Epoch 19/20\n",
      "278/278 [==============================] - 4s 15ms/step - loss: 167.9479 - val_loss: 162.4687\n",
      "Epoch 20/20\n",
      "278/278 [==============================] - 4s 16ms/step - loss: 159.9210 - val_loss: 204.7722\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "CPU times: total: 1min 2s\n",
      "Wall time: 6min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results_all001 = pd.DataFrame()\n",
    "for SEED in range(5):  \n",
    "    set_seed(SEED)\n",
    "    mse = []\n",
    "    R2_val = []\n",
    "    RMSE = []\n",
    "    score_val = []\n",
    "    \n",
    "    # parameter's sample\n",
    "    # weights_file = \"weights_file_lstm_optimalmodel_all.h5\"\n",
    "    alpha = 0.1\n",
    "    sequence_length = 30\n",
    "    epochs = 20\n",
    "    nodes_per_layer = [128]\n",
    "    dropout = 0.2\n",
    "    activation = 'tanh'\n",
    "    batch_size = 64\n",
    "    remaining_sensors = remaining_sensors\n",
    "    # create model\n",
    "    input_shape = (sequence_length, len(remaining_sensors))\n",
    "    model = model_lstm_1layer(input_shape, nodes_per_layer[0], dropout,\n",
    "                             activation)\n",
    "    \n",
    "    # Data prepration\n",
    "    X_train_interim, X_test_interim = prep_data(train, test, drop_sensors, remaining_sensors, alpha)\n",
    "\n",
    "    # create sequences train, test\n",
    "    train_array = gen_data_wrapper(X_train_interim, sequence_length,remaining_sensors)\n",
    "    label_array = gen_label_wrapper(X_train_interim, sequence_length, ['RUL'])\n",
    "\n",
    "    test_gen = (list(gen_test_data(X_test_interim[X_test_interim['Unit']==unit_nr], sequence_length,remaining_sensors, -99.))\n",
    "               for unit_nr in X_test_interim['Unit'].unique())\n",
    "    \n",
    "    test_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "    test_rul = rul_piecewise_fct(y_test,rul_piecewise)\n",
    "    print(train_array.shape, label_array.shape, test_array.shape)\n",
    "            \n",
    "    # Model fitting\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        start_time = time.time()\n",
    "        model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n",
    "        weights_file = model.get_weights()\n",
    "        model.set_weights(weights_file)  # reset optimizer and node weights before every training iteration\n",
    "        history = model.fit(train_array, label_array,\n",
    "                                validation_data=(test_array, test_rul),\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                # callbacks=[cb],\n",
    "                                verbose=1)\n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "        mse.append(history.history['val_loss'][-1])\n",
    "\n",
    "        y_hat_val_split = model.predict(test_array)\n",
    "        R2_val.append(r2_score(test_rul, y_hat_val_split))\n",
    "        RMSE.append(np.sqrt(mean_squared_error(test_rul, y_hat_val_split)))\n",
    "        score_val.append(compute_s_score(test_rul, y_hat_val_split))\n",
    "            \n",
    "        \n",
    "    #  append results\n",
    "    d = {'RMSE' :np.mean(RMSE), 'std_RMSE' :np.std(RMSE),\n",
    "         'S_score' :np.mean(score_val), 'std_S_score' :np.std(score_val),\n",
    "         'MSE':np.mean(mse), 'std_MSE':np.std(mse),\n",
    "         'nodes':str(nodes_per_layer), 'dropout':dropout, \n",
    "         'activation':activation, 'batch_size':batch_size, 'TW' : sequence_length,\n",
    "         'time':training_time}\n",
    "\n",
    "#     results = results.append(pd.DataFrame(d, index=[0]), ignore_index=True)\n",
    "    results_all001 = pd.concat([results_all001, pd.DataFrame(d, index=[0])], ignore_index=True)\n",
    "    results_all001.to_csv('results/all/fd001.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
